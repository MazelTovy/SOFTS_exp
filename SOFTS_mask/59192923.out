================ JOB INFORMATION ================
Running on node: gv005
Job ID: 59192923
Start time: Fri Apr 11 01:44:35 AM EDT 2025
===========================================
Starting training...
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_48_48', model='SOFTS', data='ETTh1', root_path='./dataset/ETT/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=48, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh1_48_48_SOFTS_ETTh1_ftM_sl48_ll48_pl48_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8545
val 2833
test 2833

------ Mask Loss Comparison ------
Original Loss: 0.6100354
Best Mask Loss: 0.6092065 (mask length: 3)
Improvement: 0.0008289 (0.14%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6104499
---------------------------------

	iters: 100, epoch: 1 | loss: 0.3511860 | orig_loss: 0.3646877 | penalty: 0.3649506
	speed: 0.0731s/iter; left time: 5872.5490s
	iters: 200, epoch: 1 | loss: 0.3244077 | orig_loss: 0.3280406 | penalty: 0.3281484
	speed: 0.0186s/iter; left time: 1491.4209s
Epoch: 1 cost time: 7.954662799835205
Epoch Losses - Original: 0.3424675, With Penalty: 0.3428681, Difference: 0.0004006 (0.12%)
Epoch: 1, Steps: 268 | Train Loss: 0.3377968 Vali Loss: 0.5229658 Test Loss: 0.3500969
Validation loss decreased (inf --> 0.522966).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.3035941 | orig_loss: 0.3255299 | penalty: 0.3257753
	speed: 0.0414s/iter; left time: 3314.2269s
	iters: 200, epoch: 2 | loss: 0.3221043 | orig_loss: 0.3155633 | penalty: 0.3156319
	speed: 0.0190s/iter; left time: 1522.4532s
Epoch: 2 cost time: 5.254741191864014
Epoch Losses - Original: 0.3191283, With Penalty: 0.3192455, Difference: 0.0001172 (0.04%)
Epoch: 2, Steps: 268 | Train Loss: 0.3128492 Vali Loss: 0.5188747 Test Loss: 0.3545437
Validation loss decreased (0.522966 --> 0.518875).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.2489745 | orig_loss: 0.3144284 | penalty: 0.3144797
	speed: 0.0411s/iter; left time: 3277.7329s
	iters: 200, epoch: 3 | loss: 0.2809164 | orig_loss: 0.3143381 | penalty: 0.3145589
	speed: 0.0190s/iter; left time: 1517.0034s
Epoch: 3 cost time: 5.227534294128418
Epoch Losses - Original: 0.3116411, With Penalty: 0.3117592, Difference: 0.0001181 (0.04%)
Epoch: 3, Steps: 268 | Train Loss: 0.2649454 Vali Loss: 0.5077504 Test Loss: 0.3514032
Validation loss decreased (0.518875 --> 0.507750).  Saving model ...
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.3642004 | orig_loss: 0.3029420 | penalty: 0.3032003
	speed: 0.0412s/iter; left time: 3277.3800s
	iters: 200, epoch: 4 | loss: 0.2564780 | orig_loss: 0.3014700 | penalty: 0.3015702
	speed: 0.0190s/iter; left time: 1511.5275s
Epoch: 4 cost time: 5.2471983432769775
Epoch Losses - Original: 0.3045605, With Penalty: 0.3047800, Difference: 0.0002196 (0.07%)
Epoch: 4, Steps: 268 | Train Loss: 0.3103392 Vali Loss: 0.5108233 Test Loss: 0.3483142
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.2872378 | orig_loss: 0.3023383 | penalty: 0.3028036
	speed: 0.0404s/iter; left time: 3199.7739s
	iters: 200, epoch: 5 | loss: 0.2638353 | orig_loss: 0.2978678 | penalty: 0.2981319
	speed: 0.0185s/iter; left time: 1465.5802s
Epoch: 5 cost time: 5.097736835479736
Epoch Losses - Original: 0.3001614, With Penalty: 0.3004681, Difference: 0.0003067 (0.10%)
Epoch: 5, Steps: 268 | Train Loss: 0.2755365 Vali Loss: 0.5054315 Test Loss: 0.3518313
Validation loss decreased (0.507750 --> 0.505431).  Saving model ...
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.2974496 | orig_loss: 0.2963387 | penalty: 0.2967076
	speed: 0.0402s/iter; left time: 3175.8894s
	iters: 200, epoch: 6 | loss: 0.2834260 | orig_loss: 0.2932563 | penalty: 0.2935924
	speed: 0.0185s/iter; left time: 1461.1583s

------ Mask Loss Comparison ------
Original Loss: 0.3108766
Best Mask Loss: 0.3085118 (mask length: 3)
Improvement: 0.0023648 (0.76%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3120590
---------------------------------

Epoch: 6 cost time: 5.1175384521484375
Epoch Losses - Original: 0.2941764, With Penalty: 0.2945282, Difference: 0.0003517 (0.12%)
Epoch: 6, Steps: 268 | Train Loss: 0.2904378 Vali Loss: 0.5059985 Test Loss: 0.3557894
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.3545229 | orig_loss: 0.2909498 | penalty: 0.2911386
	speed: 0.0407s/iter; left time: 3205.0713s
	iters: 200, epoch: 7 | loss: 0.2671940 | orig_loss: 0.2884152 | penalty: 0.2888356
	speed: 0.0190s/iter; left time: 1489.6849s
Epoch: 7 cost time: 5.203315734863281
Epoch Losses - Original: 0.2883900, With Penalty: 0.2888259, Difference: 0.0004359 (0.15%)
Epoch: 7, Steps: 268 | Train Loss: 0.3108584 Vali Loss: 0.5055263 Test Loss: 0.3507798
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.3064354 | orig_loss: 0.2884310 | penalty: 0.2887858
	speed: 0.0399s/iter; left time: 3132.3607s
	iters: 200, epoch: 8 | loss: 0.2526701 | orig_loss: 0.2787618 | penalty: 0.2790052
	speed: 0.0190s/iter; left time: 1489.2458s
Epoch: 8 cost time: 5.167710065841675
Epoch Losses - Original: 0.2834715, With Penalty: 0.2838085, Difference: 0.0003370 (0.12%)
Epoch: 8, Steps: 268 | Train Loss: 0.2795528 Vali Loss: 0.4986609 Test Loss: 0.3545359
Validation loss decreased (0.505431 --> 0.498661).  Saving model ...
Updating learning rate to 0.0002994739288874256
	iters: 100, epoch: 9 | loss: 0.2786992 | orig_loss: 0.2796498 | penalty: 0.2799430
	speed: 0.0402s/iter; left time: 3138.2237s
	iters: 200, epoch: 9 | loss: 0.2880606 | orig_loss: 0.2753279 | penalty: 0.2758842
	speed: 0.0185s/iter; left time: 1442.9078s
Epoch: 9 cost time: 5.085549831390381
Epoch Losses - Original: 0.2798038, With Penalty: 0.2806112, Difference: 0.0008074 (0.29%)
Epoch: 9, Steps: 268 | Train Loss: 0.2833799 Vali Loss: 0.5167333 Test Loss: 0.3579345
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299334294690462
	iters: 100, epoch: 10 | loss: 0.2924336 | orig_loss: 0.2855214 | penalty: 0.2859066
	speed: 0.0399s/iter; left time: 3109.3607s
	iters: 200, epoch: 10 | loss: 0.2471610 | orig_loss: 0.2764769 | penalty: 0.2766163
	speed: 0.0185s/iter; left time: 1439.8898s
Epoch: 10 cost time: 5.0805957317352295
Epoch Losses - Original: 0.2785455, With Penalty: 0.2788741, Difference: 0.0003285 (0.12%)
Epoch: 10, Steps: 268 | Train Loss: 0.2697973 Vali Loss: 0.5098664 Test Loss: 0.3514473
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029917828430524096
	iters: 100, epoch: 11 | loss: 0.2196062 | orig_loss: 0.2711447 | penalty: 0.2716266
	speed: 0.0402s/iter; left time: 3123.8998s
	iters: 200, epoch: 11 | loss: 0.2947928 | orig_loss: 0.2728144 | penalty: 0.2729370
	speed: 0.0186s/iter; left time: 1438.0390s
Epoch: 11 cost time: 5.1109778881073
Epoch Losses - Original: 0.2702772, With Penalty: 0.2705389, Difference: 0.0002616 (0.10%)
Epoch: 11, Steps: 268 | Train Loss: 0.2571995 Vali Loss: 0.5064998 Test Loss: 0.3640602
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh1, seq_len: 48, pred_len: 48
Avg Original Loss: 0.2975112
Avg Penalty Loss: 0.2978461
Avg Difference: 0.0003349 (0.11%)
================================

>>>>>>>testing : long_term_forecast_ETTh1_48_48_SOFTS_ETTh1_ftM_sl48_ll48_pl48_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2833
mse:0.35453587967180505, mae:0.3807691102035467
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_48_72', model='SOFTS', data='ETTh1', root_path='./dataset/ETT/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=72, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh1_48_72_SOFTS_ETTh1_ftM_sl48_ll48_pl72_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8521
val 2809
test 2809

------ Mask Loss Comparison ------
Original Loss: 0.6763204
Best Mask Loss: 0.6664174 (mask length: 3)
Improvement: 0.0099031 (1.46%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6812720
---------------------------------

	iters: 100, epoch: 1 | loss: 0.3107539 | orig_loss: 0.3992417 | penalty: 0.3995739
	speed: 0.0332s/iter; left time: 2656.9741s
	iters: 200, epoch: 1 | loss: 0.4034353 | orig_loss: 0.3629420 | penalty: 0.3632371
	speed: 0.0188s/iter; left time: 1505.3259s
Epoch: 1 cost time: 5.577853441238403
Epoch Losses - Original: 0.3762874, With Penalty: 0.3765673, Difference: 0.0002799 (0.07%)
Epoch: 1, Steps: 267 | Train Loss: 0.3570946 Vali Loss: 0.6209042 Test Loss: 0.3763492
Validation loss decreased (inf --> 0.620904).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.4112106 | orig_loss: 0.3534886 | penalty: 0.3536891
	speed: 0.0411s/iter; left time: 3274.5749s
	iters: 200, epoch: 2 | loss: 0.3461439 | orig_loss: 0.3551236 | penalty: 0.3552259
	speed: 0.0193s/iter; left time: 1534.6945s
Epoch: 2 cost time: 5.304877042770386
Epoch Losses - Original: 0.3535302, With Penalty: 0.3537504, Difference: 0.0002203 (0.06%)
Epoch: 2, Steps: 267 | Train Loss: 0.3786772 Vali Loss: 0.6159728 Test Loss: 0.3719113
Validation loss decreased (0.620904 --> 0.615973).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.4731243 | orig_loss: 0.3474246 | penalty: 0.3475070
	speed: 0.0409s/iter; left time: 3252.1074s
	iters: 200, epoch: 3 | loss: 0.2925144 | orig_loss: 0.3400567 | penalty: 0.3406009
	speed: 0.0188s/iter; left time: 1494.8807s
Epoch: 3 cost time: 5.173914909362793
Epoch Losses - Original: 0.3443280, With Penalty: 0.3446908, Difference: 0.0003628 (0.11%)
Epoch: 3, Steps: 267 | Train Loss: 0.3828194 Vali Loss: 0.6088827 Test Loss: 0.3783689
Validation loss decreased (0.615973 --> 0.608883).  Saving model ...
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.3116070 | orig_loss: 0.3386169 | penalty: 0.3393436
	speed: 0.0404s/iter; left time: 3201.0932s

------ Mask Loss Comparison ------
Original Loss: 0.4287301
Best Mask Loss: 0.4242875 (mask length: 3)
Improvement: 0.0044426 (1.04%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4309514
---------------------------------

	iters: 200, epoch: 4 | loss: 0.3561908 | orig_loss: 0.3375248 | penalty: 0.3377829
	speed: 0.0188s/iter; left time: 1488.9322s
Epoch: 4 cost time: 5.17078709602356
Epoch Losses - Original: 0.3369421, With Penalty: 0.3374389, Difference: 0.0004969 (0.15%)
Epoch: 4, Steps: 267 | Train Loss: 0.3338989 Vali Loss: 0.6022105 Test Loss: 0.3735373
Validation loss decreased (0.608883 --> 0.602210).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.2911978 | orig_loss: 0.3270944 | penalty: 0.3276347
	speed: 0.0406s/iter; left time: 3205.2387s
	iters: 200, epoch: 5 | loss: 0.3669652 | orig_loss: 0.3315822 | penalty: 0.3320908
	speed: 0.0188s/iter; left time: 1479.6316s
Epoch: 5 cost time: 5.1659464836120605
Epoch Losses - Original: 0.3312998, With Penalty: 0.3317766, Difference: 0.0004768 (0.14%)
Epoch: 5, Steps: 267 | Train Loss: 0.3290815 Vali Loss: 0.6009483 Test Loss: 0.3796910
Validation loss decreased (0.602210 --> 0.600948).  Saving model ...
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.2952144 | orig_loss: 0.3241619 | penalty: 0.3244256
	speed: 0.0405s/iter; left time: 3187.9104s
	iters: 200, epoch: 6 | loss: 0.3411502 | orig_loss: 0.3312057 | penalty: 0.3318049
	speed: 0.0188s/iter; left time: 1475.9575s
Epoch: 6 cost time: 5.172439098358154
Epoch Losses - Original: 0.3264203, With Penalty: 0.3269536, Difference: 0.0005333 (0.16%)
Epoch: 6, Steps: 267 | Train Loss: 0.3181823 Vali Loss: 0.6021757 Test Loss: 0.3764437
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.2856812 | orig_loss: 0.3227091 | penalty: 0.3230268
	speed: 0.0403s/iter; left time: 3161.0958s
	iters: 200, epoch: 7 | loss: 0.3559425 | orig_loss: 0.3201952 | penalty: 0.3207585
	speed: 0.0188s/iter; left time: 1470.8428s
Epoch: 7 cost time: 5.145677089691162
Epoch Losses - Original: 0.3216489, With Penalty: 0.3220655, Difference: 0.0004166 (0.13%)
Epoch: 7, Steps: 267 | Train Loss: 0.3208118 Vali Loss: 0.6015319 Test Loss: 0.3805076
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.3400921 | orig_loss: 0.3191461 | penalty: 0.3195511
	speed: 0.0402s/iter; left time: 3143.7397s
	iters: 200, epoch: 8 | loss: 0.4135509 | orig_loss: 0.3131497 | penalty: 0.3138041
	speed: 0.0188s/iter; left time: 1464.5597s
Epoch: 8 cost time: 5.1536338329315186
Epoch Losses - Original: 0.3163602, With Penalty: 0.3167570, Difference: 0.0003968 (0.13%)
Epoch: 8, Steps: 267 | Train Loss: 0.3768215 Vali Loss: 0.6025843 Test Loss: 0.3861308
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh1, seq_len: 48, pred_len: 72
Avg Original Loss: 0.3383521
Avg Penalty Loss: 0.3387500
Avg Difference: 0.0003979 (0.12%)
================================

>>>>>>>testing : long_term_forecast_ETTh1_48_72_SOFTS_ETTh1_ftM_sl48_ll48_pl72_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2809
mse:0.37969096135991287, mae:0.39754239803703284
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_48_96', model='SOFTS', data='ETTh1', root_path='./dataset/ETT/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=96, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh1_48_96_SOFTS_ETTh1_ftM_sl48_ll48_pl96_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8497
val 2785
test 2785

------ Mask Loss Comparison ------
Original Loss: 0.6946704
Best Mask Loss: 0.6625137 (mask length: 6)
Improvement: 0.0321567 (4.63%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.7107488
---------------------------------

	iters: 100, epoch: 1 | loss: 0.3489209 | orig_loss: 0.4299590 | penalty: 0.4308880
	speed: 0.0333s/iter; left time: 2653.6408s
	iters: 200, epoch: 1 | loss: 0.3998481 | orig_loss: 0.3842557 | penalty: 0.3847146
	speed: 0.0190s/iter; left time: 1509.7539s
Epoch: 1 cost time: 5.58809232711792
Epoch Losses - Original: 0.4004274, With Penalty: 0.4011066, Difference: 0.0006791 (0.17%)
Epoch: 1, Steps: 266 | Train Loss: 0.3743845 Vali Loss: 0.7064955 Test Loss: 0.3951861
Validation loss decreased (inf --> 0.706496).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.3507405 | orig_loss: 0.3782663 | penalty: 0.3787459
	speed: 0.0416s/iter; left time: 3306.1403s

------ Mask Loss Comparison ------
Original Loss: 0.3731611
Best Mask Loss: 0.3680351 (mask length: 3)
Improvement: 0.0051260 (1.37%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3757241
---------------------------------

	iters: 200, epoch: 2 | loss: 0.4108655 | orig_loss: 0.3777093 | penalty: 0.3784080
	speed: 0.0189s/iter; left time: 1500.0622s
Epoch: 2 cost time: 5.160691022872925
Epoch Losses - Original: 0.3756152, With Penalty: 0.3761427, Difference: 0.0005275 (0.14%)
Epoch: 2, Steps: 266 | Train Loss: 0.3808030 Vali Loss: 0.6946682 Test Loss: 0.3914354
Validation loss decreased (0.706496 --> 0.694668).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.2916305 | orig_loss: 0.3693336 | penalty: 0.3700003
	speed: 0.0404s/iter; left time: 3200.5335s
	iters: 200, epoch: 3 | loss: 0.3384529 | orig_loss: 0.3664029 | penalty: 0.3669582
	speed: 0.0189s/iter; left time: 1493.3626s
Epoch: 3 cost time: 5.180430173873901
Epoch Losses - Original: 0.3661001, With Penalty: 0.3667795, Difference: 0.0006794 (0.19%)
Epoch: 3, Steps: 266 | Train Loss: 0.3150417 Vali Loss: 0.6950850 Test Loss: 0.3907271
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.3337746 | orig_loss: 0.3674931 | penalty: 0.3681660
	speed: 0.0411s/iter; left time: 3245.0582s
	iters: 200, epoch: 4 | loss: 0.4180139 | orig_loss: 0.3538475 | penalty: 0.3541790
	speed: 0.0194s/iter; left time: 1529.9014s
Epoch: 4 cost time: 5.312413930892944
Epoch Losses - Original: 0.3592252, With Penalty: 0.3598189, Difference: 0.0005937 (0.17%)
Epoch: 4, Steps: 266 | Train Loss: 0.3758943 Vali Loss: 0.6829370 Test Loss: 0.3968459
Validation loss decreased (0.694668 --> 0.682937).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.4062090 | orig_loss: 0.3584611 | penalty: 0.3591640
	speed: 0.0408s/iter; left time: 3209.7226s
	iters: 200, epoch: 5 | loss: 0.3227125 | orig_loss: 0.3499754 | penalty: 0.3505696
	speed: 0.0190s/iter; left time: 1490.9530s
Epoch: 5 cost time: 5.169862508773804
Epoch Losses - Original: 0.3534216, With Penalty: 0.3539933, Difference: 0.0005717 (0.16%)
Epoch: 5, Steps: 266 | Train Loss: 0.3644607 Vali Loss: 0.6846000 Test Loss: 0.3976619
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299794430213186

------ Mask Loss Comparison ------
Original Loss: 0.4194491
Best Mask Loss: 0.4116674 (mask length: 6)
Improvement: 0.0077817 (1.86%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4233400
---------------------------------

	iters: 100, epoch: 6 | loss: 0.3139205 | orig_loss: 0.3466765 | penalty: 0.3471918
	speed: 0.0407s/iter; left time: 3191.7828s
	iters: 200, epoch: 6 | loss: 0.3254511 | orig_loss: 0.3489412 | penalty: 0.3495667
	speed: 0.0190s/iter; left time: 1487.5947s
Epoch: 6 cost time: 5.208042144775391
Epoch Losses - Original: 0.3481210, With Penalty: 0.3487608, Difference: 0.0006399 (0.18%)
Epoch: 6, Steps: 266 | Train Loss: 0.3196858 Vali Loss: 0.6886336 Test Loss: 0.4018001
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.3606964 | orig_loss: 0.3337426 | penalty: 0.3339509
	speed: 0.0411s/iter; left time: 3207.1091s
	iters: 200, epoch: 7 | loss: 0.3197409 | orig_loss: 0.3439326 | penalty: 0.3445696
	speed: 0.0190s/iter; left time: 1484.8554s
Epoch: 7 cost time: 5.245880365371704
Epoch Losses - Original: 0.3438606, With Penalty: 0.3444026, Difference: 0.0005419 (0.16%)
Epoch: 7, Steps: 266 | Train Loss: 0.3402187 Vali Loss: 0.6883977 Test Loss: 0.4024800
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh1, seq_len: 48, pred_len: 96
Avg Original Loss: 0.3638245
Avg Penalty Loss: 0.3644292
Avg Difference: 0.0006047 (0.17%)
================================

>>>>>>>testing : long_term_forecast_ETTh1_48_96_SOFTS_ETTh1_ftM_sl48_ll48_pl96_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3968458990648362, mae:0.4074682182212706
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_48_144', model='SOFTS', data='ETTh1', root_path='./dataset/ETT/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=144, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh1_48_144_SOFTS_ETTh1_ftM_sl48_ll48_pl144_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8449
val 2737
test 2737

------ Mask Loss Comparison ------
Original Loss: 0.6882842
Best Mask Loss: 0.6773845 (mask length: 3)
Improvement: 0.0108997 (1.58%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6937340
---------------------------------

	iters: 100, epoch: 1 | loss: 0.4093786 | orig_loss: 0.4680319 | penalty: 0.4697373
	speed: 0.0343s/iter; left time: 2725.5926s
	iters: 200, epoch: 1 | loss: 0.4976701 | orig_loss: 0.4176355 | penalty: 0.4190652
	speed: 0.0197s/iter; left time: 1558.4128s

------ Mask Loss Comparison ------
Original Loss: 0.3958465
Best Mask Loss: 0.3957588 (mask length: 3)
Improvement: 0.0000877 (0.02%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3958903
---------------------------------

Epoch: 1 cost time: 5.769957542419434
Epoch Losses - Original: 0.4354112, With Penalty: 0.4367843, Difference: 0.0013731 (0.32%)
Epoch: 1, Steps: 265 | Train Loss: 0.4535243 Vali Loss: 0.8682636 Test Loss: 0.4348254
Validation loss decreased (inf --> 0.868264).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.4484987 | orig_loss: 0.4155444 | penalty: 0.4166735
	speed: 0.0412s/iter; left time: 3260.5193s
	iters: 200, epoch: 2 | loss: 0.4643926 | orig_loss: 0.4149506 | penalty: 0.4162160
	speed: 0.0195s/iter; left time: 1541.6136s
Epoch: 2 cost time: 5.2969443798065186
Epoch Losses - Original: 0.4111768, With Penalty: 0.4122028, Difference: 0.0010260 (0.25%)
Epoch: 2, Steps: 265 | Train Loss: 0.4564457 Vali Loss: 0.8568605 Test Loss: 0.4264999
Validation loss decreased (0.868264 --> 0.856861).  Saving model ...
Updating learning rate to 0.0002999671025212268

------ Mask Loss Comparison ------
Original Loss: 0.4524377
Best Mask Loss: 0.4463668 (mask length: 3)
Improvement: 0.0060709 (1.34%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4554732
---------------------------------

	iters: 100, epoch: 3 | loss: 0.3619325 | orig_loss: 0.4049900 | penalty: 0.4062081
	speed: 0.0419s/iter; left time: 3302.2440s
	iters: 200, epoch: 3 | loss: 0.4260673 | orig_loss: 0.4004605 | penalty: 0.4011739
	speed: 0.0192s/iter; left time: 1511.7642s
Epoch: 3 cost time: 5.271074056625366
Epoch Losses - Original: 0.4034092, With Penalty: 0.4044147, Difference: 0.0010055 (0.25%)
Epoch: 3, Steps: 265 | Train Loss: 0.3939999 Vali Loss: 0.8571653 Test Loss: 0.4278289
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.3132929 | orig_loss: 0.4044661 | penalty: 0.4053758
	speed: 0.0407s/iter; left time: 3202.9160s
	iters: 200, epoch: 4 | loss: 0.3443603 | orig_loss: 0.3936156 | penalty: 0.3944838
	speed: 0.0194s/iter; left time: 1525.1199s
Epoch: 4 cost time: 5.266418218612671
Epoch Losses - Original: 0.3949711, With Penalty: 0.3958202, Difference: 0.0008492 (0.21%)
Epoch: 4, Steps: 265 | Train Loss: 0.3288266 Vali Loss: 0.8588630 Test Loss: 0.4289579
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029986842451482874

------ Mask Loss Comparison ------
Original Loss: 0.4437751
Best Mask Loss: 0.4310760 (mask length: 3)
Improvement: 0.0126991 (2.86%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4501247
---------------------------------

	iters: 100, epoch: 5 | loss: 0.4473936 | orig_loss: 0.3900176 | penalty: 0.3906253
	speed: 0.0409s/iter; left time: 3206.6818s
	iters: 200, epoch: 5 | loss: 0.4066837 | orig_loss: 0.3908159 | penalty: 0.3920429
	speed: 0.0190s/iter; left time: 1490.1646s
Epoch: 5 cost time: 5.197981119155884
Epoch Losses - Original: 0.3923731, With Penalty: 0.3942167, Difference: 0.0018435 (0.47%)
Epoch: 5, Steps: 265 | Train Loss: 0.4270386 Vali Loss: 0.8476237 Test Loss: 0.4274896
Validation loss decreased (0.856861 --> 0.847624).  Saving model ...
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.4216072 | orig_loss: 0.3892950 | penalty: 0.3899844
	speed: 0.0409s/iter; left time: 3196.9838s
	iters: 200, epoch: 6 | loss: 0.3922138 | orig_loss: 0.3817824 | penalty: 0.3821770
	speed: 0.0194s/iter; left time: 1516.2989s
Epoch: 6 cost time: 5.244858264923096
Epoch Losses - Original: 0.3852720, With Penalty: 0.3860170, Difference: 0.0007450 (0.19%)
Epoch: 6, Steps: 265 | Train Loss: 0.4069105 Vali Loss: 0.8457652 Test Loss: 0.4307964
Validation loss decreased (0.847624 --> 0.845765).  Saving model ...
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.3756029 | orig_loss: 0.3837122 | penalty: 0.3844306
	speed: 0.0408s/iter; left time: 3175.8215s
	iters: 200, epoch: 7 | loss: 0.3926704 | orig_loss: 0.3720598 | penalty: 0.3723529
	speed: 0.0190s/iter; left time: 1476.3968s
Epoch: 7 cost time: 5.202677011489868
Epoch Losses - Original: 0.3797233, With Penalty: 0.3802899, Difference: 0.0005666 (0.15%)
Epoch: 7, Steps: 265 | Train Loss: 0.3841366 Vali Loss: 0.8436678 Test Loss: 0.4248366
Validation loss decreased (0.845765 --> 0.843668).  Saving model ...
Updating learning rate to 0.0002995971715836687

------ Mask Loss Comparison ------
Original Loss: 0.4411387
Best Mask Loss: 0.4389628 (mask length: 3)
Improvement: 0.0021759 (0.49%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4422267
---------------------------------

	iters: 100, epoch: 8 | loss: 0.3846275 | orig_loss: 0.3796882 | penalty: 0.3802170
	speed: 0.0411s/iter; left time: 3188.6571s
	iters: 200, epoch: 8 | loss: 0.3082314 | orig_loss: 0.3755761 | penalty: 0.3761355
	speed: 0.0191s/iter; left time: 1476.2472s
Epoch: 8 cost time: 5.2348480224609375
Epoch Losses - Original: 0.3761464, With Penalty: 0.3767847, Difference: 0.0006383 (0.17%)
Epoch: 8, Steps: 265 | Train Loss: 0.3464295 Vali Loss: 0.8414862 Test Loss: 0.4272905
Validation loss decreased (0.843668 --> 0.841486).  Saving model ...
Updating learning rate to 0.0002994739288874256
	iters: 100, epoch: 9 | loss: 0.3777101 | orig_loss: 0.3760441 | penalty: 0.3766995
	speed: 0.0408s/iter; left time: 3149.8880s
	iters: 200, epoch: 9 | loss: 0.3760146 | orig_loss: 0.3712360 | penalty: 0.3718451
	speed: 0.0191s/iter; left time: 1473.4647s
Epoch: 9 cost time: 5.209604263305664
Epoch Losses - Original: 0.3727838, With Penalty: 0.3734669, Difference: 0.0006831 (0.18%)
Epoch: 9, Steps: 265 | Train Loss: 0.3768624 Vali Loss: 0.8415798 Test Loss: 0.4363024
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299334294690462
	iters: 100, epoch: 10 | loss: 0.3348634 | orig_loss: 0.3697500 | penalty: 0.3700339
	speed: 0.0410s/iter; left time: 3160.1100s
	iters: 200, epoch: 10 | loss: 0.4746203 | orig_loss: 0.3708949 | penalty: 0.3715310
	speed: 0.0191s/iter; left time: 1468.8314s
Epoch: 10 cost time: 5.237587213516235
Epoch Losses - Original: 0.3691692, With Penalty: 0.3695757, Difference: 0.0004065 (0.11%)
Epoch: 10, Steps: 265 | Train Loss: 0.4047419 Vali Loss: 0.8537171 Test Loss: 0.4374588
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029917828430524096
	iters: 100, epoch: 11 | loss: 0.3460397 | orig_loss: 0.3663745 | penalty: 0.3670160
	speed: 0.0407s/iter; left time: 3123.2044s
	iters: 200, epoch: 11 | loss: 0.4125509 | orig_loss: 0.3641506 | penalty: 0.3647999
	speed: 0.0194s/iter; left time: 1485.4742s
Epoch: 11 cost time: 5.274343013763428
Epoch Losses - Original: 0.3653962, With Penalty: 0.3659920, Difference: 0.0005958 (0.16%)
Epoch: 11, Steps: 265 | Train Loss: 0.3792953 Vali Loss: 0.8482067 Test Loss: 0.4355219
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh1, seq_len: 48, pred_len: 144
Avg Original Loss: 0.3896211
Avg Penalty Loss: 0.3905059
Avg Difference: 0.0008848 (0.23%)
================================

>>>>>>>testing : long_term_forecast_ETTh1_48_144_SOFTS_ETTh1_ftM_sl48_ll48_pl144_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2737
mse:0.42729054446371967, mae:0.42600746405442613
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_48_192', model='SOFTS', data='ETTh1', root_path='./dataset/ETT/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=192, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh1_48_192_SOFTS_ETTh1_ftM_sl48_ll48_pl192_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8401
val 2689
test 2689

------ Mask Loss Comparison ------
Original Loss: 0.6732689
Best Mask Loss: 0.6580580 (mask length: 6)
Improvement: 0.0152109 (2.26%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6808743
---------------------------------

	iters: 100, epoch: 1 | loss: 0.4624861 | orig_loss: 0.4932195 | penalty: 0.4951777
	speed: 0.0338s/iter; left time: 2662.6983s

------ Mask Loss Comparison ------
Original Loss: 0.3921417
Best Mask Loss: 0.3906726 (mask length: 6)
Improvement: 0.0014691 (0.37%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3928762
---------------------------------

	iters: 200, epoch: 1 | loss: 0.4609668 | orig_loss: 0.4474343 | penalty: 0.4490731
	speed: 0.0192s/iter; left time: 1512.8891s
Epoch: 1 cost time: 5.6459643840789795
Epoch Losses - Original: 0.4679644, With Penalty: 0.4700341, Difference: 0.0020697 (0.44%)
Epoch: 1, Steps: 263 | Train Loss: 0.4617264 Vali Loss: 1.0206649 Test Loss: 0.4531146
Validation loss decreased (inf --> 1.020665).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.3761298 | orig_loss: 0.4425199 | penalty: 0.4438561
	speed: 0.0411s/iter; left time: 3225.1039s

------ Mask Loss Comparison ------
Original Loss: 0.5283123
Best Mask Loss: 0.5236962 (mask length: 15)
Improvement: 0.0046161 (0.87%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5306203
---------------------------------

	iters: 200, epoch: 2 | loss: 0.4309733 | orig_loss: 0.4479137 | penalty: 0.4496141
	speed: 0.0192s/iter; left time: 1503.9703s
Epoch: 2 cost time: 5.2040934562683105
Epoch Losses - Original: 0.4435899, With Penalty: 0.4449627, Difference: 0.0013728 (0.31%)
Epoch: 2, Steps: 263 | Train Loss: 0.4035515 Vali Loss: 1.0040268 Test Loss: 0.4495204
Validation loss decreased (1.020665 --> 1.004027).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.4741563 | orig_loss: 0.4393658 | penalty: 0.4406452
	speed: 0.0412s/iter; left time: 3224.3635s

------ Mask Loss Comparison ------
Original Loss: 0.5083635
Best Mask Loss: 0.4967060 (mask length: 12)
Improvement: 0.0116575 (2.29%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5141922
---------------------------------

	iters: 200, epoch: 3 | loss: 0.3455453 | orig_loss: 0.4299188 | penalty: 0.4308393
	speed: 0.0194s/iter; left time: 1513.6681s
Epoch: 3 cost time: 5.247349262237549
Epoch Losses - Original: 0.4339851, With Penalty: 0.4350725, Difference: 0.0010874 (0.25%)
Epoch: 3, Steps: 263 | Train Loss: 0.4098508 Vali Loss: 1.0103388 Test Loss: 0.4532355
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.4565961 | orig_loss: 0.4310405 | penalty: 0.4323218
	speed: 0.0409s/iter; left time: 3192.9664s
	iters: 200, epoch: 4 | loss: 0.4580671 | orig_loss: 0.4197055 | penalty: 0.4206601
	speed: 0.0192s/iter; left time: 1492.3488s
Epoch: 4 cost time: 5.199024677276611
Epoch Losses - Original: 0.4266887, With Penalty: 0.4278137, Difference: 0.0011250 (0.26%)
Epoch: 4, Steps: 263 | Train Loss: 0.4573316 Vali Loss: 1.0069018 Test Loss: 0.4463383
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.3162102 | orig_loss: 0.4256623 | penalty: 0.4262424
	speed: 0.0413s/iter; left time: 3210.1668s

------ Mask Loss Comparison ------
Original Loss: 0.4052478
Best Mask Loss: 0.4016199 (mask length: 3)
Improvement: 0.0036280 (0.90%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4070618
---------------------------------

	iters: 200, epoch: 5 | loss: 0.4601623 | orig_loss: 0.4146577 | penalty: 0.4159013
	speed: 0.0194s/iter; left time: 1510.2121s
Epoch: 5 cost time: 5.257174491882324
Epoch Losses - Original: 0.4200384, With Penalty: 0.4209895, Difference: 0.0009511 (0.23%)
Epoch: 5, Steps: 263 | Train Loss: 0.3881862 Vali Loss: 1.0032240 Test Loss: 0.4483490
Validation loss decreased (1.004027 --> 1.003224).  Saving model ...
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.4923162 | orig_loss: 0.4185434 | penalty: 0.4192307
	speed: 0.0407s/iter; left time: 3156.8106s
	iters: 200, epoch: 6 | loss: 0.4179224 | orig_loss: 0.4118994 | penalty: 0.4133121
	speed: 0.0191s/iter; left time: 1477.7520s
Epoch: 6 cost time: 5.1629638671875
Epoch Losses - Original: 0.4152930, With Penalty: 0.4161568, Difference: 0.0008638 (0.21%)
Epoch: 6, Steps: 263 | Train Loss: 0.4551193 Vali Loss: 1.0096655 Test Loss: 0.4543692
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.4071496 | orig_loss: 0.4086507 | penalty: 0.4094060
	speed: 0.0406s/iter; left time: 3132.0890s
	iters: 200, epoch: 7 | loss: 0.3585705 | orig_loss: 0.4080803 | penalty: 0.4087553
	speed: 0.0196s/iter; left time: 1512.6216s
Epoch: 7 cost time: 5.244643211364746
Epoch Losses - Original: 0.4104618, With Penalty: 0.4110579, Difference: 0.0005961 (0.15%)
Epoch: 7, Steps: 263 | Train Loss: 0.3828600 Vali Loss: 0.9977071 Test Loss: 0.4503906
Validation loss decreased (1.003224 --> 0.997707).  Saving model ...
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.4122063 | orig_loss: 0.4055848 | penalty: 0.4060937
	speed: 0.0407s/iter; left time: 3135.4727s
	iters: 200, epoch: 8 | loss: 0.3503133 | orig_loss: 0.4026889 | penalty: 0.4033904
	speed: 0.0194s/iter; left time: 1488.8261s

------ Mask Loss Comparison ------
Original Loss: 0.5429690
Best Mask Loss: 0.4799416 (mask length: 3)
Improvement: 0.0630274 (11.61%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5744828
---------------------------------

Epoch: 8 cost time: 5.187465190887451
Epoch Losses - Original: 0.4059841, With Penalty: 0.4067677, Difference: 0.0007836 (0.19%)
Epoch: 8, Steps: 263 | Train Loss: 0.3812598 Vali Loss: 0.9921663 Test Loss: 0.4523012
Validation loss decreased (0.997707 --> 0.992166).  Saving model ...
Updating learning rate to 0.0002994739288874256
	iters: 100, epoch: 9 | loss: 0.3488756 | orig_loss: 0.3968341 | penalty: 0.3974731
	speed: 0.0407s/iter; left time: 3118.8282s
	iters: 200, epoch: 9 | loss: 0.3811090 | orig_loss: 0.4094624 | penalty: 0.4098877
	speed: 0.0195s/iter; left time: 1496.4738s
Epoch: 9 cost time: 5.227536916732788
Epoch Losses - Original: 0.4017017, With Penalty: 0.4021591, Difference: 0.0004574 (0.11%)
Epoch: 9, Steps: 263 | Train Loss: 0.3649923 Vali Loss: 0.9897836 Test Loss: 0.4531367
Validation loss decreased (0.992166 --> 0.989784).  Saving model ...
Updating learning rate to 0.000299334294690462
	iters: 100, epoch: 10 | loss: 0.3877567 | orig_loss: 0.3990131 | penalty: 0.3991771
	speed: 0.0407s/iter; left time: 3107.6031s
	iters: 200, epoch: 10 | loss: 0.4367124 | orig_loss: 0.3977577 | penalty: 0.3982453
	speed: 0.0192s/iter; left time: 1465.0081s
Epoch: 10 cost time: 5.187100172042847
Epoch Losses - Original: 0.3969659, With Penalty: 0.3972680, Difference: 0.0003020 (0.08%)
Epoch: 10, Steps: 263 | Train Loss: 0.4122346 Vali Loss: 0.9970215 Test Loss: 0.4507131
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029917828430524096
	iters: 100, epoch: 11 | loss: 0.4004601 | orig_loss: 0.3948302 | penalty: 0.3952164
	speed: 0.0412s/iter; left time: 3137.9453s
	iters: 200, epoch: 11 | loss: 0.4109252 | orig_loss: 0.3907918 | penalty: 0.3912314
	speed: 0.0193s/iter; left time: 1466.7270s
Epoch: 11 cost time: 5.242855548858643
Epoch Losses - Original: 0.3925914, With Penalty: 0.3929734, Difference: 0.0003819 (0.10%)
Epoch: 11, Steps: 263 | Train Loss: 0.4056927 Vali Loss: 0.9904922 Test Loss: 0.4521306
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002990059148400594
	iters: 100, epoch: 12 | loss: 0.4606983 | orig_loss: 0.3836304 | penalty: 0.3840657
	speed: 0.0409s/iter; left time: 3103.4114s
	iters: 200, epoch: 12 | loss: 0.4153454 | orig_loss: 0.3924550 | penalty: 0.3926880
	speed: 0.0193s/iter; left time: 1464.6844s
Epoch: 12 cost time: 5.202174186706543
Epoch Losses - Original: 0.3883650, With Penalty: 0.3887039, Difference: 0.0003389 (0.09%)
Epoch: 12, Steps: 263 | Train Loss: 0.4380218 Vali Loss: 0.9917258 Test Loss: 0.4483702
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh1, seq_len: 48, pred_len: 192
Avg Original Loss: 0.4169691
Avg Penalty Loss: 0.4178299
Avg Difference: 0.0008608 (0.21%)
================================

>>>>>>>testing : long_term_forecast_ETTh1_48_192_SOFTS_ETTh1_ftM_sl48_ll48_pl192_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4531367314309602, mae:0.4439547860564774
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_48_48', model='SOFTS', data='ETTh2', root_path='./dataset/ETT/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=48, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh2_48_48_SOFTS_ETTh2_ftM_sl48_ll48_pl48_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8545
val 2833
test 2833

------ Mask Loss Comparison ------
Original Loss: 0.6509397
Best Mask Loss: 0.5433925 (mask length: 6)
Improvement: 0.1075472 (16.52%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.7047133
---------------------------------

	iters: 100, epoch: 1 | loss: 0.3330741 | orig_loss: 0.3218378 | penalty: 0.3281844
	speed: 0.0338s/iter; left time: 2716.8125s
	iters: 200, epoch: 1 | loss: 0.2356244 | orig_loss: 0.3615658 | penalty: 0.3684362
	speed: 0.0194s/iter; left time: 1558.9350s

------ Mask Loss Comparison ------
Original Loss: 0.6018608
Best Mask Loss: 0.5638530 (mask length: 24)
Improvement: 0.0380077 (6.32%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6208646
---------------------------------

Epoch: 1 cost time: 5.768246650695801
Epoch Losses - Original: 0.3386744, With Penalty: 0.3444783, Difference: 0.0058039 (1.71%)
Epoch: 1, Steps: 268 | Train Loss: 0.2843492 Vali Loss: 0.1684537 Test Loss: 0.2366272
Validation loss decreased (inf --> 0.168454).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.3227106 | orig_loss: 0.3029038 | penalty: 0.3071392
	speed: 0.0418s/iter; left time: 3345.7386s
	iters: 200, epoch: 2 | loss: 0.3971503 | orig_loss: 0.3248823 | penalty: 0.3291875
	speed: 0.0195s/iter; left time: 1555.5976s
Epoch: 2 cost time: 5.357759475708008
Epoch Losses - Original: 0.3171103, With Penalty: 0.3210536, Difference: 0.0039433 (1.24%)
Epoch: 2, Steps: 268 | Train Loss: 0.3599305 Vali Loss: 0.1691943 Test Loss: 0.2427371
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999671025212268

------ Mask Loss Comparison ------
Original Loss: 0.2489267
Best Mask Loss: 0.2471494 (mask length: 3)
Improvement: 0.0017773 (0.71%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2498153
---------------------------------

	iters: 100, epoch: 3 | loss: 0.2516065 | orig_loss: 0.2889693 | penalty: 0.2901694
	speed: 0.0415s/iter; left time: 3309.4540s
	iters: 200, epoch: 3 | loss: 0.1968645 | orig_loss: 0.2956713 | penalty: 0.2968311
	speed: 0.0189s/iter; left time: 1509.6088s
Epoch: 3 cost time: 5.259038686752319
Epoch Losses - Original: 0.3113835, With Penalty: 0.3133241, Difference: 0.0019406 (0.62%)
Epoch: 3, Steps: 268 | Train Loss: 0.2242355 Vali Loss: 0.1731125 Test Loss: 0.2677204
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.2961840 | orig_loss: 0.3139907 | penalty: 0.3146721
	speed: 0.0403s/iter; left time: 3206.5909s
	iters: 200, epoch: 4 | loss: 0.1938173 | orig_loss: 0.2764757 | penalty: 0.2772306
	speed: 0.0189s/iter; left time: 1498.3674s
Epoch: 4 cost time: 5.179053544998169
Epoch Losses - Original: 0.2920245, With Penalty: 0.2925888, Difference: 0.0005642 (0.19%)
Epoch: 4, Steps: 268 | Train Loss: 0.2450007 Vali Loss: 0.1685785 Test Loss: 0.2484968
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh2, seq_len: 48, pred_len: 48
Avg Original Loss: 0.3147982
Avg Penalty Loss: 0.3178612
Avg Difference: 0.0030630 (0.97%)
================================

>>>>>>>testing : long_term_forecast_ETTh2_48_48_SOFTS_ETTh2_ftM_sl48_ll48_pl48_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2833
mse:0.2366272311906195, mae:0.30422084247039227
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_48_72', model='SOFTS', data='ETTh2', root_path='./dataset/ETT/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=72, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh2_48_72_SOFTS_ETTh2_ftM_sl48_ll48_pl72_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8521
val 2809
test 2809

------ Mask Loss Comparison ------
Original Loss: 0.2458730
Best Mask Loss: 0.2426488 (mask length: 3)
Improvement: 0.0032241 (1.31%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2474850
---------------------------------

	iters: 100, epoch: 1 | loss: 0.2573658 | orig_loss: 0.4117034 | penalty: 0.4210807
	speed: 0.0331s/iter; left time: 2649.4622s

------ Mask Loss Comparison ------
Original Loss: 0.2561960
Best Mask Loss: 0.2487235 (mask length: 9)
Improvement: 0.0074725 (2.92%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2599322
---------------------------------

	iters: 200, epoch: 1 | loss: 0.2111544 | orig_loss: 0.4110170 | penalty: 0.4218537
	speed: 0.0189s/iter; left time: 1508.8780s
Epoch: 1 cost time: 5.580939531326294
Epoch Losses - Original: 0.4081622, With Penalty: 0.4173408, Difference: 0.0091787 (2.25%)
Epoch: 1, Steps: 267 | Train Loss: 0.2342601 Vali Loss: 0.2025007 Test Loss: 0.2756384
Validation loss decreased (inf --> 0.202501).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.4113019 | orig_loss: 0.3764379 | penalty: 0.3823104
	speed: 0.0405s/iter; left time: 3226.7372s

------ Mask Loss Comparison ------
Original Loss: 0.2466580
Best Mask Loss: 0.2280340 (mask length: 15)
Improvement: 0.0186240 (7.55%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2559700
---------------------------------

	iters: 200, epoch: 2 | loss: 0.4057103 | orig_loss: 0.3898817 | penalty: 0.3954414
	speed: 0.0188s/iter; left time: 1499.2780s
Epoch: 2 cost time: 5.161723613739014
Epoch Losses - Original: 0.3820323, With Penalty: 0.3878955, Difference: 0.0058631 (1.53%)
Epoch: 2, Steps: 267 | Train Loss: 0.4085061 Vali Loss: 0.1973274 Test Loss: 0.2809091
Validation loss decreased (0.202501 --> 0.197327).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.3265266 | orig_loss: 0.3664590 | penalty: 0.3718291
	speed: 0.0404s/iter; left time: 3212.7934s

------ Mask Loss Comparison ------
Original Loss: 0.5718527
Best Mask Loss: 0.5135286 (mask length: 6)
Improvement: 0.0583242 (10.20%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6010149
---------------------------------

	iters: 200, epoch: 3 | loss: 0.3012598 | orig_loss: 0.3554241 | penalty: 0.3584133
	speed: 0.0188s/iter; left time: 1492.8038s
Epoch: 3 cost time: 5.167463779449463
Epoch Losses - Original: 0.3574760, With Penalty: 0.3608146, Difference: 0.0033386 (0.93%)
Epoch: 3, Steps: 267 | Train Loss: 0.3138932 Vali Loss: 0.2028433 Test Loss: 0.2958274
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.1774949 | orig_loss: 0.3524990 | penalty: 0.3539079
	speed: 0.0403s/iter; left time: 3188.2453s
	iters: 200, epoch: 4 | loss: 0.2928318 | orig_loss: 0.3571312 | penalty: 0.3581423
	speed: 0.0188s/iter; left time: 1484.6992s
Epoch: 4 cost time: 5.146778583526611
Epoch Losses - Original: 0.3432287, With Penalty: 0.3445520, Difference: 0.0013233 (0.39%)
Epoch: 4, Steps: 267 | Train Loss: 0.2351633 Vali Loss: 0.1994700 Test Loss: 0.2831222
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.2183496 | orig_loss: 0.3146719 | penalty: 0.3152214
	speed: 0.0405s/iter; left time: 3193.1578s
	iters: 200, epoch: 5 | loss: 0.2792137 | orig_loss: 0.3336558 | penalty: 0.3338995
	speed: 0.0188s/iter; left time: 1484.6133s
Epoch: 5 cost time: 5.158752202987671
Epoch Losses - Original: 0.3252177, With Penalty: 0.3256620, Difference: 0.0004443 (0.14%)
Epoch: 5, Steps: 267 | Train Loss: 0.2487816 Vali Loss: 0.1995949 Test Loss: 0.2894704
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh2, seq_len: 48, pred_len: 72
Avg Original Loss: 0.3632234
Avg Penalty Loss: 0.3672530
Avg Difference: 0.0040296 (1.11%)
================================

>>>>>>>testing : long_term_forecast_ETTh2_48_72_SOFTS_ETTh2_ftM_sl48_ll48_pl72_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2809
mse:0.2809091181454178, mae:0.3335418521657708
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_48_96', model='SOFTS', data='ETTh2', root_path='./dataset/ETT/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=96, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh2_48_96_SOFTS_ETTh2_ftM_sl48_ll48_pl96_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8497
val 2785
test 2785

------ Mask Loss Comparison ------
Original Loss: 0.5675319
Best Mask Loss: 0.5318457 (mask length: 12)
Improvement: 0.0356862 (6.29%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5853751
---------------------------------

	iters: 100, epoch: 1 | loss: 0.5237679 | orig_loss: 0.4509969 | penalty: 0.4618289
	speed: 0.0334s/iter; left time: 2663.3026s

------ Mask Loss Comparison ------
Original Loss: 0.7267000
Best Mask Loss: 0.7002180 (mask length: 12)
Improvement: 0.0264820 (3.64%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.7399410
---------------------------------

	iters: 200, epoch: 1 | loss: 1.1827552 | orig_loss: 0.4580933 | penalty: 0.4723281
	speed: 0.0190s/iter; left time: 1510.9386s
Epoch: 1 cost time: 5.608937978744507
Epoch Losses - Original: 0.4563700, With Penalty: 0.4683327, Difference: 0.0119627 (2.62%)
Epoch: 1, Steps: 266 | Train Loss: 0.8532616 Vali Loss: 0.2248292 Test Loss: 0.3080514
Validation loss decreased (inf --> 0.224829).  Saving model ...
Updating learning rate to 0.0002999917754048268

------ Mask Loss Comparison ------
Original Loss: 0.5627484
Best Mask Loss: 0.5605680 (mask length: 6)
Improvement: 0.0021805 (0.39%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5638387
---------------------------------

	iters: 100, epoch: 2 | loss: 0.6053055 | orig_loss: 0.4240525 | penalty: 0.4308183
	speed: 0.0408s/iter; left time: 3238.2689s
	iters: 200, epoch: 2 | loss: 0.4249982 | orig_loss: 0.4163540 | penalty: 0.4230049
	speed: 0.0189s/iter; left time: 1500.1536s

------ Mask Loss Comparison ------
Original Loss: 0.3769254
Best Mask Loss: 0.3213754 (mask length: 24)
Improvement: 0.0555500 (14.74%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4047004
---------------------------------

Epoch: 2 cost time: 5.193996906280518
Epoch Losses - Original: 0.4277436, With Penalty: 0.4353555, Difference: 0.0076120 (1.78%)
Epoch: 2, Steps: 266 | Train Loss: 0.5151518 Vali Loss: 0.2235015 Test Loss: 0.3181505
Validation loss decreased (0.224829 --> 0.223501).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.3091466 | orig_loss: 0.4080693 | penalty: 0.4124602
	speed: 0.0406s/iter; left time: 3215.7093s
	iters: 200, epoch: 3 | loss: 0.3893147 | orig_loss: 0.3946456 | penalty: 0.3980938
	speed: 0.0189s/iter; left time: 1493.1854s
Epoch: 3 cost time: 5.169872283935547
Epoch Losses - Original: 0.4023607, With Penalty: 0.4059879, Difference: 0.0036272 (0.90%)
Epoch: 3, Steps: 266 | Train Loss: 0.3492306 Vali Loss: 0.2323579 Test Loss: 0.3222792
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.2605891 | orig_loss: 0.3841668 | penalty: 0.3849715
	speed: 0.0404s/iter; left time: 3187.2760s
	iters: 200, epoch: 4 | loss: 0.2896267 | orig_loss: 0.3817026 | penalty: 0.3839798
	speed: 0.0189s/iter; left time: 1487.3437s

------ Mask Loss Comparison ------
Original Loss: 0.2792691
Best Mask Loss: 0.2712135 (mask length: 3)
Improvement: 0.0080556 (2.88%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2832969
---------------------------------

Epoch: 4 cost time: 5.165272235870361
Epoch Losses - Original: 0.3798057, With Penalty: 0.3813421, Difference: 0.0015363 (0.40%)
Epoch: 4, Steps: 266 | Train Loss: 0.2751079 Vali Loss: 0.2253483 Test Loss: 0.3148486
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.4253676 | orig_loss: 0.3698835 | penalty: 0.3700248
	speed: 0.0402s/iter; left time: 3164.0229s
	iters: 200, epoch: 5 | loss: 0.4351677 | orig_loss: 0.3589736 | penalty: 0.3594140
	speed: 0.0188s/iter; left time: 1478.8593s
Epoch: 5 cost time: 5.148480176925659
Epoch Losses - Original: 0.3614284, With Penalty: 0.3618821, Difference: 0.0004537 (0.13%)
Epoch: 5, Steps: 266 | Train Loss: 0.4302677 Vali Loss: 0.2268652 Test Loss: 0.3179796
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh2, seq_len: 48, pred_len: 96
Avg Original Loss: 0.4055417
Avg Penalty Loss: 0.4105801
Avg Difference: 0.0050384 (1.24%)
================================

>>>>>>>testing : long_term_forecast_ETTh2_48_96_SOFTS_ETTh2_ftM_sl48_ll48_pl96_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.31815045264882286, mae:0.35634619206778667
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_48_144', model='SOFTS', data='ETTh2', root_path='./dataset/ETT/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=144, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh2_48_144_SOFTS_ETTh2_ftM_sl48_ll48_pl144_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8449
val 2737
test 2737

------ Mask Loss Comparison ------
Original Loss: 1.0572425
Best Mask Loss: 0.8863550 (mask length: 15)
Improvement: 0.1708875 (16.16%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 1.1426862
---------------------------------

	iters: 100, epoch: 1 | loss: 0.6075190 | orig_loss: 0.5654283 | penalty: 0.5844846
	speed: 0.0336s/iter; left time: 2670.5713s

------ Mask Loss Comparison ------
Original Loss: 1.0418035
Best Mask Loss: 0.9879838 (mask length: 12)
Improvement: 0.0538197 (5.17%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 1.0687133
---------------------------------

	iters: 200, epoch: 1 | loss: 0.4758230 | orig_loss: 0.5158878 | penalty: 0.5330746
	speed: 0.0191s/iter; left time: 1518.5638s

------ Mask Loss Comparison ------
Original Loss: 0.8301179
Best Mask Loss: 0.7983627 (mask length: 9)
Improvement: 0.0317552 (3.83%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.8459955
---------------------------------

Epoch: 1 cost time: 5.639181852340698
Epoch Losses - Original: 0.5263650, With Penalty: 0.5436194, Difference: 0.0172544 (3.28%)
Epoch: 1, Steps: 265 | Train Loss: 0.5416710 Vali Loss: 0.2628414 Test Loss: 0.3538753
Validation loss decreased (inf --> 0.262841).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.4213427 | orig_loss: 0.4991546 | penalty: 0.5131388
	speed: 0.0409s/iter; left time: 3233.0117s

------ Mask Loss Comparison ------
Original Loss: 0.4563095
Best Mask Loss: 0.4321568 (mask length: 3)
Improvement: 0.0241527 (5.29%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4683858
---------------------------------

	iters: 200, epoch: 2 | loss: 0.2877350 | orig_loss: 0.4892709 | penalty: 0.4980410
	speed: 0.0191s/iter; left time: 1509.7365s
Epoch: 2 cost time: 5.2177300453186035
Epoch Losses - Original: 0.4980620, With Penalty: 0.5088601, Difference: 0.0107981 (2.17%)
Epoch: 2, Steps: 265 | Train Loss: 0.3545388 Vali Loss: 0.2644629 Test Loss: 0.3554232
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999671025212268

------ Mask Loss Comparison ------
Original Loss: 0.4161724
Best Mask Loss: 0.4087578 (mask length: 6)
Improvement: 0.0074147 (1.78%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4198798
---------------------------------

	iters: 100, epoch: 3 | loss: 0.4253634 | orig_loss: 0.4961212 | penalty: 0.5057180
	speed: 0.0408s/iter; left time: 3214.5014s
	iters: 200, epoch: 3 | loss: 0.3341640 | orig_loss: 0.4837148 | penalty: 0.4877903
	speed: 0.0195s/iter; left time: 1534.0733s
Epoch: 3 cost time: 5.254171133041382
Epoch Losses - Original: 0.4757284, With Penalty: 0.4819317, Difference: 0.0062033 (1.30%)
Epoch: 3, Steps: 265 | Train Loss: 0.3797637 Vali Loss: 0.2684769 Test Loss: 0.3631957
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002999259840548597

------ Mask Loss Comparison ------
Original Loss: 0.6646366
Best Mask Loss: 0.6395143 (mask length: 3)
Improvement: 0.0251223 (3.78%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6771977
---------------------------------

	iters: 100, epoch: 4 | loss: 0.5805726 | orig_loss: 0.4511978 | penalty: 0.4564716
	speed: 0.0411s/iter; left time: 3233.0432s
	iters: 200, epoch: 4 | loss: 0.7042867 | orig_loss: 0.4347539 | penalty: 0.4378052
	speed: 0.0193s/iter; left time: 1514.6373s
Epoch: 4 cost time: 5.262219429016113
Epoch Losses - Original: 0.4546738, With Penalty: 0.4586365, Difference: 0.0039626 (0.87%)
Epoch: 4, Steps: 265 | Train Loss: 0.6424296 Vali Loss: 0.2692394 Test Loss: 0.3619161
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh2, seq_len: 48, pred_len: 144
Avg Original Loss: 0.4887073
Avg Penalty Loss: 0.4982619
Avg Difference: 0.0095546 (1.96%)
================================

>>>>>>>testing : long_term_forecast_ETTh2_48_144_SOFTS_ETTh2_ftM_sl48_ll48_pl144_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2737
mse:0.3538753405066359, mae:0.3753552417228986
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_48_192', model='SOFTS', data='ETTh2', root_path='./dataset/ETT/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=192, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTh2_48_192_SOFTS_ETTh2_ftM_sl48_ll48_pl192_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8401
val 2689
test 2689

------ Mask Loss Comparison ------
Original Loss: 0.7116410
Best Mask Loss: 0.6157588 (mask length: 9)
Improvement: 0.0958822 (13.47%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.7595821
---------------------------------

	iters: 100, epoch: 1 | loss: 0.8967873 | orig_loss: 0.5914859 | penalty: 0.6143060
	speed: 0.0340s/iter; left time: 2677.1000s

------ Mask Loss Comparison ------
Original Loss: 0.3306216
Best Mask Loss: 0.3169839 (mask length: 12)
Improvement: 0.0136378 (4.12%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3374405
---------------------------------

	iters: 200, epoch: 1 | loss: 0.5515134 | orig_loss: 0.6023123 | penalty: 0.6303155
	speed: 0.0195s/iter; left time: 1536.7371s

------ Mask Loss Comparison ------
Original Loss: 0.3401426
Best Mask Loss: 0.3192615 (mask length: 3)
Improvement: 0.0208811 (6.14%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3505831
---------------------------------

Epoch: 1 cost time: 5.66872239112854
Epoch Losses - Original: 0.5828570, With Penalty: 0.6067196, Difference: 0.0238626 (4.09%)
Epoch: 1, Steps: 263 | Train Loss: 0.7241504 Vali Loss: 0.2871248 Test Loss: 0.3904024
Validation loss decreased (inf --> 0.287125).  Saving model ...
Updating learning rate to 0.0002999917754048268

------ Mask Loss Comparison ------
Original Loss: 0.3223706
Best Mask Loss: 0.3064058 (mask length: 6)
Improvement: 0.0159648 (4.95%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3303531
---------------------------------

	iters: 100, epoch: 2 | loss: 0.6904343 | orig_loss: 0.5772835 | penalty: 0.5993695
	speed: 0.0414s/iter; left time: 3248.0129s

------ Mask Loss Comparison ------
Original Loss: 0.9832694
Best Mask Loss: 0.9470346 (mask length: 3)
Improvement: 0.0362348 (3.69%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 1.0013868
---------------------------------

	iters: 200, epoch: 2 | loss: 0.7700431 | orig_loss: 0.5389018 | penalty: 0.5553365
	speed: 0.0194s/iter; left time: 1524.2073s
Epoch: 2 cost time: 5.31904935836792
Epoch Losses - Original: 0.5564475, With Penalty: 0.5743354, Difference: 0.0178879 (3.21%)
Epoch: 2, Steps: 263 | Train Loss: 0.7302387 Vali Loss: 0.2897387 Test Loss: 0.3944914
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999671025212268

------ Mask Loss Comparison ------
Original Loss: 0.5514743
Best Mask Loss: 0.5330753 (mask length: 6)
Improvement: 0.0183990 (3.34%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5606738
---------------------------------

	iters: 100, epoch: 3 | loss: 0.5316188 | orig_loss: 0.5420586 | penalty: 0.5540506
	speed: 0.0408s/iter; left time: 3196.9933s
	iters: 200, epoch: 3 | loss: 0.4502499 | orig_loss: 0.5405735 | penalty: 0.5520495
	speed: 0.0193s/iter; left time: 1507.2272s
Epoch: 3 cost time: 5.193508863449097
Epoch Losses - Original: 0.5358755, With Penalty: 0.5471171, Difference: 0.0112416 (2.10%)
Epoch: 3, Steps: 263 | Train Loss: 0.4909344 Vali Loss: 0.2928765 Test Loss: 0.3967796
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002999259840548597

------ Mask Loss Comparison ------
Original Loss: 0.6829973
Best Mask Loss: 0.5943364 (mask length: 18)
Improvement: 0.0886610 (12.98%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.7273278
---------------------------------

	iters: 100, epoch: 4 | loss: 0.3645021 | orig_loss: 0.5171267 | penalty: 0.5242887
	speed: 0.0410s/iter; left time: 3196.5842s
	iters: 200, epoch: 4 | loss: 0.5204037 | orig_loss: 0.5211501 | penalty: 0.5280715
	speed: 0.0191s/iter; left time: 1487.4100s
Epoch: 4 cost time: 5.179666519165039
Epoch Losses - Original: 0.5139840, With Penalty: 0.5205260, Difference: 0.0065420 (1.27%)
Epoch: 4, Steps: 263 | Train Loss: 0.4424529 Vali Loss: 0.2966236 Test Loss: 0.4096403
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTh2, seq_len: 48, pred_len: 192
Avg Original Loss: 0.5472910
Avg Penalty Loss: 0.5621745
Avg Difference: 0.0148835 (2.72%)
================================

>>>>>>>testing : long_term_forecast_ETTh2_48_192_SOFTS_ETTh2_ftM_sl48_ll48_pl192_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3904023958919393, mae:0.3987248714593495
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm1_48_48', model='SOFTS', data='ETTm1', root_path='./dataset/ETT/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=48, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm1_48_48_SOFTS_ETTm1_ftM_sl48_ll48_pl48_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34465
val 11473
test 11473

------ Mask Loss Comparison ------
Original Loss: 0.5282359
Best Mask Loss: 0.5190362 (mask length: 3)
Improvement: 0.0091996 (1.74%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5328357
---------------------------------

	iters: 100, epoch: 1 | loss: 0.3649814 | orig_loss: 0.4488968 | penalty: 0.4500013
	speed: 0.0334s/iter; left time: 10802.7385s
	iters: 200, epoch: 1 | loss: 0.5108365 | orig_loss: 0.4000495 | penalty: 0.4000495
	speed: 0.0190s/iter; left time: 6149.2887s
	iters: 300, epoch: 1 | loss: 0.2862757 | orig_loss: 0.3918454 | penalty: 0.3920278
	speed: 0.0190s/iter; left time: 6139.6319s
	iters: 400, epoch: 1 | loss: 0.3909964 | orig_loss: 0.3781322 | penalty: 0.3781322
	speed: 0.0190s/iter; left time: 6137.6038s
	iters: 500, epoch: 1 | loss: 0.3985609 | orig_loss: 0.3632040 | penalty: 0.3632040
	speed: 0.0190s/iter; left time: 6139.7300s
	iters: 600, epoch: 1 | loss: 0.3945883 | orig_loss: 0.3739386 | penalty: 0.3740185
	speed: 0.0190s/iter; left time: 6133.9474s
	iters: 700, epoch: 1 | loss: 0.2818926 | orig_loss: 0.3674759 | penalty: 0.3674759
	speed: 0.0190s/iter; left time: 6121.7282s
	iters: 800, epoch: 1 | loss: 0.3326554 | orig_loss: 0.3615379 | penalty: 0.3615379
	speed: 0.0190s/iter; left time: 6119.4274s
	iters: 900, epoch: 1 | loss: 0.4223037 | orig_loss: 0.3636797 | penalty: 0.3636870
	speed: 0.0190s/iter; left time: 6119.5250s
	iters: 1000, epoch: 1 | loss: 0.3490714 | orig_loss: 0.3501426 | penalty: 0.3501426
	speed: 0.0190s/iter; left time: 6118.2194s
Epoch: 1 cost time: 21.042686939239502
Epoch Losses - Original: 0.3777563, With Penalty: 0.3778838, Difference: 0.0001275 (0.03%)
Epoch: 1, Steps: 1078 | Train Loss: 0.3732162 Vali Loss: 0.5666431 Test Loss: 0.5875384
Validation loss decreased (inf --> 0.566643).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.3228461 | orig_loss: 0.3474294 | penalty: 0.3475459
	speed: 0.0575s/iter; left time: 18512.6736s
	iters: 200, epoch: 2 | loss: 0.2805672 | orig_loss: 0.3413734 | penalty: 0.3413734
	speed: 0.0193s/iter; left time: 6204.5256s
	iters: 300, epoch: 2 | loss: 0.2635533 | orig_loss: 0.3382533 | penalty: 0.3382533
	speed: 0.0194s/iter; left time: 6257.9022s
	iters: 400, epoch: 2 | loss: 0.3522368 | orig_loss: 0.3514209 | penalty: 0.3514758
	speed: 0.0194s/iter; left time: 6251.5978s
	iters: 500, epoch: 2 | loss: 0.2751589 | orig_loss: 0.3333286 | penalty: 0.3333286
	speed: 0.0195s/iter; left time: 6266.9673s
	iters: 600, epoch: 2 | loss: 0.3598991 | orig_loss: 0.3432148 | penalty: 0.3432148
	speed: 0.0194s/iter; left time: 6256.8356s
	iters: 700, epoch: 2 | loss: 0.3275824 | orig_loss: 0.3410142 | penalty: 0.3410142
	speed: 0.0195s/iter; left time: 6258.0110s
	iters: 800, epoch: 2 | loss: 0.2656127 | orig_loss: 0.3354772 | penalty: 0.3354865
	speed: 0.0194s/iter; left time: 6248.2454s
	iters: 900, epoch: 2 | loss: 0.3066400 | orig_loss: 0.3267254 | penalty: 0.3267254
	speed: 0.0192s/iter; left time: 6160.9633s
	iters: 1000, epoch: 2 | loss: 0.2442808 | orig_loss: 0.3288547 | penalty: 0.3288547
	speed: 0.0190s/iter; left time: 6099.9057s
Epoch: 2 cost time: 20.948423385620117
Epoch Losses - Original: 0.3372975, With Penalty: 0.3373143, Difference: 0.0000168 (0.00%)
Epoch: 2, Steps: 1078 | Train Loss: 0.2998377 Vali Loss: 0.5245813 Test Loss: 0.5322404
Validation loss decreased (0.566643 --> 0.524581).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.3197943 | orig_loss: 0.3347821 | penalty: 0.3347821
	speed: 0.0573s/iter; left time: 18404.1927s
	iters: 200, epoch: 3 | loss: 0.3870957 | orig_loss: 0.3245434 | penalty: 0.3245434
	speed: 0.0190s/iter; left time: 6105.9122s
	iters: 300, epoch: 3 | loss: 0.2644224 | orig_loss: 0.3160107 | penalty: 0.3160107
	speed: 0.0189s/iter; left time: 6079.9802s
	iters: 400, epoch: 3 | loss: 0.2418961 | orig_loss: 0.3156227 | penalty: 0.3157673
	speed: 0.0189s/iter; left time: 6071.5607s
	iters: 500, epoch: 3 | loss: 0.2871779 | orig_loss: 0.3161282 | penalty: 0.3161282
	speed: 0.0190s/iter; left time: 6078.6637s
	iters: 600, epoch: 3 | loss: 0.2937224 | orig_loss: 0.3156336 | penalty: 0.3156375
	speed: 0.0190s/iter; left time: 6107.4623s
	iters: 700, epoch: 3 | loss: 0.2913953 | orig_loss: 0.3210294 | penalty: 0.3210294
	speed: 0.0190s/iter; left time: 6099.2289s
	iters: 800, epoch: 3 | loss: 0.3739454 | orig_loss: 0.3192757 | penalty: 0.3192757
	speed: 0.0190s/iter; left time: 6088.4454s
	iters: 900, epoch: 3 | loss: 0.3093846 | orig_loss: 0.3140686 | penalty: 0.3140686
	speed: 0.0190s/iter; left time: 6083.2536s
	iters: 1000, epoch: 3 | loss: 0.3117284 | orig_loss: 0.3087490 | penalty: 0.3087490
	speed: 0.0190s/iter; left time: 6087.4918s
Epoch: 3 cost time: 20.63265347480774
Epoch Losses - Original: 0.3182005, With Penalty: 0.3182143, Difference: 0.0000138 (0.00%)
Epoch: 3, Steps: 1078 | Train Loss: 0.3080563 Vali Loss: 0.5071290 Test Loss: 0.5488241
Validation loss decreased (0.524581 --> 0.507129).  Saving model ...
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.3094604 | orig_loss: 0.3079132 | penalty: 0.3079132
	speed: 0.0581s/iter; left time: 18607.3329s
	iters: 200, epoch: 4 | loss: 0.3167409 | orig_loss: 0.3216653 | penalty: 0.3217925
	speed: 0.0190s/iter; left time: 6094.5535s
	iters: 300, epoch: 4 | loss: 0.2257101 | orig_loss: 0.3122163 | penalty: 0.3122163
	speed: 0.0190s/iter; left time: 6087.6009s
	iters: 400, epoch: 4 | loss: 0.3019498 | orig_loss: 0.3056238 | penalty: 0.3056238
	speed: 0.0190s/iter; left time: 6082.5903s
	iters: 500, epoch: 4 | loss: 0.3839345 | orig_loss: 0.2978795 | penalty: 0.2978795
	speed: 0.0190s/iter; left time: 6079.5731s
	iters: 600, epoch: 4 | loss: 0.4937355 | orig_loss: 0.3122377 | penalty: 0.3122744
	speed: 0.0190s/iter; left time: 6067.1241s
	iters: 700, epoch: 4 | loss: 0.2402840 | orig_loss: 0.2913928 | penalty: 0.2913928
	speed: 0.0190s/iter; left time: 6073.3854s
	iters: 800, epoch: 4 | loss: 0.3043599 | orig_loss: 0.2951282 | penalty: 0.2951282
	speed: 0.0190s/iter; left time: 6071.1804s
	iters: 900, epoch: 4 | loss: 0.2785713 | orig_loss: 0.3034716 | penalty: 0.3034716
	speed: 0.0190s/iter; left time: 6070.0285s
	iters: 1000, epoch: 4 | loss: 0.3480787 | orig_loss: 0.3013814 | penalty: 0.3013814
	speed: 0.0190s/iter; left time: 6056.7259s
Epoch: 4 cost time: 20.683607578277588
Epoch Losses - Original: 0.3039978, With Penalty: 0.3040130, Difference: 0.0000152 (0.01%)
Epoch: 4, Steps: 1078 | Train Loss: 0.3202825 Vali Loss: 0.5128929 Test Loss: 0.5069815
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.3589475 | orig_loss: 0.3085560 | penalty: 0.3085560
	speed: 0.0571s/iter; left time: 18210.2792s
	iters: 200, epoch: 5 | loss: 0.2701785 | orig_loss: 0.2940846 | penalty: 0.2940846
	speed: 0.0189s/iter; left time: 6042.6933s
	iters: 300, epoch: 5 | loss: 0.3295425 | orig_loss: 0.3035875 | penalty: 0.3035875
	speed: 0.0190s/iter; left time: 6044.4960s
	iters: 400, epoch: 5 | loss: 0.3269215 | orig_loss: 0.2902752 | penalty: 0.2902752
	speed: 0.0190s/iter; left time: 6061.9160s
	iters: 500, epoch: 5 | loss: 0.2809284 | orig_loss: 0.2993248 | penalty: 0.2993248
	speed: 0.0189s/iter; left time: 6035.7894s
	iters: 600, epoch: 5 | loss: 0.2187448 | orig_loss: 0.2874071 | penalty: 0.2874071
	speed: 0.0189s/iter; left time: 6027.5863s
	iters: 700, epoch: 5 | loss: 0.2146334 | orig_loss: 0.2966244 | penalty: 0.2966244
	speed: 0.0189s/iter; left time: 6026.5576s
	iters: 800, epoch: 5 | loss: 0.3190450 | orig_loss: 0.2841574 | penalty: 0.2841574
	speed: 0.0189s/iter; left time: 6026.4056s
	iters: 900, epoch: 5 | loss: 0.2941816 | orig_loss: 0.2839903 | penalty: 0.2839903
	speed: 0.0190s/iter; left time: 6035.4994s
	iters: 1000, epoch: 5 | loss: 0.2754017 | orig_loss: 0.2788358 | penalty: 0.2788358
	speed: 0.0189s/iter; left time: 6023.9513s
Epoch: 5 cost time: 20.577060222625732
Epoch Losses - Original: 0.2926717, With Penalty: 0.2926717, Difference: 0.0000000 (0.00%)
Epoch: 5, Steps: 1078 | Train Loss: 0.2888525 Vali Loss: 0.4950893 Test Loss: 0.5018211
Validation loss decreased (0.507129 --> 0.495089).  Saving model ...
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.2534067 | orig_loss: 0.2762593 | penalty: 0.2762593
	speed: 0.0581s/iter; left time: 18466.6602s
	iters: 200, epoch: 6 | loss: 0.2678457 | orig_loss: 0.2869141 | penalty: 0.2869141
	speed: 0.0194s/iter; left time: 6170.9013s
	iters: 300, epoch: 6 | loss: 0.2864782 | orig_loss: 0.2888766 | penalty: 0.2888766
	speed: 0.0194s/iter; left time: 6169.2133s
	iters: 400, epoch: 6 | loss: 0.2646187 | orig_loss: 0.2850150 | penalty: 0.2850150
	speed: 0.0194s/iter; left time: 6169.7008s
	iters: 500, epoch: 6 | loss: 0.3676541 | orig_loss: 0.2788193 | penalty: 0.2788193
	speed: 0.0194s/iter; left time: 6169.7537s
	iters: 600, epoch: 6 | loss: 0.2086688 | orig_loss: 0.2843573 | penalty: 0.2843573
	speed: 0.0194s/iter; left time: 6162.9922s
	iters: 700, epoch: 6 | loss: 0.2626662 | orig_loss: 0.2819040 | penalty: 0.2819040
	speed: 0.0194s/iter; left time: 6167.6354s
	iters: 800, epoch: 6 | loss: 0.3551476 | orig_loss: 0.2831001 | penalty: 0.2831001
	speed: 0.0194s/iter; left time: 6166.1962s
	iters: 900, epoch: 6 | loss: 0.3651886 | orig_loss: 0.2839939 | penalty: 0.2839939
	speed: 0.0194s/iter; left time: 6160.5038s
	iters: 1000, epoch: 6 | loss: 0.2398820 | orig_loss: 0.2737111 | penalty: 0.2737111
	speed: 0.0194s/iter; left time: 6155.9664s
Epoch: 6 cost time: 21.06525945663452
Epoch Losses - Original: 0.2819830, With Penalty: 0.2820233, Difference: 0.0000403 (0.01%)
Epoch: 6, Steps: 1078 | Train Loss: 0.2871557 Vali Loss: 0.5040606 Test Loss: 0.4907946
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.2664882 | orig_loss: 0.2725368 | penalty: 0.2725368
	speed: 0.0573s/iter; left time: 18150.1180s
	iters: 200, epoch: 7 | loss: 0.2663594 | orig_loss: 0.2724054 | penalty: 0.2724054
	speed: 0.0193s/iter; left time: 6098.7616s
	iters: 300, epoch: 7 | loss: 0.2672960 | orig_loss: 0.2774987 | penalty: 0.2774987
	speed: 0.0195s/iter; left time: 6161.6951s
	iters: 400, epoch: 7 | loss: 0.2354564 | orig_loss: 0.2748656 | penalty: 0.2748656
	speed: 0.0195s/iter; left time: 6161.6365s
	iters: 500, epoch: 7 | loss: 0.2929858 | orig_loss: 0.2762345 | penalty: 0.2762345
	speed: 0.0190s/iter; left time: 6014.3035s
	iters: 600, epoch: 7 | loss: 0.2712014 | orig_loss: 0.2738882 | penalty: 0.2738882
	speed: 0.0189s/iter; left time: 5993.4491s
	iters: 700, epoch: 7 | loss: 0.2574587 | orig_loss: 0.2643535 | penalty: 0.2643535
	speed: 0.0189s/iter; left time: 5988.7000s
	iters: 800, epoch: 7 | loss: 0.3282709 | orig_loss: 0.2692139 | penalty: 0.2692139
	speed: 0.0190s/iter; left time: 5993.7555s
	iters: 900, epoch: 7 | loss: 0.2367626 | orig_loss: 0.2764442 | penalty: 0.2764442
	speed: 0.0189s/iter; left time: 5986.1647s
	iters: 1000, epoch: 7 | loss: 0.3597728 | orig_loss: 0.2748788 | penalty: 0.2748788
	speed: 0.0189s/iter; left time: 5979.8460s
Epoch: 7 cost time: 20.716111421585083
Epoch Losses - Original: 0.2751742, With Penalty: 0.2755530, Difference: 0.0003789 (0.14%)
Epoch: 7, Steps: 1078 | Train Loss: 0.2782052 Vali Loss: 0.4841531 Test Loss: 0.4952483
Validation loss decreased (0.495089 --> 0.484153).  Saving model ...
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.2030357 | orig_loss: 0.2714967 | penalty: 0.2714967
	speed: 0.0573s/iter; left time: 18086.2459s
	iters: 200, epoch: 8 | loss: 0.2582211 | orig_loss: 0.2658471 | penalty: 0.2658471
	speed: 0.0190s/iter; left time: 5989.2557s
	iters: 300, epoch: 8 | loss: 0.2793982 | orig_loss: 0.2684238 | penalty: 0.2684238
	speed: 0.0190s/iter; left time: 5983.0000s
	iters: 400, epoch: 8 | loss: 0.2135435 | orig_loss: 0.2749220 | penalty: 0.2749220
	speed: 0.0190s/iter; left time: 5984.0146s
	iters: 500, epoch: 8 | loss: 0.2140506 | orig_loss: 0.2648059 | penalty: 0.2648059
	speed: 0.0191s/iter; left time: 6019.0801s
	iters: 600, epoch: 8 | loss: 0.2192407 | orig_loss: 0.2625124 | penalty: 0.2625124
	speed: 0.0193s/iter; left time: 6096.6093s
	iters: 700, epoch: 8 | loss: 0.2266375 | orig_loss: 0.2627064 | penalty: 0.2627064
	speed: 0.0190s/iter; left time: 5982.8524s
	iters: 800, epoch: 8 | loss: 0.2745638 | orig_loss: 0.2689471 | penalty: 0.2689471
	speed: 0.0190s/iter; left time: 5984.4566s
	iters: 900, epoch: 8 | loss: 0.2603434 | orig_loss: 0.2680386 | penalty: 0.2680386
	speed: 0.0190s/iter; left time: 5975.3108s
	iters: 1000, epoch: 8 | loss: 0.2047933 | orig_loss: 0.2604167 | penalty: 0.2604167
	speed: 0.0190s/iter; left time: 5973.8828s
Epoch: 8 cost time: 20.63933563232422
Epoch Losses - Original: 0.2667105, With Penalty: 0.2667105, Difference: 0.0000000 (0.00%)
Epoch: 8, Steps: 1078 | Train Loss: 0.2353828 Vali Loss: 0.4956665 Test Loss: 0.4857756
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002994739288874256
	iters: 100, epoch: 9 | loss: 0.2710015 | orig_loss: 0.2620238 | penalty: 0.2620238
	speed: 0.0579s/iter; left time: 18215.6328s
	iters: 200, epoch: 9 | loss: 0.2476645 | orig_loss: 0.2538814 | penalty: 0.2538814
	speed: 0.0194s/iter; left time: 6118.0237s
	iters: 300, epoch: 9 | loss: 0.2182943 | orig_loss: 0.2628926 | penalty: 0.2628926
	speed: 0.0194s/iter; left time: 6116.3450s
	iters: 400, epoch: 9 | loss: 0.2461325 | orig_loss: 0.2571715 | penalty: 0.2571715
	speed: 0.0193s/iter; left time: 6061.5737s
	iters: 500, epoch: 9 | loss: 0.2076476 | orig_loss: 0.2513283 | penalty: 0.2513283
	speed: 0.0189s/iter; left time: 5948.1491s
	iters: 600, epoch: 9 | loss: 0.2494198 | orig_loss: 0.2665299 | penalty: 0.2665299
	speed: 0.0189s/iter; left time: 5945.2235s
	iters: 700, epoch: 9 | loss: 0.2957945 | orig_loss: 0.2591090 | penalty: 0.2591090
	speed: 0.0193s/iter; left time: 6060.2628s
	iters: 800, epoch: 9 | loss: 0.3060127 | orig_loss: 0.2616130 | penalty: 0.2616130
	speed: 0.0193s/iter; left time: 6046.1299s
	iters: 900, epoch: 9 | loss: 0.2075569 | orig_loss: 0.2553171 | penalty: 0.2553171
	speed: 0.0194s/iter; left time: 6094.0789s
	iters: 1000, epoch: 9 | loss: 0.3236794 | orig_loss: 0.2556285 | penalty: 0.2556285
	speed: 0.0194s/iter; left time: 6086.7817s
Epoch: 9 cost time: 20.957011699676514
Epoch Losses - Original: 0.2583571, With Penalty: 0.2583571, Difference: 0.0000000 (0.00%)
Epoch: 9, Steps: 1078 | Train Loss: 0.2573204 Vali Loss: 0.4904552 Test Loss: 0.5014107
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.000299334294690462
	iters: 100, epoch: 10 | loss: 0.2166138 | orig_loss: 0.2518904 | penalty: 0.2518904
	speed: 0.0580s/iter; left time: 18186.4353s
	iters: 200, epoch: 10 | loss: 0.2239567 | orig_loss: 0.2515703 | penalty: 0.2515703
	speed: 0.0194s/iter; left time: 6097.2003s
	iters: 300, epoch: 10 | loss: 0.3976212 | orig_loss: 0.2449390 | penalty: 0.2449390
	speed: 0.0191s/iter; left time: 5978.8362s
	iters: 400, epoch: 10 | loss: 0.2294868 | orig_loss: 0.2458973 | penalty: 0.2458973
	speed: 0.0189s/iter; left time: 5929.8078s
	iters: 500, epoch: 10 | loss: 0.2698058 | orig_loss: 0.2542342 | penalty: 0.2542342
	speed: 0.0190s/iter; left time: 5935.9379s
	iters: 600, epoch: 10 | loss: 0.2167146 | orig_loss: 0.2500310 | penalty: 0.2500310
	speed: 0.0190s/iter; left time: 5935.7319s
	iters: 700, epoch: 10 | loss: 0.2124424 | orig_loss: 0.2700895 | penalty: 0.2700895
	speed: 0.0189s/iter; left time: 5929.0602s
	iters: 800, epoch: 10 | loss: 0.2213103 | orig_loss: 0.2581926 | penalty: 0.2581926
	speed: 0.0189s/iter; left time: 5924.7272s
	iters: 900, epoch: 10 | loss: 0.2451417 | orig_loss: 0.2498425 | penalty: 0.2498425
	speed: 0.0190s/iter; left time: 5953.0829s
	iters: 1000, epoch: 10 | loss: 0.2096148 | orig_loss: 0.2515116 | penalty: 0.2515116
	speed: 0.0190s/iter; left time: 5937.3084s
Epoch: 10 cost time: 20.695411920547485
Epoch Losses - Original: 0.2526978, With Penalty: 0.2526978, Difference: 0.0000000 (0.00%)
Epoch: 10, Steps: 1078 | Train Loss: 0.2442708 Vali Loss: 0.4765374 Test Loss: 0.4854344
Validation loss decreased (0.484153 --> 0.476537).  Saving model ...
Updating learning rate to 0.00029917828430524096
	iters: 100, epoch: 11 | loss: 0.3059206 | orig_loss: 0.2504574 | penalty: 0.2504574
	speed: 0.0578s/iter; left time: 18055.6293s
	iters: 200, epoch: 11 | loss: 0.4007331 | orig_loss: 0.2571130 | penalty: 0.2571130
	speed: 0.0189s/iter; left time: 5911.5944s
	iters: 300, epoch: 11 | loss: 0.2639573 | orig_loss: 0.2438258 | penalty: 0.2438258
	speed: 0.0189s/iter; left time: 5901.7555s
	iters: 400, epoch: 11 | loss: 0.2621713 | orig_loss: 0.2475662 | penalty: 0.2475662
	speed: 0.0189s/iter; left time: 5913.7853s
	iters: 500, epoch: 11 | loss: 0.2935668 | orig_loss: 0.2446292 | penalty: 0.2446292
	speed: 0.0189s/iter; left time: 5898.0514s
	iters: 600, epoch: 11 | loss: 0.2441320 | orig_loss: 0.2468269 | penalty: 0.2468269
	speed: 0.0189s/iter; left time: 5897.7738s
	iters: 700, epoch: 11 | loss: 0.3199008 | orig_loss: 0.2442771 | penalty: 0.2442771
	speed: 0.0189s/iter; left time: 5894.6409s
	iters: 800, epoch: 11 | loss: 0.2191371 | orig_loss: 0.2401088 | penalty: 0.2401088
	speed: 0.0189s/iter; left time: 5892.4054s
	iters: 900, epoch: 11 | loss: 0.3994931 | orig_loss: 0.2474890 | penalty: 0.2474890
	speed: 0.0189s/iter; left time: 5885.5170s
	iters: 1000, epoch: 11 | loss: 0.2583523 | orig_loss: 0.2511645 | penalty: 0.2511645
	speed: 0.0189s/iter; left time: 5899.5372s
Epoch: 11 cost time: 20.573718309402466
Epoch Losses - Original: 0.2476947, With Penalty: 0.2476947, Difference: 0.0000000 (0.00%)
Epoch: 11, Steps: 1078 | Train Loss: 0.2967364 Vali Loss: 0.5009375 Test Loss: 0.4923349
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002990059148400594
	iters: 100, epoch: 12 | loss: 0.3849503 | orig_loss: 0.2489672 | penalty: 0.2489672
	speed: 0.0574s/iter; left time: 17864.0677s
	iters: 200, epoch: 12 | loss: 0.2542006 | orig_loss: 0.2418360 | penalty: 0.2418360
	speed: 0.0189s/iter; left time: 5899.4948s
	iters: 300, epoch: 12 | loss: 0.2778136 | orig_loss: 0.2364636 | penalty: 0.2364636
	speed: 0.0190s/iter; left time: 5899.6695s
	iters: 400, epoch: 12 | loss: 0.2113762 | orig_loss: 0.2469918 | penalty: 0.2469918
	speed: 0.0189s/iter; left time: 5892.6487s
	iters: 500, epoch: 12 | loss: 0.1955615 | orig_loss: 0.2391811 | penalty: 0.2391811
	speed: 0.0189s/iter; left time: 5888.4916s
	iters: 600, epoch: 12 | loss: 0.2442455 | orig_loss: 0.2429870 | penalty: 0.2429870
	speed: 0.0189s/iter; left time: 5888.9366s
	iters: 700, epoch: 12 | loss: 0.1878073 | orig_loss: 0.2378786 | penalty: 0.2378786
	speed: 0.0190s/iter; left time: 5892.9242s
	iters: 800, epoch: 12 | loss: 0.1866677 | orig_loss: 0.2398001 | penalty: 0.2398001
	speed: 0.0194s/iter; left time: 6016.5946s
	iters: 900, epoch: 12 | loss: 0.3855295 | orig_loss: 0.2520941 | penalty: 0.2520941
	speed: 0.0193s/iter; left time: 6009.5213s
	iters: 1000, epoch: 12 | loss: 0.2246262 | orig_loss: 0.2477638 | penalty: 0.2477638
	speed: 0.0193s/iter; left time: 6008.8869s
Epoch: 12 cost time: 20.73244023323059
Epoch Losses - Original: 0.2431626, With Penalty: 0.2431626, Difference: 0.0000000 (0.00%)
Epoch: 12, Steps: 1078 | Train Loss: 0.2552779 Vali Loss: 0.4982671 Test Loss: 0.4857861
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002988172051971717
	iters: 100, epoch: 13 | loss: 0.2021127 | orig_loss: 0.2365786 | penalty: 0.2365786
	speed: 0.0573s/iter; left time: 17793.6111s
	iters: 200, epoch: 13 | loss: 0.2589592 | orig_loss: 0.2406294 | penalty: 0.2406294
	speed: 0.0190s/iter; left time: 5889.2358s
	iters: 300, epoch: 13 | loss: 0.2520058 | orig_loss: 0.2381807 | penalty: 0.2381807
	speed: 0.0194s/iter; left time: 6015.2541s
	iters: 400, epoch: 13 | loss: 0.2410039 | orig_loss: 0.2414809 | penalty: 0.2414809
	speed: 0.0194s/iter; left time: 6016.9459s
	iters: 500, epoch: 13 | loss: 0.3029882 | orig_loss: 0.2344527 | penalty: 0.2344527
	speed: 0.0194s/iter; left time: 6012.3848s
	iters: 600, epoch: 13 | loss: 0.2008156 | orig_loss: 0.2382431 | penalty: 0.2382431
	speed: 0.0194s/iter; left time: 6006.3368s
	iters: 700, epoch: 13 | loss: 0.1954265 | orig_loss: 0.2400223 | penalty: 0.2400223
	speed: 0.0194s/iter; left time: 6012.5859s
	iters: 800, epoch: 13 | loss: 0.2818893 | orig_loss: 0.2401261 | penalty: 0.2401261
	speed: 0.0194s/iter; left time: 6016.2796s
	iters: 900, epoch: 13 | loss: 0.2082426 | orig_loss: 0.2376789 | penalty: 0.2376789
	speed: 0.0194s/iter; left time: 6000.2228s
	iters: 1000, epoch: 13 | loss: 0.1751657 | orig_loss: 0.2338921 | penalty: 0.2339865
	speed: 0.0189s/iter; left time: 5848.1318s
Epoch: 13 cost time: 20.87284517288208
Epoch Losses - Original: 0.2385976, With Penalty: 0.2386064, Difference: 0.0000088 (0.00%)
Epoch: 13, Steps: 1078 | Train Loss: 0.2318610 Vali Loss: 0.4842680 Test Loss: 0.4928803
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm1, seq_len: 48, pred_len: 48
Avg Original Loss: 0.2841770
Avg Penalty Loss: 0.2842233
Avg Difference: 0.0000462 (0.02%)
================================

>>>>>>>testing : long_term_forecast_ETTm1_48_48_SOFTS_ETTm1_ftM_sl48_ll48_pl48_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11473
mse:0.48543439415866907, mae:0.42910117872827586
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm1_48_72', model='SOFTS', data='ETTm1', root_path='./dataset/ETT/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=72, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm1_48_72_SOFTS_ETTm1_ftM_sl48_ll48_pl72_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34441
val 11449
test 11449

------ Mask Loss Comparison ------
Original Loss: 0.6720536
Best Mask Loss: 0.6564505 (mask length: 3)
Improvement: 0.0156032 (2.32%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6798552
---------------------------------

	iters: 100, epoch: 1 | loss: 0.4562441 | orig_loss: 0.4403426 | penalty: 0.4414018
	speed: 0.0332s/iter; left time: 10708.6754s
	iters: 200, epoch: 1 | loss: 0.3613708 | orig_loss: 0.3902351 | penalty: 0.3903347
	speed: 0.0189s/iter; left time: 6097.2147s
	iters: 300, epoch: 1 | loss: 0.3950091 | orig_loss: 0.3855770 | penalty: 0.3855770
	speed: 0.0189s/iter; left time: 6088.4907s
	iters: 400, epoch: 1 | loss: 0.4145444 | orig_loss: 0.3806133 | penalty: 0.3806356
	speed: 0.0189s/iter; left time: 6085.9021s
	iters: 500, epoch: 1 | loss: 0.4069424 | orig_loss: 0.3823742 | penalty: 0.3823742
	speed: 0.0188s/iter; left time: 6078.9506s
	iters: 600, epoch: 1 | loss: 0.3587165 | orig_loss: 0.3671063 | penalty: 0.3671063
	speed: 0.0189s/iter; left time: 6081.4774s
	iters: 700, epoch: 1 | loss: 0.3089348 | orig_loss: 0.3609801 | penalty: 0.3609801
	speed: 0.0188s/iter; left time: 6075.6877s
	iters: 800, epoch: 1 | loss: 0.3738109 | orig_loss: 0.3589387 | penalty: 0.3589387
	speed: 0.0189s/iter; left time: 6082.5763s
	iters: 900, epoch: 1 | loss: 0.3761749 | orig_loss: 0.3640086 | penalty: 0.3640919
	speed: 0.0189s/iter; left time: 6081.8513s
	iters: 1000, epoch: 1 | loss: 0.2965747 | orig_loss: 0.3490476 | penalty: 0.3490476
	speed: 0.0188s/iter; left time: 6069.4021s
Epoch: 1 cost time: 20.86199641227722
Epoch Losses - Original: 0.3760441, With Penalty: 0.3761840, Difference: 0.0001399 (0.04%)
Epoch: 1, Steps: 1077 | Train Loss: 0.3748323 Vali Loss: 0.5213683 Test Loss: 0.5037618
Validation loss decreased (inf --> 0.521368).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.2771541 | orig_loss: 0.3389556 | penalty: 0.3389556
	speed: 0.0569s/iter; left time: 18317.4528s
	iters: 200, epoch: 2 | loss: 0.3271364 | orig_loss: 0.3531588 | penalty: 0.3531588
	speed: 0.0188s/iter; left time: 6062.3932s
	iters: 300, epoch: 2 | loss: 0.2997003 | orig_loss: 0.3458162 | penalty: 0.3458162
	speed: 0.0188s/iter; left time: 6055.2974s
	iters: 400, epoch: 2 | loss: 0.3701202 | orig_loss: 0.3411236 | penalty: 0.3411236
	speed: 0.0188s/iter; left time: 6054.9895s
	iters: 500, epoch: 2 | loss: 0.4905106 | orig_loss: 0.3397234 | penalty: 0.3397234
	speed: 0.0188s/iter; left time: 6049.0946s
	iters: 600, epoch: 2 | loss: 0.3562191 | orig_loss: 0.3500336 | penalty: 0.3500336
	speed: 0.0188s/iter; left time: 6058.3941s
	iters: 700, epoch: 2 | loss: 0.2399923 | orig_loss: 0.3416738 | penalty: 0.3416848
	speed: 0.0189s/iter; left time: 6061.8589s
	iters: 800, epoch: 2 | loss: 0.4677989 | orig_loss: 0.3331816 | penalty: 0.3331816
	speed: 0.0188s/iter; left time: 6051.0677s
	iters: 900, epoch: 2 | loss: 0.2924313 | orig_loss: 0.3324260 | penalty: 0.3324260
	speed: 0.0188s/iter; left time: 6047.3595s
	iters: 1000, epoch: 2 | loss: 0.3765871 | orig_loss: 0.3331154 | penalty: 0.3331154
	speed: 0.0188s/iter; left time: 6045.6317s
Epoch: 2 cost time: 20.437670707702637
Epoch Losses - Original: 0.3399122, With Penalty: 0.3399150, Difference: 0.0000027 (0.00%)
Epoch: 2, Steps: 1077 | Train Loss: 0.3497651 Vali Loss: 0.5048802 Test Loss: 0.5091519
Validation loss decreased (0.521368 --> 0.504880).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.3115839 | orig_loss: 0.3354612 | penalty: 0.3354612
	speed: 0.0569s/iter; left time: 18255.5206s
	iters: 200, epoch: 3 | loss: 0.2691746 | orig_loss: 0.3284158 | penalty: 0.3284158
	speed: 0.0189s/iter; left time: 6051.7811s
	iters: 300, epoch: 3 | loss: 0.2300426 | orig_loss: 0.3219876 | penalty: 0.3219876
	speed: 0.0188s/iter; left time: 6040.0777s
	iters: 400, epoch: 3 | loss: 0.3115987 | orig_loss: 0.3233624 | penalty: 0.3233774
	speed: 0.0188s/iter; left time: 6040.9857s
	iters: 500, epoch: 3 | loss: 0.3154223 | orig_loss: 0.3220233 | penalty: 0.3220233
	speed: 0.0188s/iter; left time: 6036.8473s
	iters: 600, epoch: 3 | loss: 0.3803469 | orig_loss: 0.3205247 | penalty: 0.3205247
	speed: 0.0188s/iter; left time: 6033.5489s
	iters: 700, epoch: 3 | loss: 0.3063743 | orig_loss: 0.3172326 | penalty: 0.3172326
	speed: 0.0188s/iter; left time: 6030.0155s
	iters: 800, epoch: 3 | loss: 0.3648143 | orig_loss: 0.3115746 | penalty: 0.3115746
	speed: 0.0188s/iter; left time: 6034.3442s
	iters: 900, epoch: 3 | loss: 0.2761412 | orig_loss: 0.3185079 | penalty: 0.3185079
	speed: 0.0188s/iter; left time: 6025.8803s
	iters: 1000, epoch: 3 | loss: 0.3602451 | orig_loss: 0.3232662 | penalty: 0.3232662
	speed: 0.0189s/iter; left time: 6036.5389s
Epoch: 3 cost time: 20.429126501083374
Epoch Losses - Original: 0.3214773, With Penalty: 0.3214787, Difference: 0.0000014 (0.00%)
Epoch: 3, Steps: 1077 | Train Loss: 0.3125744 Vali Loss: 0.5012801 Test Loss: 0.4770348
Validation loss decreased (0.504880 --> 0.501280).  Saving model ...
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.2660143 | orig_loss: 0.3123167 | penalty: 0.3123167
	speed: 0.0569s/iter; left time: 18189.9836s
	iters: 200, epoch: 4 | loss: 0.2761610 | orig_loss: 0.3129916 | penalty: 0.3129916
	speed: 0.0188s/iter; left time: 6008.8559s
	iters: 300, epoch: 4 | loss: 0.3547188 | orig_loss: 0.3033736 | penalty: 0.3033736
	speed: 0.0188s/iter; left time: 6007.2116s
	iters: 400, epoch: 4 | loss: 0.3120796 | orig_loss: 0.3123794 | penalty: 0.3123794
	speed: 0.0188s/iter; left time: 5999.9658s
	iters: 500, epoch: 4 | loss: 0.3398609 | orig_loss: 0.3179044 | penalty: 0.3179044
	speed: 0.0188s/iter; left time: 6002.5771s
	iters: 600, epoch: 4 | loss: 0.2878739 | orig_loss: 0.3028432 | penalty: 0.3028432
	speed: 0.0188s/iter; left time: 6008.0013s
	iters: 700, epoch: 4 | loss: 0.3940713 | orig_loss: 0.3090655 | penalty: 0.3090655
	speed: 0.0188s/iter; left time: 5996.7202s
	iters: 800, epoch: 4 | loss: 0.2901565 | orig_loss: 0.3061651 | penalty: 0.3061651
	speed: 0.0188s/iter; left time: 5992.0959s
	iters: 900, epoch: 4 | loss: 0.2857963 | orig_loss: 0.2995600 | penalty: 0.2995600
	speed: 0.0188s/iter; left time: 5994.7367s
	iters: 1000, epoch: 4 | loss: 0.3003870 | orig_loss: 0.3027831 | penalty: 0.3027831
	speed: 0.0188s/iter; left time: 5985.0587s
Epoch: 4 cost time: 20.401166200637817
Epoch Losses - Original: 0.3079847, With Penalty: 0.3079876, Difference: 0.0000029 (0.00%)
Epoch: 4, Steps: 1077 | Train Loss: 0.3107120 Vali Loss: 0.4801971 Test Loss: 0.4783031
Validation loss decreased (0.501280 --> 0.480197).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.3806770 | orig_loss: 0.3086882 | penalty: 0.3086882
	speed: 0.0570s/iter; left time: 18170.6186s
	iters: 200, epoch: 5 | loss: 0.3155922 | orig_loss: 0.2984113 | penalty: 0.2984113
	speed: 0.0188s/iter; left time: 5987.9043s
	iters: 300, epoch: 5 | loss: 0.3123455 | orig_loss: 0.2913532 | penalty: 0.2913532
	speed: 0.0188s/iter; left time: 5986.0582s
	iters: 400, epoch: 5 | loss: 0.3550765 | orig_loss: 0.2928684 | penalty: 0.2928684
	speed: 0.0188s/iter; left time: 5979.9801s
	iters: 500, epoch: 5 | loss: 0.2871916 | orig_loss: 0.3039575 | penalty: 0.3039575
	speed: 0.0188s/iter; left time: 5978.9799s
	iters: 600, epoch: 5 | loss: 0.2621253 | orig_loss: 0.2968623 | penalty: 0.2968623
	speed: 0.0188s/iter; left time: 5976.3451s
	iters: 700, epoch: 5 | loss: 0.2413948 | orig_loss: 0.2958801 | penalty: 0.2958801
	speed: 0.0188s/iter; left time: 5982.0660s
	iters: 800, epoch: 5 | loss: 0.2711103 | orig_loss: 0.3018288 | penalty: 0.3018288
	speed: 0.0188s/iter; left time: 5964.0261s
	iters: 900, epoch: 5 | loss: 0.2956688 | orig_loss: 0.2870328 | penalty: 0.2870328
	speed: 0.0187s/iter; left time: 5948.1882s
	iters: 1000, epoch: 5 | loss: 0.2686993 | orig_loss: 0.3002323 | penalty: 0.3002323
	speed: 0.0187s/iter; left time: 5945.1594s
Epoch: 5 cost time: 20.34798288345337
Epoch Losses - Original: 0.2975339, With Penalty: 0.2975339, Difference: 0.0000000 (0.00%)
Epoch: 5, Steps: 1077 | Train Loss: 0.2989881 Vali Loss: 0.4819719 Test Loss: 0.4663835
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.2569487 | orig_loss: 0.2831048 | penalty: 0.2831048
	speed: 0.0569s/iter; left time: 18083.4679s
	iters: 200, epoch: 6 | loss: 0.3039272 | orig_loss: 0.2909616 | penalty: 0.2909616
	speed: 0.0187s/iter; left time: 5950.1380s
	iters: 300, epoch: 6 | loss: 0.2942162 | orig_loss: 0.2921096 | penalty: 0.2921096
	speed: 0.0187s/iter; left time: 5947.8418s
	iters: 400, epoch: 6 | loss: 0.2312484 | orig_loss: 0.2865231 | penalty: 0.2865231
	speed: 0.0187s/iter; left time: 5949.4420s
	iters: 500, epoch: 6 | loss: 0.2395723 | orig_loss: 0.2900545 | penalty: 0.2900545
	speed: 0.0187s/iter; left time: 5939.0329s
	iters: 600, epoch: 6 | loss: 0.3097091 | orig_loss: 0.2888700 | penalty: 0.2888700
	speed: 0.0187s/iter; left time: 5938.1323s
	iters: 700, epoch: 6 | loss: 0.2866239 | orig_loss: 0.2838511 | penalty: 0.2838511
	speed: 0.0187s/iter; left time: 5942.4167s
	iters: 800, epoch: 6 | loss: 0.2501284 | orig_loss: 0.2940258 | penalty: 0.2940258
	speed: 0.0187s/iter; left time: 5938.5845s
	iters: 900, epoch: 6 | loss: 0.2585751 | orig_loss: 0.2823680 | penalty: 0.2823680
	speed: 0.0187s/iter; left time: 5935.2293s
	iters: 1000, epoch: 6 | loss: 0.2566912 | orig_loss: 0.2899008 | penalty: 0.2899008
	speed: 0.0187s/iter; left time: 5936.5251s
Epoch: 6 cost time: 20.317837238311768
Epoch Losses - Original: 0.2880548, With Penalty: 0.2880548, Difference: 0.0000000 (0.00%)
Epoch: 6, Steps: 1077 | Train Loss: 0.2687640 Vali Loss: 0.4901147 Test Loss: 0.4787924
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.2753914 | orig_loss: 0.2805935 | penalty: 0.2805935
	speed: 0.0567s/iter; left time: 17963.4809s
	iters: 200, epoch: 7 | loss: 0.3141488 | orig_loss: 0.2848735 | penalty: 0.2848735
	speed: 0.0188s/iter; left time: 5955.4256s
	iters: 300, epoch: 7 | loss: 0.2321475 | orig_loss: 0.2833736 | penalty: 0.2833736
	speed: 0.0188s/iter; left time: 5957.4316s
	iters: 400, epoch: 7 | loss: 0.2951480 | orig_loss: 0.2771282 | penalty: 0.2771282
	speed: 0.0188s/iter; left time: 5954.3593s
	iters: 500, epoch: 7 | loss: 0.2393165 | orig_loss: 0.2861116 | penalty: 0.2861116
	speed: 0.0188s/iter; left time: 5949.7532s
	iters: 600, epoch: 7 | loss: 0.2271969 | orig_loss: 0.2840611 | penalty: 0.2840611
	speed: 0.0188s/iter; left time: 5946.0470s
	iters: 700, epoch: 7 | loss: 0.3853742 | orig_loss: 0.2801582 | penalty: 0.2801582
	speed: 0.0189s/iter; left time: 5955.7514s
	iters: 800, epoch: 7 | loss: 0.2813856 | orig_loss: 0.2829576 | penalty: 0.2829576
	speed: 0.0188s/iter; left time: 5947.4273s
	iters: 900, epoch: 7 | loss: 0.2861373 | orig_loss: 0.2765643 | penalty: 0.2765643
	speed: 0.0188s/iter; left time: 5944.5919s
	iters: 1000, epoch: 7 | loss: 0.3403554 | orig_loss: 0.2752149 | penalty: 0.2752149
	speed: 0.0189s/iter; left time: 5953.6888s
Epoch: 7 cost time: 20.427010536193848
Epoch Losses - Original: 0.2808478, With Penalty: 0.2808478, Difference: 0.0000000 (0.00%)
Epoch: 7, Steps: 1077 | Train Loss: 0.2876601 Vali Loss: 0.4870659 Test Loss: 0.4736836
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm1, seq_len: 48, pred_len: 72
Avg Original Loss: 0.3159793
Avg Penalty Loss: 0.3160003
Avg Difference: 0.0000210 (0.01%)
================================

>>>>>>>testing : long_term_forecast_ETTm1_48_72_SOFTS_ETTm1_ftM_sl48_ll48_pl72_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11449
mse:0.4783030987600643, mae:0.42705145599030125
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm1_48_96', model='SOFTS', data='ETTm1', root_path='./dataset/ETT/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=96, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm1_48_96_SOFTS_ETTm1_ftM_sl48_ll48_pl96_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34417
val 11425
test 11425

------ Mask Loss Comparison ------
Original Loss: 0.7000728
Best Mask Loss: 0.6198180 (mask length: 12)
Improvement: 0.0802547 (11.46%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.7402002
---------------------------------

	iters: 100, epoch: 1 | loss: 0.3818849 | orig_loss: 0.4543884 | penalty: 0.4555102
	speed: 0.0337s/iter; left time: 10889.4954s
	iters: 200, epoch: 1 | loss: 0.3824271 | orig_loss: 0.3971630 | penalty: 0.3972103
	speed: 0.0195s/iter; left time: 6275.1984s
	iters: 300, epoch: 1 | loss: 0.3911283 | orig_loss: 0.3977337 | penalty: 0.3977337
	speed: 0.0194s/iter; left time: 6257.9028s
	iters: 400, epoch: 1 | loss: 0.3899838 | orig_loss: 0.3834470 | penalty: 0.3834499
	speed: 0.0192s/iter; left time: 6195.8898s
	iters: 500, epoch: 1 | loss: 0.3731055 | orig_loss: 0.3713468 | penalty: 0.3714338
	speed: 0.0189s/iter; left time: 6077.6414s
	iters: 600, epoch: 1 | loss: 0.3281654 | orig_loss: 0.3645826 | penalty: 0.3645826
	speed: 0.0188s/iter; left time: 6059.9395s
	iters: 700, epoch: 1 | loss: 0.3621304 | orig_loss: 0.3544954 | penalty: 0.3545019
	speed: 0.0188s/iter; left time: 6050.3101s
	iters: 800, epoch: 1 | loss: 0.4260580 | orig_loss: 0.3617146 | penalty: 0.3617325
	speed: 0.0188s/iter; left time: 6056.3085s
	iters: 900, epoch: 1 | loss: 0.3771141 | orig_loss: 0.3628340 | penalty: 0.3628340
	speed: 0.0188s/iter; left time: 6055.7546s
	iters: 1000, epoch: 1 | loss: 0.3797876 | orig_loss: 0.3575717 | penalty: 0.3576491
	speed: 0.0188s/iter; left time: 6046.4082s
Epoch: 1 cost time: 21.016040086746216
Epoch Losses - Original: 0.3786433, With Penalty: 0.3787698, Difference: 0.0001265 (0.03%)
Epoch: 1, Steps: 1076 | Train Loss: 0.3791785 Vali Loss: 0.5193456 Test Loss: 0.4675948
Validation loss decreased (inf --> 0.519346).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.4092174 | orig_loss: 0.3505600 | penalty: 0.3505600
	speed: 0.0568s/iter; left time: 18269.5877s
	iters: 200, epoch: 2 | loss: 0.3481507 | orig_loss: 0.3540015 | penalty: 0.3540015
	speed: 0.0189s/iter; left time: 6065.4798s
	iters: 300, epoch: 2 | loss: 0.3817064 | orig_loss: 0.3508197 | penalty: 0.3508957
	speed: 0.0189s/iter; left time: 6060.8476s
	iters: 400, epoch: 2 | loss: 0.4204202 | orig_loss: 0.3575253 | penalty: 0.3575874
	speed: 0.0189s/iter; left time: 6065.6339s
	iters: 500, epoch: 2 | loss: 0.3125674 | orig_loss: 0.3387017 | penalty: 0.3387017
	speed: 0.0189s/iter; left time: 6064.2302s
	iters: 600, epoch: 2 | loss: 0.3518546 | orig_loss: 0.3402491 | penalty: 0.3402491
	speed: 0.0189s/iter; left time: 6054.2176s
	iters: 700, epoch: 2 | loss: 0.2740287 | orig_loss: 0.3419139 | penalty: 0.3419575
	speed: 0.0189s/iter; left time: 6053.6036s
	iters: 800, epoch: 2 | loss: 0.3886119 | orig_loss: 0.3372123 | penalty: 0.3372123
	speed: 0.0189s/iter; left time: 6050.5227s
	iters: 900, epoch: 2 | loss: 0.3996506 | orig_loss: 0.3358145 | penalty: 0.3358145
	speed: 0.0188s/iter; left time: 6041.4144s
	iters: 1000, epoch: 2 | loss: 0.2992495 | orig_loss: 0.3343663 | penalty: 0.3343663
	speed: 0.0188s/iter; left time: 6041.1157s
Epoch: 2 cost time: 20.4431471824646
Epoch Losses - Original: 0.3435139, With Penalty: 0.3435308, Difference: 0.0000169 (0.00%)
Epoch: 2, Steps: 1076 | Train Loss: 0.3585457 Vali Loss: 0.4954209 Test Loss: 0.4725877
Validation loss decreased (0.519346 --> 0.495421).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.3862604 | orig_loss: 0.3297592 | penalty: 0.3297592
	speed: 0.0572s/iter; left time: 18344.1497s
	iters: 200, epoch: 3 | loss: 0.2616164 | orig_loss: 0.3313584 | penalty: 0.3313584
	speed: 0.0187s/iter; left time: 6005.9487s
	iters: 300, epoch: 3 | loss: 0.4058607 | orig_loss: 0.3330629 | penalty: 0.3330629
	speed: 0.0188s/iter; left time: 6010.3168s
	iters: 400, epoch: 3 | loss: 0.3435711 | orig_loss: 0.3214421 | penalty: 0.3214421
	speed: 0.0188s/iter; left time: 6004.9719s
	iters: 500, epoch: 3 | loss: 0.3179823 | orig_loss: 0.3282689 | penalty: 0.3282689
	speed: 0.0187s/iter; left time: 6001.2076s
	iters: 600, epoch: 3 | loss: 0.3057169 | orig_loss: 0.3241732 | penalty: 0.3241732
	speed: 0.0187s/iter; left time: 5998.8158s
	iters: 700, epoch: 3 | loss: 0.3374014 | orig_loss: 0.3265673 | penalty: 0.3265673
	speed: 0.0188s/iter; left time: 6002.2064s
	iters: 800, epoch: 3 | loss: 0.2543326 | orig_loss: 0.3209979 | penalty: 0.3209979
	speed: 0.0188s/iter; left time: 6012.6979s
	iters: 900, epoch: 3 | loss: 0.4076660 | orig_loss: 0.3155071 | penalty: 0.3155071
	speed: 0.0188s/iter; left time: 6008.2673s
	iters: 1000, epoch: 3 | loss: 0.3130514 | orig_loss: 0.3203644 | penalty: 0.3203644
	speed: 0.0188s/iter; left time: 6012.6344s
Epoch: 3 cost time: 20.361932039260864
Epoch Losses - Original: 0.3251681, With Penalty: 0.3251681, Difference: 0.0000000 (0.00%)
Epoch: 3, Steps: 1076 | Train Loss: 0.3333459 Vali Loss: 0.4986907 Test Loss: 0.4537834
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.2584079 | orig_loss: 0.3152723 | penalty: 0.3152723
	speed: 0.0568s/iter; left time: 18149.5483s
	iters: 200, epoch: 4 | loss: 0.3465915 | orig_loss: 0.3156498 | penalty: 0.3156498
	speed: 0.0188s/iter; left time: 6000.9157s
	iters: 300, epoch: 4 | loss: 0.2894457 | orig_loss: 0.3119649 | penalty: 0.3119649
	speed: 0.0188s/iter; left time: 5995.5626s
	iters: 400, epoch: 4 | loss: 0.3526247 | orig_loss: 0.3113211 | penalty: 0.3113211
	speed: 0.0188s/iter; left time: 6008.7070s
	iters: 500, epoch: 4 | loss: 0.2689644 | orig_loss: 0.3138360 | penalty: 0.3138360
	speed: 0.0193s/iter; left time: 6169.0355s
	iters: 600, epoch: 4 | loss: 0.4043141 | orig_loss: 0.3174861 | penalty: 0.3174861
	speed: 0.0193s/iter; left time: 6159.9511s
	iters: 700, epoch: 4 | loss: 0.3520872 | orig_loss: 0.3122282 | penalty: 0.3122282
	speed: 0.0193s/iter; left time: 6142.0562s
	iters: 800, epoch: 4 | loss: 0.2768384 | orig_loss: 0.3121888 | penalty: 0.3121888
	speed: 0.0192s/iter; left time: 6128.1081s
	iters: 900, epoch: 4 | loss: 0.2904924 | orig_loss: 0.3113552 | penalty: 0.3113552
	speed: 0.0192s/iter; left time: 6128.6952s
	iters: 1000, epoch: 4 | loss: 0.2978239 | orig_loss: 0.3060396 | penalty: 0.3060396
	speed: 0.0192s/iter; left time: 6125.7786s
Epoch: 4 cost time: 20.689539432525635
Epoch Losses - Original: 0.3121464, With Penalty: 0.3121464, Difference: 0.0000000 (0.00%)
Epoch: 4, Steps: 1076 | Train Loss: 0.3137590 Vali Loss: 0.4942377 Test Loss: 0.4567200
Validation loss decreased (0.495421 --> 0.494238).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.3634425 | orig_loss: 0.3091290 | penalty: 0.3091290
	speed: 0.0575s/iter; left time: 18298.2839s
	iters: 200, epoch: 5 | loss: 0.3095713 | orig_loss: 0.3059121 | penalty: 0.3059121
	speed: 0.0188s/iter; left time: 5986.3251s
	iters: 300, epoch: 5 | loss: 0.2658216 | orig_loss: 0.3091965 | penalty: 0.3091965
	speed: 0.0188s/iter; left time: 5984.7789s
	iters: 400, epoch: 5 | loss: 0.2995467 | orig_loss: 0.3031610 | penalty: 0.3031610
	speed: 0.0188s/iter; left time: 5982.0312s
	iters: 500, epoch: 5 | loss: 0.3345248 | orig_loss: 0.2997499 | penalty: 0.2997499
	speed: 0.0188s/iter; left time: 5987.1114s
	iters: 600, epoch: 5 | loss: 0.3115171 | orig_loss: 0.3076843 | penalty: 0.3076843
	speed: 0.0188s/iter; left time: 5979.4979s
	iters: 700, epoch: 5 | loss: 0.3313128 | orig_loss: 0.3021129 | penalty: 0.3021129
	speed: 0.0188s/iter; left time: 5979.0081s
	iters: 800, epoch: 5 | loss: 0.3336271 | orig_loss: 0.2900570 | penalty: 0.2900570
	speed: 0.0188s/iter; left time: 5977.3532s
	iters: 900, epoch: 5 | loss: 0.2859046 | orig_loss: 0.2994714 | penalty: 0.2994714
	speed: 0.0188s/iter; left time: 5982.6387s
	iters: 1000, epoch: 5 | loss: 0.2790843 | orig_loss: 0.2965429 | penalty: 0.2965429
	speed: 0.0188s/iter; left time: 5973.4078s
Epoch: 5 cost time: 20.39824080467224
Epoch Losses - Original: 0.3017494, With Penalty: 0.3017494, Difference: 0.0000000 (0.00%)
Epoch: 5, Steps: 1076 | Train Loss: 0.3114353 Vali Loss: 0.4882889 Test Loss: 0.4529258
Validation loss decreased (0.494238 --> 0.488289).  Saving model ...
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.3312666 | orig_loss: 0.2940514 | penalty: 0.2940514
	speed: 0.0569s/iter; left time: 18057.5904s
	iters: 200, epoch: 6 | loss: 0.2727850 | orig_loss: 0.2840125 | penalty: 0.2840125
	speed: 0.0188s/iter; left time: 5961.9574s
	iters: 300, epoch: 6 | loss: 0.2606992 | orig_loss: 0.2958689 | penalty: 0.2958689
	speed: 0.0188s/iter; left time: 5961.4162s
	iters: 400, epoch: 6 | loss: 0.2814237 | orig_loss: 0.2974551 | penalty: 0.2974551
	speed: 0.0188s/iter; left time: 5960.9286s
	iters: 500, epoch: 6 | loss: 0.2533329 | orig_loss: 0.2888762 | penalty: 0.2888762
	speed: 0.0192s/iter; left time: 6099.4733s
	iters: 600, epoch: 6 | loss: 0.4378548 | orig_loss: 0.2982550 | penalty: 0.2982550
	speed: 0.0193s/iter; left time: 6102.8362s
	iters: 700, epoch: 6 | loss: 0.2811317 | orig_loss: 0.2930824 | penalty: 0.2930824
	speed: 0.0193s/iter; left time: 6100.0023s
	iters: 800, epoch: 6 | loss: 0.2768742 | orig_loss: 0.2934021 | penalty: 0.2934021
	speed: 0.0193s/iter; left time: 6098.8274s
	iters: 900, epoch: 6 | loss: 0.2680228 | orig_loss: 0.2973507 | penalty: 0.2973507
	speed: 0.0192s/iter; left time: 6092.0677s
	iters: 1000, epoch: 6 | loss: 0.2816589 | orig_loss: 0.2917047 | penalty: 0.2917047
	speed: 0.0193s/iter; left time: 6099.5105s
Epoch: 6 cost time: 20.69556212425232
Epoch Losses - Original: 0.2931293, With Penalty: 0.2931293, Difference: 0.0000000 (0.00%)
Epoch: 6, Steps: 1076 | Train Loss: 0.2945050 Vali Loss: 0.4890594 Test Loss: 0.4327234
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.2750963 | orig_loss: 0.2867861 | penalty: 0.2867861
	speed: 0.0573s/iter; left time: 18114.3965s
	iters: 200, epoch: 7 | loss: 0.2881438 | orig_loss: 0.2840462 | penalty: 0.2840462
	speed: 0.0187s/iter; left time: 5905.7862s
	iters: 300, epoch: 7 | loss: 0.2850507 | orig_loss: 0.2832782 | penalty: 0.2832782
	speed: 0.0187s/iter; left time: 5920.5097s
	iters: 400, epoch: 7 | loss: 0.2746974 | orig_loss: 0.2860494 | penalty: 0.2860494
	speed: 0.0187s/iter; left time: 5919.9387s
	iters: 500, epoch: 7 | loss: 0.2614790 | orig_loss: 0.2849083 | penalty: 0.2849083
	speed: 0.0187s/iter; left time: 5908.7387s
	iters: 600, epoch: 7 | loss: 0.2924407 | orig_loss: 0.2824886 | penalty: 0.2824886
	speed: 0.0187s/iter; left time: 5907.9843s
	iters: 700, epoch: 7 | loss: 0.3195491 | orig_loss: 0.2913251 | penalty: 0.2913251
	speed: 0.0187s/iter; left time: 5906.3224s
	iters: 800, epoch: 7 | loss: 0.2384829 | orig_loss: 0.2872002 | penalty: 0.2872002
	speed: 0.0187s/iter; left time: 5898.7103s
	iters: 900, epoch: 7 | loss: 0.2971008 | orig_loss: 0.2816286 | penalty: 0.2816286
	speed: 0.0187s/iter; left time: 5896.8928s
	iters: 1000, epoch: 7 | loss: 0.2729039 | orig_loss: 0.2836500 | penalty: 0.2836500
	speed: 0.0187s/iter; left time: 5891.5883s
Epoch: 7 cost time: 20.276116609573364
Epoch Losses - Original: 0.2851406, With Penalty: 0.2851406, Difference: 0.0000000 (0.00%)
Epoch: 7, Steps: 1076 | Train Loss: 0.2804945 Vali Loss: 0.4933063 Test Loss: 0.4405501
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.2844448 | orig_loss: 0.2760534 | penalty: 0.2760534
	speed: 0.0566s/iter; left time: 17834.0181s
	iters: 200, epoch: 8 | loss: 0.2839276 | orig_loss: 0.2825359 | penalty: 0.2825359
	speed: 0.0188s/iter; left time: 5916.2890s
	iters: 300, epoch: 8 | loss: 0.2848695 | orig_loss: 0.2825043 | penalty: 0.2825043
	speed: 0.0188s/iter; left time: 5914.0801s
	iters: 400, epoch: 8 | loss: 0.3206666 | orig_loss: 0.2774922 | penalty: 0.2774922
	speed: 0.0188s/iter; left time: 5914.7653s
	iters: 500, epoch: 8 | loss: 0.2438419 | orig_loss: 0.2863963 | penalty: 0.2863963
	speed: 0.0188s/iter; left time: 5914.0643s
	iters: 600, epoch: 8 | loss: 0.3333992 | orig_loss: 0.2749847 | penalty: 0.2749847
	speed: 0.0188s/iter; left time: 5912.4608s
	iters: 700, epoch: 8 | loss: 0.2576537 | orig_loss: 0.2779414 | penalty: 0.2779414
	speed: 0.0188s/iter; left time: 5909.4096s
	iters: 800, epoch: 8 | loss: 0.2922567 | orig_loss: 0.2786821 | penalty: 0.2786821
	speed: 0.0188s/iter; left time: 5903.7440s
	iters: 900, epoch: 8 | loss: 0.2692632 | orig_loss: 0.2746408 | penalty: 0.2746408
	speed: 0.0188s/iter; left time: 5905.1983s
	iters: 1000, epoch: 8 | loss: 0.2481090 | orig_loss: 0.2776907 | penalty: 0.2776907
	speed: 0.0188s/iter; left time: 5905.6328s
Epoch: 8 cost time: 20.351283311843872
Epoch Losses - Original: 0.2792959, With Penalty: 0.2792959, Difference: 0.0000000 (0.00%)
Epoch: 8, Steps: 1076 | Train Loss: 0.2818432 Vali Loss: 0.4959438 Test Loss: 0.4441183
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm1, seq_len: 48, pred_len: 96
Avg Original Loss: 0.3148484
Avg Penalty Loss: 0.3148663
Avg Difference: 0.0000179 (0.01%)
================================

>>>>>>>testing : long_term_forecast_ETTm1_48_96_SOFTS_ETTm1_ftM_sl48_ll48_pl96_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.45292581433409934, mae:0.4204299003472735
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm1_48_144', model='SOFTS', data='ETTm1', root_path='./dataset/ETT/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=144, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm1_48_144_SOFTS_ETTm1_ftM_sl48_ll48_pl144_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34369
val 11377
test 11377

------ Mask Loss Comparison ------
Original Loss: 0.6630356
Best Mask Loss: 0.6560709 (mask length: 3)
Improvement: 0.0069647 (1.05%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6665180
---------------------------------

	iters: 100, epoch: 1 | loss: 0.5099376 | orig_loss: 0.4919829 | penalty: 0.4925578
	speed: 0.0335s/iter; left time: 10788.4659s
	iters: 200, epoch: 1 | loss: 0.3750360 | orig_loss: 0.4462522 | penalty: 0.4462522
	speed: 0.0191s/iter; left time: 6162.8160s
	iters: 300, epoch: 1 | loss: 0.5500132 | orig_loss: 0.4381376 | penalty: 0.4381376
	speed: 0.0191s/iter; left time: 6140.0707s
	iters: 400, epoch: 1 | loss: 0.5271806 | orig_loss: 0.4275094 | penalty: 0.4275510
	speed: 0.0190s/iter; left time: 6127.4544s
	iters: 500, epoch: 1 | loss: 0.3999245 | orig_loss: 0.4146313 | penalty: 0.4146313
	speed: 0.0193s/iter; left time: 6198.5736s
	iters: 600, epoch: 1 | loss: 0.3847132 | orig_loss: 0.4122588 | penalty: 0.4124810
	speed: 0.0192s/iter; left time: 6185.2240s
	iters: 700, epoch: 1 | loss: 0.4225152 | orig_loss: 0.4145192 | penalty: 0.4147394
	speed: 0.0194s/iter; left time: 6238.5119s
	iters: 800, epoch: 1 | loss: 0.4189432 | orig_loss: 0.4144609 | penalty: 0.4145429
	speed: 0.0194s/iter; left time: 6244.8882s
	iters: 900, epoch: 1 | loss: 0.4965513 | orig_loss: 0.4089536 | penalty: 0.4089536
	speed: 0.0195s/iter; left time: 6255.8807s
	iters: 1000, epoch: 1 | loss: 0.4301803 | orig_loss: 0.4017704 | penalty: 0.4018250
	speed: 0.0194s/iter; left time: 6245.0599s
Epoch: 1 cost time: 21.251426458358765
Epoch Losses - Original: 0.4247663, With Penalty: 0.4248797, Difference: 0.0001135 (0.03%)
Epoch: 1, Steps: 1075 | Train Loss: 0.4514995 Vali Loss: 0.6029047 Test Loss: 0.5561827
Validation loss decreased (inf --> 0.602905).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.3417843 | orig_loss: 0.3956311 | penalty: 0.3956311
	speed: 0.0584s/iter; left time: 18755.6849s
	iters: 200, epoch: 2 | loss: 0.5376735 | orig_loss: 0.3991996 | penalty: 0.3991996
	speed: 0.0192s/iter; left time: 6152.3064s
	iters: 300, epoch: 2 | loss: 0.3667567 | orig_loss: 0.3862212 | penalty: 0.3863824
	speed: 0.0191s/iter; left time: 6121.7676s
	iters: 400, epoch: 2 | loss: 0.4282425 | orig_loss: 0.3985141 | penalty: 0.3985141
	speed: 0.0190s/iter; left time: 6101.4506s
	iters: 500, epoch: 2 | loss: 0.3261799 | orig_loss: 0.3911759 | penalty: 0.3911759
	speed: 0.0190s/iter; left time: 6094.9523s
	iters: 600, epoch: 2 | loss: 0.3860830 | orig_loss: 0.4033899 | penalty: 0.4033899
	speed: 0.0190s/iter; left time: 6103.4620s
	iters: 700, epoch: 2 | loss: 0.4904257 | orig_loss: 0.3895460 | penalty: 0.3895460
	speed: 0.0190s/iter; left time: 6092.8247s
	iters: 800, epoch: 2 | loss: 0.3774391 | orig_loss: 0.3768515 | penalty: 0.3768515
	speed: 0.0190s/iter; left time: 6088.1562s
	iters: 900, epoch: 2 | loss: 0.4625296 | orig_loss: 0.3874967 | penalty: 0.3874967
	speed: 0.0190s/iter; left time: 6087.3624s
	iters: 1000, epoch: 2 | loss: 0.3300925 | orig_loss: 0.3767546 | penalty: 0.3767546
	speed: 0.0189s/iter; left time: 6054.0639s
Epoch: 2 cost time: 20.60269260406494
Epoch Losses - Original: 0.3906532, With Penalty: 0.3906682, Difference: 0.0000150 (0.00%)
Epoch: 2, Steps: 1075 | Train Loss: 0.4047207 Vali Loss: 0.5989715 Test Loss: 0.5442880
Validation loss decreased (0.602905 --> 0.598971).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.3483692 | orig_loss: 0.3815592 | penalty: 0.3815592
	speed: 0.0580s/iter; left time: 18581.6733s
	iters: 200, epoch: 3 | loss: 0.3890888 | orig_loss: 0.3818657 | penalty: 0.3818657
	speed: 0.0189s/iter; left time: 6065.0379s
	iters: 300, epoch: 3 | loss: 0.3802819 | orig_loss: 0.3760250 | penalty: 0.3760250
	speed: 0.0190s/iter; left time: 6083.7682s
	iters: 400, epoch: 3 | loss: 0.3677786 | orig_loss: 0.3700361 | penalty: 0.3701828
	speed: 0.0189s/iter; left time: 6062.9387s
	iters: 500, epoch: 3 | loss: 0.3742213 | orig_loss: 0.3751106 | penalty: 0.3751106
	speed: 0.0191s/iter; left time: 6099.0616s
	iters: 600, epoch: 3 | loss: 0.3628334 | orig_loss: 0.3746191 | penalty: 0.3746191
	speed: 0.0190s/iter; left time: 6090.5040s
	iters: 700, epoch: 3 | loss: 0.3914245 | orig_loss: 0.3720773 | penalty: 0.3720773
	speed: 0.0190s/iter; left time: 6081.0201s
	iters: 800, epoch: 3 | loss: 0.3131343 | orig_loss: 0.3700608 | penalty: 0.3700608
	speed: 0.0191s/iter; left time: 6111.0827s
	iters: 900, epoch: 3 | loss: 0.4261218 | orig_loss: 0.3638580 | penalty: 0.3638580
	speed: 0.0193s/iter; left time: 6181.3195s
	iters: 1000, epoch: 3 | loss: 0.4080557 | orig_loss: 0.3669241 | penalty: 0.3669825
	speed: 0.0189s/iter; left time: 6047.6211s
Epoch: 3 cost time: 20.63097620010376
Epoch Losses - Original: 0.3726901, With Penalty: 0.3727130, Difference: 0.0000229 (0.01%)
Epoch: 3, Steps: 1075 | Train Loss: 0.3761310 Vali Loss: 0.5891686 Test Loss: 0.5207647
Validation loss decreased (0.598971 --> 0.589169).  Saving model ...
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.3974611 | orig_loss: 0.3662024 | penalty: 0.3662024
	speed: 0.0582s/iter; left time: 18582.9854s
	iters: 200, epoch: 4 | loss: 0.3708610 | orig_loss: 0.3721574 | penalty: 0.3721574
	speed: 0.0192s/iter; left time: 6112.8515s
	iters: 300, epoch: 4 | loss: 0.3569055 | orig_loss: 0.3584889 | penalty: 0.3584889
	speed: 0.0189s/iter; left time: 6042.0353s
	iters: 400, epoch: 4 | loss: 0.4104274 | orig_loss: 0.3603102 | penalty: 0.3603102
	speed: 0.0190s/iter; left time: 6063.0698s
	iters: 500, epoch: 4 | loss: 0.2820734 | orig_loss: 0.3578653 | penalty: 0.3578653
	speed: 0.0193s/iter; left time: 6153.9100s
	iters: 600, epoch: 4 | loss: 0.3545884 | orig_loss: 0.3556394 | penalty: 0.3556394
	speed: 0.0189s/iter; left time: 6032.8220s
	iters: 700, epoch: 4 | loss: 0.3573027 | orig_loss: 0.3521789 | penalty: 0.3521789
	speed: 0.0189s/iter; left time: 6024.5229s
	iters: 800, epoch: 4 | loss: 0.3239413 | orig_loss: 0.3537204 | penalty: 0.3537204
	speed: 0.0190s/iter; left time: 6042.9501s
	iters: 900, epoch: 4 | loss: 0.3515118 | orig_loss: 0.3544912 | penalty: 0.3544912
	speed: 0.0189s/iter; left time: 6025.9085s
	iters: 1000, epoch: 4 | loss: 0.3088673 | orig_loss: 0.3633839 | penalty: 0.3633839
	speed: 0.0191s/iter; left time: 6089.7941s
Epoch: 4 cost time: 20.65464425086975
Epoch Losses - Original: 0.3595154, With Penalty: 0.3595528, Difference: 0.0000374 (0.01%)
Epoch: 4, Steps: 1075 | Train Loss: 0.3513940 Vali Loss: 0.5826865 Test Loss: 0.5053814
Validation loss decreased (0.589169 --> 0.582687).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.3723028 | orig_loss: 0.3425590 | penalty: 0.3425590
	speed: 0.0578s/iter; left time: 18390.8246s
	iters: 200, epoch: 5 | loss: 0.3392814 | orig_loss: 0.3625199 | penalty: 0.3625199
	speed: 0.0190s/iter; left time: 6028.7065s
	iters: 300, epoch: 5 | loss: 0.3776302 | orig_loss: 0.3540537 | penalty: 0.3540537
	speed: 0.0189s/iter; left time: 6015.4864s
	iters: 400, epoch: 5 | loss: 0.3055618 | orig_loss: 0.3433331 | penalty: 0.3433331
	speed: 0.0189s/iter; left time: 6016.6606s
	iters: 500, epoch: 5 | loss: 0.3675784 | orig_loss: 0.3487646 | penalty: 0.3487646
	speed: 0.0189s/iter; left time: 6016.4587s
	iters: 600, epoch: 5 | loss: 0.4063843 | orig_loss: 0.3470940 | penalty: 0.3470940
	speed: 0.0192s/iter; left time: 6087.6087s
	iters: 700, epoch: 5 | loss: 0.3379111 | orig_loss: 0.3423117 | penalty: 0.3423117
	speed: 0.0191s/iter; left time: 6059.7888s
	iters: 800, epoch: 5 | loss: 0.3244227 | orig_loss: 0.3493859 | penalty: 0.3493859
	speed: 0.0189s/iter; left time: 6003.7186s
	iters: 900, epoch: 5 | loss: 0.3613614 | orig_loss: 0.3424221 | penalty: 0.3424221
	speed: 0.0188s/iter; left time: 5972.8144s
	iters: 1000, epoch: 5 | loss: 0.3208970 | orig_loss: 0.3473494 | penalty: 0.3473494
	speed: 0.0189s/iter; left time: 5981.2384s
Epoch: 5 cost time: 20.52968716621399
Epoch Losses - Original: 0.3476206, With Penalty: 0.3476206, Difference: 0.0000000 (0.00%)
Epoch: 5, Steps: 1075 | Train Loss: 0.3513331 Vali Loss: 0.5922648 Test Loss: 0.5090100
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.3376090 | orig_loss: 0.3376761 | penalty: 0.3376761
	speed: 0.0574s/iter; left time: 18196.4512s
	iters: 200, epoch: 6 | loss: 0.3194147 | orig_loss: 0.3391964 | penalty: 0.3392075
	speed: 0.0188s/iter; left time: 5972.4449s
	iters: 300, epoch: 6 | loss: 0.3611794 | orig_loss: 0.3389978 | penalty: 0.3389978
	speed: 0.0192s/iter; left time: 6071.5242s
	iters: 400, epoch: 6 | loss: 0.3102081 | orig_loss: 0.3385042 | penalty: 0.3385042
	speed: 0.0192s/iter; left time: 6081.0111s
	iters: 500, epoch: 6 | loss: 0.3344491 | orig_loss: 0.3372903 | penalty: 0.3372903
	speed: 0.0190s/iter; left time: 6029.0265s
	iters: 600, epoch: 6 | loss: 0.3688984 | orig_loss: 0.3463354 | penalty: 0.3463354
	speed: 0.0189s/iter; left time: 5974.0284s
	iters: 700, epoch: 6 | loss: 0.2673553 | orig_loss: 0.3347031 | penalty: 0.3347031
	speed: 0.0190s/iter; left time: 6027.1472s
	iters: 800, epoch: 6 | loss: 0.3641752 | orig_loss: 0.3415698 | penalty: 0.3415698
	speed: 0.0189s/iter; left time: 5982.7753s
	iters: 900, epoch: 6 | loss: 0.4026819 | orig_loss: 0.3316593 | penalty: 0.3316593
	speed: 0.0191s/iter; left time: 6035.1248s
	iters: 1000, epoch: 6 | loss: 0.3273360 | orig_loss: 0.3309054 | penalty: 0.3309054
	speed: 0.0191s/iter; left time: 6032.7588s
Epoch: 6 cost time: 20.56759214401245
Epoch Losses - Original: 0.3379848, With Penalty: 0.3379859, Difference: 0.0000010 (0.00%)
Epoch: 6, Steps: 1075 | Train Loss: 0.3393307 Vali Loss: 0.5779226 Test Loss: 0.5075749
Validation loss decreased (0.582687 --> 0.577923).  Saving model ...
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.3209145 | orig_loss: 0.3306335 | penalty: 0.3306335
	speed: 0.0574s/iter; left time: 18131.8135s
	iters: 200, epoch: 7 | loss: 0.3465894 | orig_loss: 0.3340706 | penalty: 0.3340706
	speed: 0.0189s/iter; left time: 5980.8491s
	iters: 300, epoch: 7 | loss: 0.3074306 | orig_loss: 0.3360587 | penalty: 0.3360587
	speed: 0.0191s/iter; left time: 6027.8693s
	iters: 400, epoch: 7 | loss: 0.3356124 | orig_loss: 0.3276711 | penalty: 0.3276711
	speed: 0.0193s/iter; left time: 6076.2884s
	iters: 500, epoch: 7 | loss: 0.3262779 | orig_loss: 0.3270354 | penalty: 0.3270354
	speed: 0.0193s/iter; left time: 6095.1187s
	iters: 600, epoch: 7 | loss: 0.3422945 | orig_loss: 0.3326111 | penalty: 0.3326111
	speed: 0.0192s/iter; left time: 6055.5967s
	iters: 700, epoch: 7 | loss: 0.3049624 | orig_loss: 0.3284528 | penalty: 0.3284528
	speed: 0.0189s/iter; left time: 5960.2884s
	iters: 800, epoch: 7 | loss: 0.2717018 | orig_loss: 0.3327471 | penalty: 0.3327471
	speed: 0.0190s/iter; left time: 5996.6248s
	iters: 900, epoch: 7 | loss: 0.4121371 | orig_loss: 0.3204779 | penalty: 0.3204779
	speed: 0.0194s/iter; left time: 6106.4180s
	iters: 1000, epoch: 7 | loss: 0.2843037 | orig_loss: 0.3356000 | penalty: 0.3356000
	speed: 0.0194s/iter; left time: 6102.6311s
Epoch: 7 cost time: 20.738545656204224
Epoch Losses - Original: 0.3300689, With Penalty: 0.3300689, Difference: 0.0000000 (0.00%)
Epoch: 7, Steps: 1075 | Train Loss: 0.3252224 Vali Loss: 0.5949657 Test Loss: 0.4896010
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.3436597 | orig_loss: 0.3192122 | penalty: 0.3192122
	speed: 0.0580s/iter; left time: 18262.8889s
	iters: 200, epoch: 8 | loss: 0.3533190 | orig_loss: 0.3228669 | penalty: 0.3228669
	speed: 0.0192s/iter; left time: 6029.9371s
	iters: 300, epoch: 8 | loss: 0.3604116 | orig_loss: 0.3243684 | penalty: 0.3243684
	speed: 0.0188s/iter; left time: 5927.0728s
	iters: 400, epoch: 8 | loss: 0.3155745 | orig_loss: 0.3280370 | penalty: 0.3281824
	speed: 0.0188s/iter; left time: 5921.8480s
	iters: 500, epoch: 8 | loss: 0.2920118 | orig_loss: 0.3237373 | penalty: 0.3237373
	speed: 0.0188s/iter; left time: 5920.1672s
	iters: 600, epoch: 8 | loss: 0.3363136 | orig_loss: 0.3250238 | penalty: 0.3250238
	speed: 0.0190s/iter; left time: 5972.3153s
	iters: 700, epoch: 8 | loss: 0.3169086 | orig_loss: 0.3211994 | penalty: 0.3211994
	speed: 0.0189s/iter; left time: 5932.6475s
	iters: 800, epoch: 8 | loss: 0.3021234 | orig_loss: 0.3262828 | penalty: 0.3262828
	speed: 0.0190s/iter; left time: 5978.4970s
	iters: 900, epoch: 8 | loss: 0.2910302 | orig_loss: 0.3185212 | penalty: 0.3185212
	speed: 0.0189s/iter; left time: 5929.0016s
	iters: 1000, epoch: 8 | loss: 0.3326279 | orig_loss: 0.3175745 | penalty: 0.3175745
	speed: 0.0190s/iter; left time: 5960.9203s
Epoch: 8 cost time: 20.531763553619385
Epoch Losses - Original: 0.3227758, With Penalty: 0.3227893, Difference: 0.0000135 (0.00%)
Epoch: 8, Steps: 1075 | Train Loss: 0.3243980 Vali Loss: 0.5790161 Test Loss: 0.4951523
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002994739288874256
	iters: 100, epoch: 9 | loss: 0.2845884 | orig_loss: 0.3063766 | penalty: 0.3063766
	speed: 0.0578s/iter; left time: 18139.8936s
	iters: 200, epoch: 9 | loss: 0.3793401 | orig_loss: 0.3154698 | penalty: 0.3154698
	speed: 0.0191s/iter; left time: 5982.5228s
	iters: 300, epoch: 9 | loss: 0.2776011 | orig_loss: 0.3131091 | penalty: 0.3131091
	speed: 0.0189s/iter; left time: 5919.5091s
	iters: 400, epoch: 9 | loss: 0.2697419 | orig_loss: 0.3217082 | penalty: 0.3217082
	speed: 0.0189s/iter; left time: 5915.3665s
	iters: 500, epoch: 9 | loss: 0.3115781 | orig_loss: 0.3246862 | penalty: 0.3246862
	speed: 0.0190s/iter; left time: 5947.2339s
	iters: 600, epoch: 9 | loss: 0.2965316 | orig_loss: 0.3206953 | penalty: 0.3206953
	speed: 0.0189s/iter; left time: 5931.6234s
	iters: 700, epoch: 9 | loss: 0.2536921 | orig_loss: 0.3165126 | penalty: 0.3165126
	speed: 0.0191s/iter; left time: 5992.9856s
	iters: 800, epoch: 9 | loss: 0.3699067 | orig_loss: 0.3182314 | penalty: 0.3182314
	speed: 0.0191s/iter; left time: 5965.8781s
	iters: 900, epoch: 9 | loss: 0.3043489 | orig_loss: 0.3171188 | penalty: 0.3171188
	speed: 0.0190s/iter; left time: 5941.7246s
	iters: 1000, epoch: 9 | loss: 0.3755746 | orig_loss: 0.3125575 | penalty: 0.3125575
	speed: 0.0189s/iter; left time: 5911.6688s
Epoch: 9 cost time: 20.579056978225708
Epoch Losses - Original: 0.3163394, With Penalty: 0.3163394, Difference: 0.0000000 (0.00%)
Epoch: 9, Steps: 1075 | Train Loss: 0.3122903 Vali Loss: 0.5855392 Test Loss: 0.4927354
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm1, seq_len: 48, pred_len: 144
Avg Original Loss: 0.3558238
Avg Penalty Loss: 0.3558464
Avg Difference: 0.0000226 (0.01%)
================================

>>>>>>>testing : long_term_forecast_ETTm1_48_144_SOFTS_ETTm1_ftM_sl48_ll48_pl144_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11377
mse:0.5075748578660925, mae:0.44751234128750345
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm1_48_192', model='SOFTS', data='ETTm1', root_path='./dataset/ETT/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=192, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm1_48_192_SOFTS_ETTm1_ftM_sl48_ll48_pl192_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34321
val 11329
test 11329

------ Mask Loss Comparison ------
Original Loss: 0.8430394
Best Mask Loss: 0.8412634 (mask length: 3)
Improvement: 0.0017760 (0.21%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.8439274
---------------------------------

	iters: 100, epoch: 1 | loss: 0.4515764 | orig_loss: 0.4892350 | penalty: 0.4903207
	speed: 0.0335s/iter; left time: 10790.8965s
	iters: 200, epoch: 1 | loss: 0.3896261 | orig_loss: 0.4371080 | penalty: 0.4371520
	speed: 0.0188s/iter; left time: 6050.0685s
	iters: 300, epoch: 1 | loss: 0.4066407 | orig_loss: 0.4285798 | penalty: 0.4285798
	speed: 0.0188s/iter; left time: 6043.2491s
	iters: 400, epoch: 1 | loss: 0.4335820 | orig_loss: 0.4234164 | penalty: 0.4235528
	speed: 0.0189s/iter; left time: 6064.4353s
	iters: 500, epoch: 1 | loss: 0.4471467 | orig_loss: 0.4178819 | penalty: 0.4178819
	speed: 0.0193s/iter; left time: 6192.6066s
	iters: 600, epoch: 1 | loss: 0.3818476 | orig_loss: 0.4180442 | penalty: 0.4181470
	speed: 0.0189s/iter; left time: 6079.5012s
	iters: 700, epoch: 1 | loss: 0.5113502 | orig_loss: 0.4131492 | penalty: 0.4131684
	speed: 0.0187s/iter; left time: 6020.0590s
	iters: 800, epoch: 1 | loss: 0.3761972 | orig_loss: 0.4114421 | penalty: 0.4115057
	speed: 0.0187s/iter; left time: 6020.0650s
	iters: 900, epoch: 1 | loss: 0.4431781 | orig_loss: 0.4104772 | penalty: 0.4104772
	speed: 0.0187s/iter; left time: 6018.5261s
	iters: 1000, epoch: 1 | loss: 0.4027422 | orig_loss: 0.4098308 | penalty: 0.4098308
	speed: 0.0187s/iter; left time: 6016.7247s
Epoch: 1 cost time: 20.81126093864441
Epoch Losses - Original: 0.4241080, With Penalty: 0.4242433, Difference: 0.0001353 (0.03%)
Epoch: 1, Steps: 1073 | Train Loss: 0.4243887 Vali Loss: 0.6265847 Test Loss: 0.5269869
Validation loss decreased (inf --> 0.626585).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.4056856 | orig_loss: 0.4019774 | penalty: 0.4020076
	speed: 0.0579s/iter; left time: 18573.1524s
	iters: 200, epoch: 2 | loss: 0.3725212 | orig_loss: 0.3979750 | penalty: 0.3979750
	speed: 0.0189s/iter; left time: 6064.2849s
	iters: 300, epoch: 2 | loss: 0.3821666 | orig_loss: 0.3859501 | penalty: 0.3859501
	speed: 0.0189s/iter; left time: 6053.2469s
	iters: 400, epoch: 2 | loss: 0.3268723 | orig_loss: 0.3891389 | penalty: 0.3891389
	speed: 0.0188s/iter; left time: 6018.3690s
	iters: 500, epoch: 2 | loss: 0.3885874 | orig_loss: 0.3934963 | penalty: 0.3935610
	speed: 0.0189s/iter; left time: 6041.2216s
	iters: 600, epoch: 2 | loss: 0.3174580 | orig_loss: 0.4109612 | penalty: 0.4109612
	speed: 0.0188s/iter; left time: 6025.5594s
	iters: 700, epoch: 2 | loss: 0.4494897 | orig_loss: 0.3959401 | penalty: 0.3959632
	speed: 0.0189s/iter; left time: 6051.9911s
	iters: 800, epoch: 2 | loss: 0.4466718 | orig_loss: 0.4001766 | penalty: 0.4002233
	speed: 0.0190s/iter; left time: 6090.8489s
	iters: 900, epoch: 2 | loss: 0.3165745 | orig_loss: 0.3920507 | penalty: 0.3920540
	speed: 0.0189s/iter; left time: 6058.6158s
	iters: 1000, epoch: 2 | loss: 0.3498254 | orig_loss: 0.3875480 | penalty: 0.3875480
	speed: 0.0188s/iter; left time: 6006.7130s
Epoch: 2 cost time: 20.3842670917511
Epoch Losses - Original: 0.3954345, With Penalty: 0.3954501, Difference: 0.0000157 (0.00%)
Epoch: 2, Steps: 1073 | Train Loss: 0.3755852 Vali Loss: 0.6168041 Test Loss: 0.5145608
Validation loss decreased (0.626585 --> 0.616804).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.4351257 | orig_loss: 0.3892931 | penalty: 0.3892931
	speed: 0.0581s/iter; left time: 18573.1724s
	iters: 200, epoch: 3 | loss: 0.3747526 | orig_loss: 0.3779805 | penalty: 0.3779805
	speed: 0.0191s/iter; left time: 6117.6473s
	iters: 300, epoch: 3 | loss: 0.3574305 | orig_loss: 0.3827213 | penalty: 0.3827213
	speed: 0.0188s/iter; left time: 5990.4766s
	iters: 400, epoch: 3 | loss: 0.4495662 | orig_loss: 0.3820594 | penalty: 0.3821011
	speed: 0.0188s/iter; left time: 5994.3089s
	iters: 500, epoch: 3 | loss: 0.3366929 | orig_loss: 0.3883277 | penalty: 0.3883593
	speed: 0.0192s/iter; left time: 6140.6197s
	iters: 600, epoch: 3 | loss: 0.3858722 | orig_loss: 0.3820542 | penalty: 0.3820542
	speed: 0.0192s/iter; left time: 6129.4685s
	iters: 700, epoch: 3 | loss: 0.3898132 | orig_loss: 0.3825216 | penalty: 0.3825216
	speed: 0.0190s/iter; left time: 6055.6335s
	iters: 800, epoch: 3 | loss: 0.3867656 | orig_loss: 0.3731215 | penalty: 0.3731215
	speed: 0.0187s/iter; left time: 5973.1151s
	iters: 900, epoch: 3 | loss: 0.4165655 | orig_loss: 0.3755101 | penalty: 0.3756147
	speed: 0.0187s/iter; left time: 5967.3539s
	iters: 1000, epoch: 3 | loss: 0.4222294 | orig_loss: 0.3739017 | penalty: 0.3739017
	speed: 0.0187s/iter; left time: 5972.6634s
Epoch: 3 cost time: 20.4347026348114
Epoch Losses - Original: 0.3807175, With Penalty: 0.3807341, Difference: 0.0000166 (0.00%)
Epoch: 3, Steps: 1073 | Train Loss: 0.3954814 Vali Loss: 0.6121331 Test Loss: 0.4966398
Validation loss decreased (0.616804 --> 0.612133).  Saving model ...
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.3787379 | orig_loss: 0.3717976 | penalty: 0.3717976
	speed: 0.0583s/iter; left time: 18583.2175s
	iters: 200, epoch: 4 | loss: 0.3715689 | orig_loss: 0.3779197 | penalty: 0.3779197
	speed: 0.0190s/iter; left time: 6064.5975s
	iters: 300, epoch: 4 | loss: 0.4730311 | orig_loss: 0.3820361 | penalty: 0.3820361
	speed: 0.0191s/iter; left time: 6083.7145s
	iters: 400, epoch: 4 | loss: 0.2884766 | orig_loss: 0.3609258 | penalty: 0.3609258
	speed: 0.0191s/iter; left time: 6064.7047s
	iters: 500, epoch: 4 | loss: 0.3893474 | orig_loss: 0.3617342 | penalty: 0.3617342
	speed: 0.0189s/iter; left time: 6006.9970s
	iters: 600, epoch: 4 | loss: 0.3755250 | orig_loss: 0.3696833 | penalty: 0.3697229
	speed: 0.0189s/iter; left time: 6000.0910s
	iters: 700, epoch: 4 | loss: 0.2784074 | orig_loss: 0.3690462 | penalty: 0.3690462
	speed: 0.0190s/iter; left time: 6052.6383s
	iters: 800, epoch: 4 | loss: 0.3154777 | orig_loss: 0.3658892 | penalty: 0.3658892
	speed: 0.0189s/iter; left time: 6000.1032s
	iters: 900, epoch: 4 | loss: 0.3223389 | orig_loss: 0.3653110 | penalty: 0.3653895
	speed: 0.0190s/iter; left time: 6029.7234s
	iters: 1000, epoch: 4 | loss: 0.3933788 | orig_loss: 0.3695341 | penalty: 0.3695341
	speed: 0.0190s/iter; left time: 6023.1065s
Epoch: 4 cost time: 20.52755880355835
Epoch Losses - Original: 0.3687563, With Penalty: 0.3687673, Difference: 0.0000110 (0.00%)
Epoch: 4, Steps: 1073 | Train Loss: 0.3586290 Vali Loss: 0.6111857 Test Loss: 0.4944571
Validation loss decreased (0.612133 --> 0.611186).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.3803135 | orig_loss: 0.3671406 | penalty: 0.3671406
	speed: 0.0584s/iter; left time: 18549.8417s
	iters: 200, epoch: 5 | loss: 0.4091960 | orig_loss: 0.3531303 | penalty: 0.3531303
	speed: 0.0188s/iter; left time: 5957.1346s
	iters: 300, epoch: 5 | loss: 0.2902756 | orig_loss: 0.3696752 | penalty: 0.3696752
	speed: 0.0188s/iter; left time: 5962.9796s
	iters: 400, epoch: 5 | loss: 0.2898453 | orig_loss: 0.3647905 | penalty: 0.3647905
	speed: 0.0188s/iter; left time: 5959.1855s
	iters: 500, epoch: 5 | loss: 0.3397069 | orig_loss: 0.3609679 | penalty: 0.3609679
	speed: 0.0188s/iter; left time: 5961.2693s
	iters: 600, epoch: 5 | loss: 0.3242513 | orig_loss: 0.3562586 | penalty: 0.3562586
	speed: 0.0188s/iter; left time: 5958.8921s
	iters: 700, epoch: 5 | loss: 0.4241758 | orig_loss: 0.3557408 | penalty: 0.3557408
	speed: 0.0188s/iter; left time: 5948.6860s
	iters: 800, epoch: 5 | loss: 0.3324657 | orig_loss: 0.3517551 | penalty: 0.3517551
	speed: 0.0188s/iter; left time: 5944.3374s
	iters: 900, epoch: 5 | loss: 0.3739725 | orig_loss: 0.3568276 | penalty: 0.3568276
	speed: 0.0188s/iter; left time: 5942.9036s
	iters: 1000, epoch: 5 | loss: 0.3816853 | orig_loss: 0.3574581 | penalty: 0.3574581
	speed: 0.0188s/iter; left time: 5947.8314s
Epoch: 5 cost time: 20.302547216415405
Epoch Losses - Original: 0.3590084, With Penalty: 0.3590084, Difference: 0.0000000 (0.00%)
Epoch: 5, Steps: 1073 | Train Loss: 0.3545888 Vali Loss: 0.6186134 Test Loss: 0.4837817
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.4085906 | orig_loss: 0.3509370 | penalty: 0.3509370
	speed: 0.0581s/iter; left time: 18398.0904s
	iters: 200, epoch: 6 | loss: 0.3090366 | orig_loss: 0.3597088 | penalty: 0.3597088
	speed: 0.0188s/iter; left time: 5946.5470s
	iters: 300, epoch: 6 | loss: 0.3710502 | orig_loss: 0.3486920 | penalty: 0.3486920
	speed: 0.0188s/iter; left time: 5957.0918s
	iters: 400, epoch: 6 | loss: 0.3147365 | orig_loss: 0.3497365 | penalty: 0.3497365
	speed: 0.0189s/iter; left time: 5986.8201s
	iters: 500, epoch: 6 | loss: 0.2973129 | orig_loss: 0.3547291 | penalty: 0.3547291
	speed: 0.0191s/iter; left time: 6048.9472s
	iters: 600, epoch: 6 | loss: 0.2994314 | orig_loss: 0.3510373 | penalty: 0.3510373
	speed: 0.0189s/iter; left time: 5966.7338s
	iters: 700, epoch: 6 | loss: 0.3837789 | orig_loss: 0.3475590 | penalty: 0.3475590
	speed: 0.0192s/iter; left time: 6061.3895s
	iters: 800, epoch: 6 | loss: 0.3944552 | orig_loss: 0.3490943 | penalty: 0.3490943
	speed: 0.0192s/iter; left time: 6059.6428s
	iters: 900, epoch: 6 | loss: 0.4137628 | orig_loss: 0.3457431 | penalty: 0.3457431
	speed: 0.0192s/iter; left time: 6064.2014s
	iters: 1000, epoch: 6 | loss: 0.3484357 | orig_loss: 0.3452280 | penalty: 0.3452280
	speed: 0.0192s/iter; left time: 6044.8645s
Epoch: 6 cost time: 20.584163665771484
Epoch Losses - Original: 0.3509794, With Penalty: 0.3509794, Difference: 0.0000000 (0.00%)
Epoch: 6, Steps: 1073 | Train Loss: 0.3540591 Vali Loss: 0.6153832 Test Loss: 0.4892299
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.3252686 | orig_loss: 0.3416195 | penalty: 0.3416195
	speed: 0.0581s/iter; left time: 18315.8540s
	iters: 200, epoch: 7 | loss: 0.3762810 | orig_loss: 0.3389490 | penalty: 0.3389490
	speed: 0.0188s/iter; left time: 5933.8900s
	iters: 300, epoch: 7 | loss: 0.3136328 | orig_loss: 0.3446679 | penalty: 0.3446679
	speed: 0.0189s/iter; left time: 5964.5422s
	iters: 400, epoch: 7 | loss: 0.3663871 | orig_loss: 0.3442501 | penalty: 0.3442501
	speed: 0.0190s/iter; left time: 5996.2247s
	iters: 500, epoch: 7 | loss: 0.3545180 | orig_loss: 0.3498653 | penalty: 0.3498653
	speed: 0.0188s/iter; left time: 5918.8395s
	iters: 600, epoch: 7 | loss: 0.2903846 | orig_loss: 0.3470866 | penalty: 0.3470866
	speed: 0.0187s/iter; left time: 5900.6049s
	iters: 700, epoch: 7 | loss: 0.3320245 | orig_loss: 0.3464620 | penalty: 0.3464620
	speed: 0.0187s/iter; left time: 5890.0849s
	iters: 800, epoch: 7 | loss: 0.2963026 | orig_loss: 0.3433478 | penalty: 0.3433478
	speed: 0.0187s/iter; left time: 5897.1450s
	iters: 900, epoch: 7 | loss: 0.3249793 | orig_loss: 0.3454499 | penalty: 0.3454499
	speed: 0.0187s/iter; left time: 5883.5435s
	iters: 1000, epoch: 7 | loss: 0.3032496 | orig_loss: 0.3355417 | penalty: 0.3355417
	speed: 0.0189s/iter; left time: 5930.7217s
Epoch: 7 cost time: 20.353878259658813
Epoch Losses - Original: 0.3436727, With Penalty: 0.3436727, Difference: 0.0000000 (0.00%)
Epoch: 7, Steps: 1073 | Train Loss: 0.3283028 Vali Loss: 0.6036773 Test Loss: 0.4968447
Validation loss decreased (0.611186 --> 0.603677).  Saving model ...
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.3419766 | orig_loss: 0.3367807 | penalty: 0.3367807
	speed: 0.0581s/iter; left time: 18250.5226s
	iters: 200, epoch: 8 | loss: 0.3442044 | orig_loss: 0.3389146 | penalty: 0.3389146
	speed: 0.0188s/iter; left time: 5908.1926s
	iters: 300, epoch: 8 | loss: 0.3460697 | orig_loss: 0.3365175 | penalty: 0.3365175
	speed: 0.0189s/iter; left time: 5922.6753s
	iters: 400, epoch: 8 | loss: 0.3719474 | orig_loss: 0.3367684 | penalty: 0.3367684
	speed: 0.0187s/iter; left time: 5883.0634s
	iters: 500, epoch: 8 | loss: 0.2863798 | orig_loss: 0.3421416 | penalty: 0.3421416
	speed: 0.0190s/iter; left time: 5969.7678s
	iters: 600, epoch: 8 | loss: 0.3916606 | orig_loss: 0.3338799 | penalty: 0.3338799
	speed: 0.0187s/iter; left time: 5872.2432s
	iters: 700, epoch: 8 | loss: 0.3182527 | orig_loss: 0.3367022 | penalty: 0.3367022
	speed: 0.0187s/iter; left time: 5869.2283s
	iters: 800, epoch: 8 | loss: 0.2933726 | orig_loss: 0.3339126 | penalty: 0.3339126
	speed: 0.0189s/iter; left time: 5922.8424s
	iters: 900, epoch: 8 | loss: 0.3081131 | orig_loss: 0.3382497 | penalty: 0.3382497
	speed: 0.0188s/iter; left time: 5908.0458s
	iters: 1000, epoch: 8 | loss: 0.3011720 | orig_loss: 0.3323015 | penalty: 0.3323015
	speed: 0.0189s/iter; left time: 5924.5044s
Epoch: 8 cost time: 20.347896575927734
Epoch Losses - Original: 0.3369629, With Penalty: 0.3369629, Difference: 0.0000000 (0.00%)
Epoch: 8, Steps: 1073 | Train Loss: 0.3303149 Vali Loss: 0.5987106 Test Loss: 0.4858881
Validation loss decreased (0.603677 --> 0.598711).  Saving model ...
Updating learning rate to 0.0002994739288874256
	iters: 100, epoch: 9 | loss: 0.4107989 | orig_loss: 0.3275076 | penalty: 0.3275076
	speed: 0.0580s/iter; left time: 18166.3909s
	iters: 200, epoch: 9 | loss: 0.3382609 | orig_loss: 0.3359648 | penalty: 0.3359648
	speed: 0.0188s/iter; left time: 5888.1423s
	iters: 300, epoch: 9 | loss: 0.3289092 | orig_loss: 0.3312896 | penalty: 0.3312896
	speed: 0.0187s/iter; left time: 5844.9277s
	iters: 400, epoch: 9 | loss: 0.3257648 | orig_loss: 0.3318912 | penalty: 0.3318912
	speed: 0.0187s/iter; left time: 5837.2166s
	iters: 500, epoch: 9 | loss: 0.3049268 | orig_loss: 0.3238951 | penalty: 0.3238951
	speed: 0.0187s/iter; left time: 5844.6455s
	iters: 600, epoch: 9 | loss: 0.2993621 | orig_loss: 0.3329724 | penalty: 0.3329724
	speed: 0.0191s/iter; left time: 5972.9926s
	iters: 700, epoch: 9 | loss: 0.3022659 | orig_loss: 0.3372117 | penalty: 0.3372117
	speed: 0.0188s/iter; left time: 5870.6022s
	iters: 800, epoch: 9 | loss: 0.3168428 | orig_loss: 0.3276519 | penalty: 0.3276519
	speed: 0.0192s/iter; left time: 5990.9928s
	iters: 900, epoch: 9 | loss: 0.3447507 | orig_loss: 0.3307666 | penalty: 0.3307666
	speed: 0.0187s/iter; left time: 5831.1922s
	iters: 1000, epoch: 9 | loss: 0.3740432 | orig_loss: 0.3246840 | penalty: 0.3246840
	speed: 0.0188s/iter; left time: 5870.9069s
Epoch: 9 cost time: 20.323413372039795
Epoch Losses - Original: 0.3304219, With Penalty: 0.3304219, Difference: 0.0000000 (0.00%)
Epoch: 9, Steps: 1073 | Train Loss: 0.3345925 Vali Loss: 0.6010107 Test Loss: 0.4853447
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299334294690462
	iters: 100, epoch: 10 | loss: 0.2775529 | orig_loss: 0.3259970 | penalty: 0.3259970
	speed: 0.0586s/iter; left time: 18297.5575s
	iters: 200, epoch: 10 | loss: 0.2735979 | orig_loss: 0.3263409 | penalty: 0.3263409
	speed: 0.0188s/iter; left time: 5879.5594s
	iters: 300, epoch: 10 | loss: 0.2642443 | orig_loss: 0.3264042 | penalty: 0.3264042
	speed: 0.0193s/iter; left time: 6005.2677s
	iters: 400, epoch: 10 | loss: 0.3654225 | orig_loss: 0.3294083 | penalty: 0.3294083
	speed: 0.0193s/iter; left time: 6032.8273s
	iters: 500, epoch: 10 | loss: 0.3357552 | orig_loss: 0.3318581 | penalty: 0.3318581
	speed: 0.0191s/iter; left time: 5945.8651s
	iters: 600, epoch: 10 | loss: 0.3318736 | orig_loss: 0.3180053 | penalty: 0.3180053
	speed: 0.0189s/iter; left time: 5898.5676s
	iters: 700, epoch: 10 | loss: 0.3027251 | orig_loss: 0.3243534 | penalty: 0.3243534
	speed: 0.0193s/iter; left time: 6011.8746s
	iters: 800, epoch: 10 | loss: 0.2504969 | orig_loss: 0.3221707 | penalty: 0.3221707
	speed: 0.0193s/iter; left time: 5999.2152s
	iters: 900, epoch: 10 | loss: 0.2474905 | orig_loss: 0.3222702 | penalty: 0.3222702
	speed: 0.0193s/iter; left time: 5997.5191s
	iters: 1000, epoch: 10 | loss: 0.3580340 | orig_loss: 0.3261150 | penalty: 0.3261150
	speed: 0.0192s/iter; left time: 5988.1217s
Epoch: 10 cost time: 20.727672576904297
Epoch Losses - Original: 0.3248662, With Penalty: 0.3248662, Difference: 0.0000000 (0.00%)
Epoch: 10, Steps: 1073 | Train Loss: 0.3007193 Vali Loss: 0.6017808 Test Loss: 0.4874375
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029917828430524096
	iters: 100, epoch: 11 | loss: 0.2865576 | orig_loss: 0.3145089 | penalty: 0.3145089
	speed: 0.0584s/iter; left time: 18173.4116s
	iters: 200, epoch: 11 | loss: 0.2347673 | orig_loss: 0.3190027 | penalty: 0.3190027
	speed: 0.0190s/iter; left time: 5905.7291s
	iters: 300, epoch: 11 | loss: 0.2993965 | orig_loss: 0.3211140 | penalty: 0.3211140
	speed: 0.0190s/iter; left time: 5906.9206s
	iters: 400, epoch: 11 | loss: 0.3535680 | orig_loss: 0.3262182 | penalty: 0.3262182
	speed: 0.0188s/iter; left time: 5829.6659s
	iters: 500, epoch: 11 | loss: 0.2609992 | orig_loss: 0.3204874 | penalty: 0.3204874
	speed: 0.0190s/iter; left time: 5898.3940s
	iters: 600, epoch: 11 | loss: 0.3208306 | orig_loss: 0.3221265 | penalty: 0.3221265
	speed: 0.0190s/iter; left time: 5890.6946s
	iters: 700, epoch: 11 | loss: 0.2933419 | orig_loss: 0.3184750 | penalty: 0.3184750
	speed: 0.0188s/iter; left time: 5836.3910s
	iters: 800, epoch: 11 | loss: 0.4339477 | orig_loss: 0.3223937 | penalty: 0.3223937
	speed: 0.0192s/iter; left time: 5945.3583s
	iters: 900, epoch: 11 | loss: 0.3727735 | orig_loss: 0.3133477 | penalty: 0.3133477
	speed: 0.0192s/iter; left time: 5948.3280s
	iters: 1000, epoch: 11 | loss: 0.2928499 | orig_loss: 0.3242369 | penalty: 0.3242369
	speed: 0.0191s/iter; left time: 5929.5398s
Epoch: 11 cost time: 20.539685487747192
Epoch Losses - Original: 0.3201771, With Penalty: 0.3201771, Difference: 0.0000000 (0.00%)
Epoch: 11, Steps: 1073 | Train Loss: 0.3149032 Vali Loss: 0.6014732 Test Loss: 0.5007830
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm1, seq_len: 48, pred_len: 192
Avg Original Loss: 0.3577368
Avg Penalty Loss: 0.3577530
Avg Difference: 0.0000162 (0.00%)
================================

>>>>>>>testing : long_term_forecast_ETTm1_48_192_SOFTS_ETTm1_ftM_sl48_ll48_pl192_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.48588808583780785, mae:0.4389887444391519
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm2_48_48', model='SOFTS', data='ETTm2', root_path='./dataset/ETT/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=48, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm2_48_48_SOFTS_ETTm2_ftM_sl48_ll48_pl48_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34465
val 11473
test 11473

------ Mask Loss Comparison ------
Original Loss: 0.2531555
Best Mask Loss: 0.2492905 (mask length: 3)
Improvement: 0.0038651 (1.53%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2550881
---------------------------------

	iters: 100, epoch: 1 | loss: 0.2119698 | orig_loss: 0.2026962 | penalty: 0.2048543
	speed: 0.0329s/iter; left time: 10638.3194s
	iters: 200, epoch: 1 | loss: 0.1622284 | orig_loss: 0.1943768 | penalty: 0.1974424
	speed: 0.0186s/iter; left time: 6025.9533s
	iters: 300, epoch: 1 | loss: 0.1338198 | orig_loss: 0.1846212 | penalty: 0.1869059
	speed: 0.0186s/iter; left time: 6013.9744s
	iters: 400, epoch: 1 | loss: 0.1576610 | orig_loss: 0.1956953 | penalty: 0.1988563
	speed: 0.0186s/iter; left time: 6010.1054s
	iters: 500, epoch: 1 | loss: 0.1808799 | orig_loss: 0.1727650 | penalty: 0.1742028
	speed: 0.0186s/iter; left time: 6004.0413s

------ Mask Loss Comparison ------
Original Loss: 0.5695328
Best Mask Loss: 0.5184218 (mask length: 27)
Improvement: 0.0511109 (8.97%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5950882
---------------------------------

	iters: 600, epoch: 1 | loss: 0.2483071 | orig_loss: 0.2037255 | penalty: 0.2059997
	speed: 0.0186s/iter; left time: 6011.0225s
	iters: 700, epoch: 1 | loss: 0.2663122 | orig_loss: 0.1859278 | penalty: 0.1875480
	speed: 0.0187s/iter; left time: 6025.7043s
	iters: 800, epoch: 1 | loss: 0.1363479 | orig_loss: 0.1877744 | penalty: 0.1897914
	speed: 0.0187s/iter; left time: 6023.8424s
	iters: 900, epoch: 1 | loss: 0.1550105 | orig_loss: 0.1782817 | penalty: 0.1793952
	speed: 0.0186s/iter; left time: 6013.7264s
	iters: 1000, epoch: 1 | loss: 0.2601418 | orig_loss: 0.1798409 | penalty: 0.1816675
	speed: 0.0186s/iter; left time: 6009.8662s
Epoch: 1 cost time: 20.643718004226685
Epoch Losses - Original: 0.1885616, With Penalty: 0.1906917, Difference: 0.0021301 (1.13%)
Epoch: 1, Steps: 1078 | Train Loss: 0.1912679 Vali Loss: 0.1272700 Test Loss: 0.1683574
Validation loss decreased (inf --> 0.127270).  Saving model ...
Updating learning rate to 0.0002999917754048268

------ Mask Loss Comparison ------
Original Loss: 0.1569645
Best Mask Loss: 0.1405995 (mask length: 3)
Improvement: 0.0163650 (10.43%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.1651469
---------------------------------

	iters: 100, epoch: 2 | loss: 0.1395825 | orig_loss: 0.1851455 | penalty: 0.1885632
	speed: 0.0567s/iter; left time: 18278.5380s
	iters: 200, epoch: 2 | loss: 0.1263510 | orig_loss: 0.1854918 | penalty: 0.1883199
	speed: 0.0186s/iter; left time: 5984.9925s
	iters: 300, epoch: 2 | loss: 0.1885280 | orig_loss: 0.1752325 | penalty: 0.1770804
	speed: 0.0186s/iter; left time: 5983.8531s
	iters: 400, epoch: 2 | loss: 0.0780731 | orig_loss: 0.1723018 | penalty: 0.1738864
	speed: 0.0186s/iter; left time: 5977.9017s
	iters: 500, epoch: 2 | loss: 0.1331897 | orig_loss: 0.1786715 | penalty: 0.1796254
	speed: 0.0186s/iter; left time: 5973.6846s
	iters: 600, epoch: 2 | loss: 0.2488847 | orig_loss: 0.1671778 | penalty: 0.1685165
	speed: 0.0185s/iter; left time: 5967.4915s
	iters: 700, epoch: 2 | loss: 0.4518207 | orig_loss: 0.1772570 | penalty: 0.1791118
	speed: 0.0186s/iter; left time: 5982.2787s
	iters: 800, epoch: 2 | loss: 0.1131586 | orig_loss: 0.1645253 | penalty: 0.1660007
	speed: 0.0186s/iter; left time: 5973.4828s
	iters: 900, epoch: 2 | loss: 0.1440259 | orig_loss: 0.1745266 | penalty: 0.1760333
	speed: 0.0186s/iter; left time: 5977.9241s

------ Mask Loss Comparison ------
Original Loss: 0.1649697
Best Mask Loss: 0.1570239 (mask length: 3)
Improvement: 0.0079458 (4.82%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.1689426
---------------------------------

	iters: 1000, epoch: 2 | loss: 0.1312475 | orig_loss: 0.1881396 | penalty: 0.1902626
	speed: 0.0186s/iter; left time: 5992.4273s
Epoch: 2 cost time: 20.217901706695557
Epoch Losses - Original: 0.1758963, With Penalty: 0.1777929, Difference: 0.0018966 (1.08%)
Epoch: 2, Steps: 1078 | Train Loss: 0.1754862 Vali Loss: 0.1218166 Test Loss: 0.1607982
Validation loss decreased (0.127270 --> 0.121817).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.1178667 | orig_loss: 0.1743328 | penalty: 0.1756088
	speed: 0.0559s/iter; left time: 17944.3416s
	iters: 200, epoch: 3 | loss: 0.1803326 | orig_loss: 0.1620724 | penalty: 0.1631594
	speed: 0.0185s/iter; left time: 5952.1003s
	iters: 300, epoch: 3 | loss: 0.1466346 | orig_loss: 0.1670728 | penalty: 0.1684589
	speed: 0.0185s/iter; left time: 5943.5226s
	iters: 400, epoch: 3 | loss: 0.1719483 | orig_loss: 0.1659056 | penalty: 0.1666573
	speed: 0.0185s/iter; left time: 5945.3517s
	iters: 500, epoch: 3 | loss: 0.2560360 | orig_loss: 0.1581864 | penalty: 0.1601537
	speed: 0.0185s/iter; left time: 5944.8966s
	iters: 600, epoch: 3 | loss: 0.1059976 | orig_loss: 0.1622309 | penalty: 0.1634968
	speed: 0.0185s/iter; left time: 5938.9501s
	iters: 700, epoch: 3 | loss: 0.4897429 | orig_loss: 0.1735241 | penalty: 0.1753163
	speed: 0.0185s/iter; left time: 5935.3577s
	iters: 800, epoch: 3 | loss: 0.1153450 | orig_loss: 0.1698392 | penalty: 0.1713148
	speed: 0.0185s/iter; left time: 5939.3706s
	iters: 900, epoch: 3 | loss: 0.1692137 | orig_loss: 0.1654904 | penalty: 0.1666445
	speed: 0.0185s/iter; left time: 5934.7546s
	iters: 1000, epoch: 3 | loss: 0.3451487 | orig_loss: 0.1728667 | penalty: 0.1738945
	speed: 0.0185s/iter; left time: 5936.7929s
Epoch: 3 cost time: 20.11468529701233
Epoch Losses - Original: 0.1676045, With Penalty: 0.1689508, Difference: 0.0013463 (0.80%)
Epoch: 3, Steps: 1078 | Train Loss: 0.2098266 Vali Loss: 0.1194660 Test Loss: 0.1594228
Validation loss decreased (0.121817 --> 0.119466).  Saving model ...
Updating learning rate to 0.0002999259840548597

------ Mask Loss Comparison ------
Original Loss: 0.4243295
Best Mask Loss: 0.4006986 (mask length: 6)
Improvement: 0.0236309 (5.57%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4361450
---------------------------------

	iters: 100, epoch: 4 | loss: 0.1595233 | orig_loss: 0.1567546 | penalty: 0.1571505
	speed: 0.0563s/iter; left time: 18021.4284s
	iters: 200, epoch: 4 | loss: 0.1381268 | orig_loss: 0.1633556 | penalty: 0.1639527
	speed: 0.0185s/iter; left time: 5928.6915s
	iters: 300, epoch: 4 | loss: 0.1728987 | orig_loss: 0.1588244 | penalty: 0.1595355
	speed: 0.0185s/iter; left time: 5925.2905s
	iters: 400, epoch: 4 | loss: 0.1879569 | orig_loss: 0.1519765 | penalty: 0.1531761
	speed: 0.0185s/iter; left time: 5920.1194s
	iters: 500, epoch: 4 | loss: 0.1853711 | orig_loss: 0.1648003 | penalty: 0.1663130
	speed: 0.0185s/iter; left time: 5918.5820s
	iters: 600, epoch: 4 | loss: 0.1123021 | orig_loss: 0.1547029 | penalty: 0.1552194
	speed: 0.0185s/iter; left time: 5915.2326s
	iters: 700, epoch: 4 | loss: 0.1837526 | orig_loss: 0.1617155 | penalty: 0.1630815
	speed: 0.0185s/iter; left time: 5916.0673s
	iters: 800, epoch: 4 | loss: 0.1303493 | orig_loss: 0.1590424 | penalty: 0.1602588
	speed: 0.0185s/iter; left time: 5908.3661s
	iters: 900, epoch: 4 | loss: 0.1286618 | orig_loss: 0.1541490 | penalty: 0.1549339
	speed: 0.0185s/iter; left time: 5914.1942s
	iters: 1000, epoch: 4 | loss: 0.4777715 | orig_loss: 0.1752270 | penalty: 0.1771943
	speed: 0.0185s/iter; left time: 5918.7429s
Epoch: 4 cost time: 20.11308455467224
Epoch Losses - Original: 0.1608739, With Penalty: 0.1620860, Difference: 0.0012121 (0.75%)
Epoch: 4, Steps: 1078 | Train Loss: 0.1876714 Vali Loss: 0.1162878 Test Loss: 0.1530645
Validation loss decreased (0.119466 --> 0.116288).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.2842922 | orig_loss: 0.1657319 | penalty: 0.1669696
	speed: 0.0558s/iter; left time: 17804.4298s
	iters: 200, epoch: 5 | loss: 0.1083915 | orig_loss: 0.1552726 | penalty: 0.1558115
	speed: 0.0185s/iter; left time: 5904.3926s
	iters: 300, epoch: 5 | loss: 0.1265097 | orig_loss: 0.1621477 | penalty: 0.1631506
	speed: 0.0185s/iter; left time: 5903.5960s
	iters: 400, epoch: 5 | loss: 0.1224178 | orig_loss: 0.1451176 | penalty: 0.1452565
	speed: 0.0185s/iter; left time: 5899.7337s
	iters: 500, epoch: 5 | loss: 0.1691447 | orig_loss: 0.1611399 | penalty: 0.1616807
	speed: 0.0185s/iter; left time: 5896.5965s
	iters: 600, epoch: 5 | loss: 0.0828958 | orig_loss: 0.1574214 | penalty: 0.1583513
	speed: 0.0185s/iter; left time: 5896.4215s

------ Mask Loss Comparison ------
Original Loss: 0.3347390
Best Mask Loss: 0.2783626 (mask length: 6)
Improvement: 0.0563764 (16.84%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3629272
---------------------------------

	iters: 700, epoch: 5 | loss: 0.0796982 | orig_loss: 0.1550805 | penalty: 0.1562691
	speed: 0.0185s/iter; left time: 5884.1538s
	iters: 800, epoch: 5 | loss: 0.2550344 | orig_loss: 0.1571592 | penalty: 0.1585562
	speed: 0.0185s/iter; left time: 5887.1639s
	iters: 900, epoch: 5 | loss: 0.3376952 | orig_loss: 0.1485813 | penalty: 0.1493039
	speed: 0.0185s/iter; left time: 5885.5085s
	iters: 1000, epoch: 5 | loss: 0.1260958 | orig_loss: 0.1532552 | penalty: 0.1537847
	speed: 0.0185s/iter; left time: 5884.8419s
Epoch: 5 cost time: 20.09534525871277
Epoch Losses - Original: 0.1560216, With Penalty: 0.1568537, Difference: 0.0008321 (0.53%)
Epoch: 5, Steps: 1078 | Train Loss: 0.1692175 Vali Loss: 0.1191426 Test Loss: 0.1596011
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.1193142 | orig_loss: 0.1544343 | penalty: 0.1557471
	speed: 0.0562s/iter; left time: 17879.8682s
	iters: 200, epoch: 6 | loss: 0.1643093 | orig_loss: 0.1472449 | penalty: 0.1476017
	speed: 0.0185s/iter; left time: 5883.8116s
	iters: 300, epoch: 6 | loss: 0.1414085 | orig_loss: 0.1490094 | penalty: 0.1492114
	speed: 0.0185s/iter; left time: 5877.4714s
	iters: 400, epoch: 6 | loss: 0.0970279 | orig_loss: 0.1477761 | penalty: 0.1479265
	speed: 0.0185s/iter; left time: 5878.9117s
	iters: 500, epoch: 6 | loss: 0.1322696 | orig_loss: 0.1471077 | penalty: 0.1474510
	speed: 0.0185s/iter; left time: 5884.2258s
	iters: 600, epoch: 6 | loss: 0.1321530 | orig_loss: 0.1561702 | penalty: 0.1569650
	speed: 0.0186s/iter; left time: 5893.1345s
	iters: 700, epoch: 6 | loss: 0.2128314 | orig_loss: 0.1611538 | penalty: 0.1615116
	speed: 0.0185s/iter; left time: 5882.0921s
	iters: 800, epoch: 6 | loss: 0.3231245 | orig_loss: 0.1719876 | penalty: 0.1733968
	speed: 0.0185s/iter; left time: 5876.9296s
	iters: 900, epoch: 6 | loss: 0.1159958 | orig_loss: 0.1558924 | penalty: 0.1567811
	speed: 0.0185s/iter; left time: 5874.2846s
	iters: 1000, epoch: 6 | loss: 0.1247351 | orig_loss: 0.1421770 | penalty: 0.1430319
	speed: 0.0185s/iter; left time: 5873.6634s
Epoch: 6 cost time: 20.173118829727173
Epoch Losses - Original: 0.1528343, With Penalty: 0.1535001, Difference: 0.0006658 (0.44%)
Epoch: 6, Steps: 1078 | Train Loss: 0.1563169 Vali Loss: 0.1156878 Test Loss: 0.1537159
Validation loss decreased (0.116288 --> 0.115688).  Saving model ...
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.0927965 | orig_loss: 0.1465296 | penalty: 0.1472473
	speed: 0.0570s/iter; left time: 18054.5439s
	iters: 200, epoch: 7 | loss: 0.0972228 | orig_loss: 0.1452348 | penalty: 0.1455007
	speed: 0.0189s/iter; left time: 5978.2026s
	iters: 300, epoch: 7 | loss: 0.1425138 | orig_loss: 0.1549810 | penalty: 0.1554264
	speed: 0.0185s/iter; left time: 5861.6976s
	iters: 400, epoch: 7 | loss: 0.1660213 | orig_loss: 0.1501805 | penalty: 0.1509209
	speed: 0.0185s/iter; left time: 5859.8268s
	iters: 500, epoch: 7 | loss: 0.1915049 | orig_loss: 0.1480351 | penalty: 0.1489921
	speed: 0.0185s/iter; left time: 5858.6130s
	iters: 600, epoch: 7 | loss: 0.3446690 | orig_loss: 0.1519333 | penalty: 0.1525409
	speed: 0.0185s/iter; left time: 5865.3232s
	iters: 700, epoch: 7 | loss: 0.1154442 | orig_loss: 0.1563413 | penalty: 0.1566258
	speed: 0.0187s/iter; left time: 5903.9189s
	iters: 800, epoch: 7 | loss: 0.1706162 | orig_loss: 0.1451518 | penalty: 0.1456987
	speed: 0.0186s/iter; left time: 5878.0543s

------ Mask Loss Comparison ------
Original Loss: 0.3376558
Best Mask Loss: 0.3030746 (mask length: 12)
Improvement: 0.0345813 (10.24%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3549465
---------------------------------

	iters: 900, epoch: 7 | loss: 0.0814377 | orig_loss: 0.1500904 | penalty: 0.1504888
	speed: 0.0186s/iter; left time: 5880.1725s
	iters: 1000, epoch: 7 | loss: 0.3053051 | orig_loss: 0.1460164 | penalty: 0.1467014
	speed: 0.0186s/iter; left time: 5882.2898s
Epoch: 7 cost time: 20.23259973526001
Epoch Losses - Original: 0.1489816, With Penalty: 0.1495446, Difference: 0.0005630 (0.38%)
Epoch: 7, Steps: 1078 | Train Loss: 0.1707531 Vali Loss: 0.1170503 Test Loss: 0.1565301
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.2916989 | orig_loss: 0.1448466 | penalty: 0.1451683
	speed: 0.0564s/iter; left time: 17797.1372s
	iters: 200, epoch: 8 | loss: 0.1789002 | orig_loss: 0.1425119 | penalty: 0.1432382
	speed: 0.0187s/iter; left time: 5895.1087s
	iters: 300, epoch: 8 | loss: 0.1068019 | orig_loss: 0.1397955 | penalty: 0.1399832
	speed: 0.0186s/iter; left time: 5864.9794s
	iters: 400, epoch: 8 | loss: 0.1449083 | orig_loss: 0.1396551 | penalty: 0.1400391
	speed: 0.0186s/iter; left time: 5857.6319s
	iters: 500, epoch: 8 | loss: 0.1287444 | orig_loss: 0.1507978 | penalty: 0.1510334
	speed: 0.0186s/iter; left time: 5858.4096s
	iters: 600, epoch: 8 | loss: 0.1104749 | orig_loss: 0.1658756 | penalty: 0.1665964
	speed: 0.0186s/iter; left time: 5857.4102s
	iters: 700, epoch: 8 | loss: 0.3094166 | orig_loss: 0.1365656 | penalty: 0.1376807
	speed: 0.0186s/iter; left time: 5852.2626s
	iters: 800, epoch: 8 | loss: 0.1359869 | orig_loss: 0.1446818 | penalty: 0.1448713
	speed: 0.0185s/iter; left time: 5840.9668s
	iters: 900, epoch: 8 | loss: 0.1185971 | orig_loss: 0.1512844 | penalty: 0.1518009
	speed: 0.0186s/iter; left time: 5850.4846s
	iters: 1000, epoch: 8 | loss: 0.1361450 | orig_loss: 0.1446950 | penalty: 0.1449143
	speed: 0.0186s/iter; left time: 5842.8213s
Epoch: 8 cost time: 20.190230131149292
Epoch Losses - Original: 0.1458328, With Penalty: 0.1462612, Difference: 0.0004284 (0.29%)
Epoch: 8, Steps: 1078 | Train Loss: 0.1661674 Vali Loss: 0.1180815 Test Loss: 0.1554016
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002994739288874256
	iters: 100, epoch: 9 | loss: 0.1032398 | orig_loss: 0.1353390 | penalty: 0.1354078
	speed: 0.0566s/iter; left time: 17799.6501s
	iters: 200, epoch: 9 | loss: 0.1903314 | orig_loss: 0.1386905 | penalty: 0.1386905
	speed: 0.0185s/iter; left time: 5824.9528s
	iters: 300, epoch: 9 | loss: 0.2118364 | orig_loss: 0.1485901 | penalty: 0.1488893
	speed: 0.0185s/iter; left time: 5827.3845s
	iters: 400, epoch: 9 | loss: 0.1152313 | orig_loss: 0.1418812 | penalty: 0.1422055
	speed: 0.0185s/iter; left time: 5820.0149s
	iters: 500, epoch: 9 | loss: 0.1188314 | orig_loss: 0.1440684 | penalty: 0.1456952
	speed: 0.0185s/iter; left time: 5819.0373s
	iters: 600, epoch: 9 | loss: 0.0955198 | orig_loss: 0.1535800 | penalty: 0.1536830
	speed: 0.0185s/iter; left time: 5815.4060s
	iters: 700, epoch: 9 | loss: 0.1259311 | orig_loss: 0.1286096 | penalty: 0.1286707
	speed: 0.0185s/iter; left time: 5818.2920s
	iters: 800, epoch: 9 | loss: 0.1581445 | orig_loss: 0.1494212 | penalty: 0.1498754
	speed: 0.0185s/iter; left time: 5806.1331s
	iters: 900, epoch: 9 | loss: 0.2764131 | orig_loss: 0.1408166 | penalty: 0.1409498
	speed: 0.0185s/iter; left time: 5800.5489s
	iters: 1000, epoch: 9 | loss: 0.0940164 | orig_loss: 0.1515567 | penalty: 0.1516990
	speed: 0.0185s/iter; left time: 5799.9696s
Epoch: 9 cost time: 20.13053011894226
Epoch Losses - Original: 0.1434559, With Penalty: 0.1437677, Difference: 0.0003118 (0.22%)
Epoch: 9, Steps: 1078 | Train Loss: 0.1489495 Vali Loss: 0.1156617 Test Loss: 0.1565695
Validation loss decreased (0.115688 --> 0.115662).  Saving model ...
Updating learning rate to 0.000299334294690462
	iters: 100, epoch: 10 | loss: 0.1045518 | orig_loss: 0.1468823 | penalty: 0.1481937
	speed: 0.0564s/iter; left time: 17702.1461s
	iters: 200, epoch: 10 | loss: 0.1084223 | orig_loss: 0.1388012 | penalty: 0.1388333
	speed: 0.0186s/iter; left time: 5822.8099s
	iters: 300, epoch: 10 | loss: 0.1057506 | orig_loss: 0.1285654 | penalty: 0.1285654
	speed: 0.0186s/iter; left time: 5818.5959s
	iters: 400, epoch: 10 | loss: 0.1136554 | orig_loss: 0.1450558 | penalty: 0.1456204
	speed: 0.0185s/iter; left time: 5809.6304s
	iters: 500, epoch: 10 | loss: 0.1075321 | orig_loss: 0.1363875 | penalty: 0.1364735
	speed: 0.0185s/iter; left time: 5803.7281s
	iters: 600, epoch: 10 | loss: 0.1265612 | orig_loss: 0.1544334 | penalty: 0.1546398
	speed: 0.0185s/iter; left time: 5799.8043s
	iters: 700, epoch: 10 | loss: 0.1497124 | orig_loss: 0.1473625 | penalty: 0.1477963
	speed: 0.0185s/iter; left time: 5805.3121s
	iters: 800, epoch: 10 | loss: 0.1271517 | orig_loss: 0.1343354 | penalty: 0.1343354
	speed: 0.0185s/iter; left time: 5798.0339s
	iters: 900, epoch: 10 | loss: 0.1428788 | orig_loss: 0.1326000 | penalty: 0.1327165
	speed: 0.0185s/iter; left time: 5786.6954s
	iters: 1000, epoch: 10 | loss: 0.1237022 | orig_loss: 0.1430512 | penalty: 0.1435079
	speed: 0.0185s/iter; left time: 5785.0632s
Epoch: 10 cost time: 20.136186838150024
Epoch Losses - Original: 0.1413847, With Penalty: 0.1417264, Difference: 0.0003417 (0.24%)
Epoch: 10, Steps: 1078 | Train Loss: 0.1209919 Vali Loss: 0.1206271 Test Loss: 0.1583000
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029917828430524096
	iters: 100, epoch: 11 | loss: 0.1504117 | orig_loss: 0.1329922 | penalty: 0.1329922
	speed: 0.0565s/iter; left time: 17670.0649s
	iters: 200, epoch: 11 | loss: 0.0957119 | orig_loss: 0.1312101 | penalty: 0.1312608
	speed: 0.0190s/iter; left time: 5936.9177s
	iters: 300, epoch: 11 | loss: 0.1754595 | orig_loss: 0.1370250 | penalty: 0.1375035
	speed: 0.0188s/iter; left time: 5866.7571s
	iters: 400, epoch: 11 | loss: 0.2663482 | orig_loss: 0.1454245 | penalty: 0.1457114
	speed: 0.0185s/iter; left time: 5771.0561s
	iters: 500, epoch: 11 | loss: 0.1385216 | orig_loss: 0.1330163 | penalty: 0.1331022
	speed: 0.0185s/iter; left time: 5771.8859s
	iters: 600, epoch: 11 | loss: 0.0965880 | orig_loss: 0.1433702 | penalty: 0.1435477
	speed: 0.0185s/iter; left time: 5786.8789s
	iters: 700, epoch: 11 | loss: 0.1475990 | orig_loss: 0.1431298 | penalty: 0.1436170
	speed: 0.0185s/iter; left time: 5766.8689s
	iters: 800, epoch: 11 | loss: 0.1027144 | orig_loss: 0.1332426 | penalty: 0.1333493
	speed: 0.0185s/iter; left time: 5762.1794s

------ Mask Loss Comparison ------
Original Loss: 0.2941825
Best Mask Loss: 0.2904895 (mask length: 12)
Improvement: 0.0036930 (1.26%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2960290
---------------------------------

	iters: 900, epoch: 11 | loss: 0.3119338 | orig_loss: 0.1240314 | penalty: 0.1241135
	speed: 0.0185s/iter; left time: 5762.9193s
	iters: 1000, epoch: 11 | loss: 0.2120392 | orig_loss: 0.1344228 | penalty: 0.1344844
	speed: 0.0185s/iter; left time: 5764.7794s
Epoch: 11 cost time: 20.215614557266235
Epoch Losses - Original: 0.1365308, With Penalty: 0.1367195, Difference: 0.0001887 (0.14%)
Epoch: 11, Steps: 1078 | Train Loss: 0.1697327 Vali Loss: 0.1189998 Test Loss: 0.1591618
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002990059148400594
	iters: 100, epoch: 12 | loss: 0.1314442 | orig_loss: 0.1344516 | penalty: 0.1344516
	speed: 0.0560s/iter; left time: 17429.6978s
	iters: 200, epoch: 12 | loss: 0.2140837 | orig_loss: 0.1365493 | penalty: 0.1366262
	speed: 0.0184s/iter; left time: 5735.8370s
	iters: 300, epoch: 12 | loss: 0.1238597 | orig_loss: 0.1367480 | penalty: 0.1367480
	speed: 0.0184s/iter; left time: 5735.6362s
	iters: 400, epoch: 12 | loss: 0.0895303 | orig_loss: 0.1327037 | penalty: 0.1328969
	speed: 0.0184s/iter; left time: 5736.1115s
	iters: 500, epoch: 12 | loss: 0.0856265 | orig_loss: 0.1371702 | penalty: 0.1374457
	speed: 0.0184s/iter; left time: 5732.3584s
	iters: 600, epoch: 12 | loss: 0.1040309 | orig_loss: 0.1397465 | penalty: 0.1397465
	speed: 0.0184s/iter; left time: 5722.8262s
	iters: 700, epoch: 12 | loss: 0.0955119 | orig_loss: 0.1262321 | penalty: 0.1262908
	speed: 0.0184s/iter; left time: 5721.5601s
	iters: 800, epoch: 12 | loss: 0.1264261 | orig_loss: 0.1344035 | penalty: 0.1347348
	speed: 0.0184s/iter; left time: 5726.1746s
	iters: 900, epoch: 12 | loss: 0.1034283 | orig_loss: 0.1361826 | penalty: 0.1369928
	speed: 0.0184s/iter; left time: 5719.9836s
	iters: 1000, epoch: 12 | loss: 0.0924758 | orig_loss: 0.1319738 | penalty: 0.1322165
	speed: 0.0184s/iter; left time: 5727.3276s
Epoch: 12 cost time: 20.012481927871704
Epoch Losses - Original: 0.1342194, With Penalty: 0.1344039, Difference: 0.0001845 (0.14%)
Epoch: 12, Steps: 1078 | Train Loss: 0.1166417 Vali Loss: 0.1196976 Test Loss: 0.1571578
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm2, seq_len: 48, pred_len: 48
Avg Original Loss: 0.1543498
Avg Penalty Loss: 0.1551915
Avg Difference: 0.0008418 (0.55%)
================================

>>>>>>>testing : long_term_forecast_ETTm2_48_48_SOFTS_ETTm2_ftM_sl48_ll48_pl48_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11473
mse:0.15656951999904326, mae:0.24771789685612536
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm2_48_72', model='SOFTS', data='ETTm2', root_path='./dataset/ETT/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=72, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm2_48_72_SOFTS_ETTm2_ftM_sl48_ll48_pl72_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34441
val 11449
test 11449

------ Mask Loss Comparison ------
Original Loss: 0.5233387
Best Mask Loss: 0.5008646 (mask length: 3)
Improvement: 0.0224741 (4.29%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5345757
---------------------------------

	iters: 100, epoch: 1 | loss: 0.2713095 | orig_loss: 0.2499881 | penalty: 0.2539055
	speed: 0.0329s/iter; left time: 10611.6759s
	iters: 200, epoch: 1 | loss: 0.1378563 | orig_loss: 0.2220153 | penalty: 0.2262613
	speed: 0.0186s/iter; left time: 6021.7588s
	iters: 300, epoch: 1 | loss: 0.1559198 | orig_loss: 0.2144848 | penalty: 0.2166207
	speed: 0.0186s/iter; left time: 6007.3968s
	iters: 400, epoch: 1 | loss: 0.1862717 | orig_loss: 0.2295940 | penalty: 0.2325884
	speed: 0.0186s/iter; left time: 6004.4110s
	iters: 500, epoch: 1 | loss: 0.1572315 | orig_loss: 0.2308035 | penalty: 0.2323242
	speed: 0.0186s/iter; left time: 5996.2534s

------ Mask Loss Comparison ------
Original Loss: 0.5093666
Best Mask Loss: 0.4221528 (mask length: 12)
Improvement: 0.0872138 (17.12%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5529736
---------------------------------

	iters: 600, epoch: 1 | loss: 0.1504110 | orig_loss: 0.2158020 | penalty: 0.2181206
	speed: 0.0186s/iter; left time: 5997.6832s
	iters: 700, epoch: 1 | loss: 0.3148587 | orig_loss: 0.2071910 | penalty: 0.2093703
	speed: 0.0186s/iter; left time: 5985.5434s
	iters: 800, epoch: 1 | loss: 0.3118338 | orig_loss: 0.2299036 | penalty: 0.2327800
	speed: 0.0186s/iter; left time: 5988.1260s
	iters: 900, epoch: 1 | loss: 0.4168204 | orig_loss: 0.2179341 | penalty: 0.2201757
	speed: 0.0189s/iter; left time: 6102.9211s
	iters: 1000, epoch: 1 | loss: 0.2050560 | orig_loss: 0.2140628 | penalty: 0.2158103
	speed: 0.0191s/iter; left time: 6136.6491s

------ Mask Loss Comparison ------
Original Loss: 0.5740139
Best Mask Loss: 0.5151730 (mask length: 30)
Improvement: 0.0588409 (10.25%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6034343
---------------------------------

Epoch: 1 cost time: 20.699429273605347
Epoch Losses - Original: 0.2233472, With Penalty: 0.2259371, Difference: 0.0025899 (1.16%)
Epoch: 1, Steps: 1077 | Train Loss: 0.2307569 Vali Loss: 0.1330994 Test Loss: 0.1831720
Validation loss decreased (inf --> 0.133099).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.5114859 | orig_loss: 0.2167419 | penalty: 0.2191776
	speed: 0.0574s/iter; left time: 18469.8981s
	iters: 200, epoch: 2 | loss: 0.2016157 | orig_loss: 0.2022665 | penalty: 0.2042090
	speed: 0.0187s/iter; left time: 6012.5932s
	iters: 300, epoch: 2 | loss: 0.1300288 | orig_loss: 0.2072931 | penalty: 0.2093544
	speed: 0.0186s/iter; left time: 5995.4568s
	iters: 400, epoch: 2 | loss: 0.2371694 | orig_loss: 0.2341644 | penalty: 0.2374954
	speed: 0.0186s/iter; left time: 5993.2689s
	iters: 500, epoch: 2 | loss: 0.1606888 | orig_loss: 0.2268561 | penalty: 0.2306598
	speed: 0.0190s/iter; left time: 6100.8391s

------ Mask Loss Comparison ------
Original Loss: 0.2593694
Best Mask Loss: 0.2558994 (mask length: 3)
Improvement: 0.0034700 (1.34%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2611044
---------------------------------

	iters: 600, epoch: 2 | loss: 0.2181738 | orig_loss: 0.2251047 | penalty: 0.2273965
	speed: 0.0190s/iter; left time: 6106.4931s
	iters: 700, epoch: 2 | loss: 0.1787613 | orig_loss: 0.2205847 | penalty: 0.2223674
	speed: 0.0187s/iter; left time: 6000.0364s
	iters: 800, epoch: 2 | loss: 0.1540715 | orig_loss: 0.2058546 | penalty: 0.2076830
	speed: 0.0189s/iter; left time: 6080.7523s
	iters: 900, epoch: 2 | loss: 0.1809693 | orig_loss: 0.1951537 | penalty: 0.1974238
	speed: 0.0191s/iter; left time: 6139.5821s
	iters: 1000, epoch: 2 | loss: 0.1390404 | orig_loss: 0.2028551 | penalty: 0.2043361
	speed: 0.0191s/iter; left time: 6137.2684s
Epoch: 2 cost time: 20.475480556488037
Epoch Losses - Original: 0.2143467, With Penalty: 0.2166324, Difference: 0.0022858 (1.07%)
Epoch: 2, Steps: 1077 | Train Loss: 0.2112005 Vali Loss: 0.1302300 Test Loss: 0.1776797
Validation loss decreased (0.133099 --> 0.130230).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.0964222 | orig_loss: 0.2112055 | penalty: 0.2129103
	speed: 0.0570s/iter; left time: 18275.4370s

------ Mask Loss Comparison ------
Original Loss: 0.2359265
Best Mask Loss: 0.2345897 (mask length: 3)
Improvement: 0.0013369 (0.57%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2365949
---------------------------------

	iters: 200, epoch: 3 | loss: 0.1967695 | orig_loss: 0.2036755 | penalty: 0.2052228
	speed: 0.0187s/iter; left time: 5992.4840s
	iters: 300, epoch: 3 | loss: 0.1033369 | orig_loss: 0.1953920 | penalty: 0.1963953
	speed: 0.0187s/iter; left time: 5982.4579s
	iters: 400, epoch: 3 | loss: 0.5383196 | orig_loss: 0.2135689 | penalty: 0.2154733
	speed: 0.0186s/iter; left time: 5970.6486s
	iters: 500, epoch: 3 | loss: 0.2064466 | orig_loss: 0.2187874 | penalty: 0.2208760
	speed: 0.0186s/iter; left time: 5971.5570s
	iters: 600, epoch: 3 | loss: 0.1700967 | orig_loss: 0.2070257 | penalty: 0.2085828
	speed: 0.0186s/iter; left time: 5971.8274s
	iters: 700, epoch: 3 | loss: 0.8693814 | orig_loss: 0.2075993 | penalty: 0.2102463
	speed: 0.0187s/iter; left time: 5976.8297s
	iters: 800, epoch: 3 | loss: 0.2219060 | orig_loss: 0.1933960 | penalty: 0.1946957
	speed: 0.0191s/iter; left time: 6101.7305s
	iters: 900, epoch: 3 | loss: 0.1226545 | orig_loss: 0.1894668 | penalty: 0.1904016
	speed: 0.0192s/iter; left time: 6132.3588s
	iters: 1000, epoch: 3 | loss: 0.7011705 | orig_loss: 0.2173796 | penalty: 0.2198311
	speed: 0.0192s/iter; left time: 6139.0740s

------ Mask Loss Comparison ------
Original Loss: 0.4481222
Best Mask Loss: 0.4003647 (mask length: 12)
Improvement: 0.0477575 (10.66%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4720010
---------------------------------

Epoch: 3 cost time: 20.4252188205719
Epoch Losses - Original: 0.2053170, With Penalty: 0.2070290, Difference: 0.0017120 (0.83%)
Epoch: 3, Steps: 1077 | Train Loss: 0.3226504 Vali Loss: 0.1273471 Test Loss: 0.1749057
Validation loss decreased (0.130230 --> 0.127347).  Saving model ...
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.1684224 | orig_loss: 0.1936110 | penalty: 0.1946664
	speed: 0.0572s/iter; left time: 18299.7069s
	iters: 200, epoch: 4 | loss: 0.1136133 | orig_loss: 0.2226969 | penalty: 0.2256443
	speed: 0.0187s/iter; left time: 5965.7660s
	iters: 300, epoch: 4 | loss: 0.1653087 | orig_loss: 0.1887097 | penalty: 0.1898468
	speed: 0.0186s/iter; left time: 5955.8525s
	iters: 400, epoch: 4 | loss: 0.1260204 | orig_loss: 0.1984334 | penalty: 0.1999304
	speed: 0.0186s/iter; left time: 5950.2748s
	iters: 500, epoch: 4 | loss: 0.1704651 | orig_loss: 0.1925694 | penalty: 0.1944943
	speed: 0.0186s/iter; left time: 5952.4913s
	iters: 600, epoch: 4 | loss: 0.1699487 | orig_loss: 0.1875259 | penalty: 0.1878373
	speed: 0.0187s/iter; left time: 5959.4094s
	iters: 700, epoch: 4 | loss: 0.1967925 | orig_loss: 0.1904874 | penalty: 0.1911053
	speed: 0.0188s/iter; left time: 5984.8431s
	iters: 800, epoch: 4 | loss: 0.2337839 | orig_loss: 0.1989076 | penalty: 0.1998795
	speed: 0.0189s/iter; left time: 6029.2237s
	iters: 900, epoch: 4 | loss: 0.2041166 | orig_loss: 0.1991607 | penalty: 0.1999162
	speed: 0.0188s/iter; left time: 6004.8960s
	iters: 1000, epoch: 4 | loss: 0.1384573 | orig_loss: 0.2126201 | penalty: 0.2143816
	speed: 0.0187s/iter; left time: 5970.2103s
Epoch: 4 cost time: 20.303704500198364
Epoch Losses - Original: 0.1976349, With Penalty: 0.1989511, Difference: 0.0013162 (0.67%)
Epoch: 4, Steps: 1077 | Train Loss: 0.1686929 Vali Loss: 0.1269642 Test Loss: 0.1748490
Validation loss decreased (0.127347 --> 0.126964).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.3215834 | orig_loss: 0.1894288 | penalty: 0.1901190
	speed: 0.0569s/iter; left time: 18141.0075s
	iters: 200, epoch: 5 | loss: 0.1228582 | orig_loss: 0.2035088 | penalty: 0.2060321
	speed: 0.0189s/iter; left time: 6011.1278s
	iters: 300, epoch: 5 | loss: 0.1586337 | orig_loss: 0.1848168 | penalty: 0.1856576
	speed: 0.0187s/iter; left time: 5961.1213s
	iters: 400, epoch: 5 | loss: 0.1168010 | orig_loss: 0.1844419 | penalty: 0.1850000
	speed: 0.0188s/iter; left time: 5971.8994s

------ Mask Loss Comparison ------
Original Loss: 0.3631350
Best Mask Loss: 0.2999959 (mask length: 9)
Improvement: 0.0631391 (17.39%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3947046
---------------------------------

	iters: 500, epoch: 5 | loss: 0.1101978 | orig_loss: 0.2003001 | penalty: 0.2012794
	speed: 0.0187s/iter; left time: 5966.9169s
	iters: 600, epoch: 5 | loss: 0.1407356 | orig_loss: 0.1814313 | penalty: 0.1818556
	speed: 0.0187s/iter; left time: 5954.9297s
	iters: 700, epoch: 5 | loss: 0.2550761 | orig_loss: 0.1829514 | penalty: 0.1834499
	speed: 0.0187s/iter; left time: 5949.7600s
	iters: 800, epoch: 5 | loss: 0.1614855 | orig_loss: 0.2025191 | penalty: 0.2037881
	speed: 0.0187s/iter; left time: 5944.5332s
	iters: 900, epoch: 5 | loss: 0.1613497 | orig_loss: 0.1857347 | penalty: 0.1866509
	speed: 0.0187s/iter; left time: 5943.6157s
	iters: 1000, epoch: 5 | loss: 0.1580001 | orig_loss: 0.2085485 | penalty: 0.2102597
	speed: 0.0187s/iter; left time: 5941.9324s
Epoch: 5 cost time: 20.336792707443237
Epoch Losses - Original: 0.1912148, With Penalty: 0.1922141, Difference: 0.0009993 (0.52%)
Epoch: 5, Steps: 1077 | Train Loss: 0.1706721 Vali Loss: 0.1270244 Test Loss: 0.1745109
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.3332737 | orig_loss: 0.1812964 | penalty: 0.1819446
	speed: 0.0566s/iter; left time: 17976.3427s
	iters: 200, epoch: 6 | loss: 0.1507017 | orig_loss: 0.1909801 | penalty: 0.1915306
	speed: 0.0186s/iter; left time: 5903.8109s
	iters: 300, epoch: 6 | loss: 0.1910081 | orig_loss: 0.1759049 | penalty: 0.1767056
	speed: 0.0186s/iter; left time: 5906.5671s
	iters: 400, epoch: 6 | loss: 0.1402622 | orig_loss: 0.1815798 | penalty: 0.1819727
	speed: 0.0186s/iter; left time: 5912.8413s
	iters: 500, epoch: 6 | loss: 0.1280376 | orig_loss: 0.1889651 | penalty: 0.1902561
	speed: 0.0186s/iter; left time: 5913.1160s
	iters: 600, epoch: 6 | loss: 0.1595375 | orig_loss: 0.1817616 | penalty: 0.1829920
	speed: 0.0186s/iter; left time: 5908.7675s
	iters: 700, epoch: 6 | loss: 0.1395116 | orig_loss: 0.1893483 | penalty: 0.1902042
	speed: 0.0186s/iter; left time: 5910.6871s
	iters: 800, epoch: 6 | loss: 0.1269301 | orig_loss: 0.1726109 | penalty: 0.1733060
	speed: 0.0189s/iter; left time: 5979.1904s
	iters: 900, epoch: 6 | loss: 0.1941937 | orig_loss: 0.1862400 | penalty: 0.1866038
	speed: 0.0191s/iter; left time: 6051.2901s
	iters: 1000, epoch: 6 | loss: 0.1236533 | orig_loss: 0.2189584 | penalty: 0.2202033
	speed: 0.0191s/iter; left time: 6056.0281s
Epoch: 6 cost time: 20.37425661087036
Epoch Losses - Original: 0.1865443, With Penalty: 0.1873514, Difference: 0.0008071 (0.43%)
Epoch: 6, Steps: 1077 | Train Loss: 0.1687110 Vali Loss: 0.1261716 Test Loss: 0.1737846
Validation loss decreased (0.126964 --> 0.126172).  Saving model ...
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.2119141 | orig_loss: 0.1852468 | penalty: 0.1856293
	speed: 0.0566s/iter; left time: 17904.4991s
	iters: 200, epoch: 7 | loss: 0.1537054 | orig_loss: 0.1851140 | penalty: 0.1858238
	speed: 0.0186s/iter; left time: 5890.3491s
	iters: 300, epoch: 7 | loss: 0.1463000 | orig_loss: 0.1880493 | penalty: 0.1885530
	speed: 0.0187s/iter; left time: 5913.2092s

------ Mask Loss Comparison ------
Original Loss: 0.4132180
Best Mask Loss: 0.3977956 (mask length: 15)
Improvement: 0.0154225 (3.73%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4209293
---------------------------------

	iters: 400, epoch: 7 | loss: 0.1531837 | orig_loss: 0.1796039 | penalty: 0.1799589
	speed: 0.0186s/iter; left time: 5868.4947s
	iters: 500, epoch: 7 | loss: 0.0910086 | orig_loss: 0.1911611 | penalty: 0.1914660
	speed: 0.0186s/iter; left time: 5875.9498s
	iters: 600, epoch: 7 | loss: 0.0931975 | orig_loss: 0.1832246 | penalty: 0.1841833
	speed: 0.0186s/iter; left time: 5890.6711s
	iters: 700, epoch: 7 | loss: 0.1216364 | orig_loss: 0.1736965 | penalty: 0.1737638
	speed: 0.0187s/iter; left time: 5895.1948s
	iters: 800, epoch: 7 | loss: 0.1104170 | orig_loss: 0.1745836 | penalty: 0.1752566
	speed: 0.0187s/iter; left time: 5912.0098s
	iters: 900, epoch: 7 | loss: 0.0924197 | orig_loss: 0.1679562 | penalty: 0.1681065
	speed: 0.0187s/iter; left time: 5891.9462s
	iters: 1000, epoch: 7 | loss: 0.1434446 | orig_loss: 0.1797617 | penalty: 0.1800849
	speed: 0.0186s/iter; left time: 5881.1046s
Epoch: 7 cost time: 20.229527950286865
Epoch Losses - Original: 0.1821119, With Penalty: 0.1825441, Difference: 0.0004321 (0.24%)
Epoch: 7, Steps: 1077 | Train Loss: 0.1317227 Vali Loss: 0.1289449 Test Loss: 0.1772067
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.2963293 | orig_loss: 0.1777230 | penalty: 0.1777230
	speed: 0.0564s/iter; left time: 17796.5578s
	iters: 200, epoch: 8 | loss: 0.2112378 | orig_loss: 0.1827020 | penalty: 0.1836234
	speed: 0.0186s/iter; left time: 5863.8825s
	iters: 300, epoch: 8 | loss: 0.2043465 | orig_loss: 0.1766111 | penalty: 0.1767967
	speed: 0.0186s/iter; left time: 5867.2034s
	iters: 400, epoch: 8 | loss: 0.1953210 | orig_loss: 0.1906689 | penalty: 0.1908005
	speed: 0.0186s/iter; left time: 5874.6658s
	iters: 500, epoch: 8 | loss: 0.2350265 | orig_loss: 0.1659604 | penalty: 0.1663258
	speed: 0.0187s/iter; left time: 5884.8775s
	iters: 600, epoch: 8 | loss: 0.3617343 | orig_loss: 0.1780909 | penalty: 0.1784935
	speed: 0.0187s/iter; left time: 5891.9495s
	iters: 700, epoch: 8 | loss: 0.1237095 | orig_loss: 0.1653153 | penalty: 0.1655660
	speed: 0.0187s/iter; left time: 5881.8910s
	iters: 800, epoch: 8 | loss: 0.1948422 | orig_loss: 0.1758663 | penalty: 0.1758744
	speed: 0.0188s/iter; left time: 5932.4877s
	iters: 900, epoch: 8 | loss: 0.3031372 | orig_loss: 0.1806485 | penalty: 0.1820790
	speed: 0.0187s/iter; left time: 5896.1119s
	iters: 1000, epoch: 8 | loss: 0.2103612 | orig_loss: 0.1855260 | penalty: 0.1857416
	speed: 0.0189s/iter; left time: 5938.0096s
Epoch: 8 cost time: 20.338141202926636
Epoch Losses - Original: 0.1776874, With Penalty: 0.1780862, Difference: 0.0003988 (0.22%)
Epoch: 8, Steps: 1077 | Train Loss: 0.2336046 Vali Loss: 0.1270879 Test Loss: 0.1742745
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002994739288874256
	iters: 100, epoch: 9 | loss: 0.1298344 | orig_loss: 0.1681942 | penalty: 0.1683187
	speed: 0.0575s/iter; left time: 18068.1848s
	iters: 200, epoch: 9 | loss: 0.2822776 | orig_loss: 0.1636796 | penalty: 0.1638802
	speed: 0.0190s/iter; left time: 5975.9629s
	iters: 300, epoch: 9 | loss: 0.1207743 | orig_loss: 0.1754748 | penalty: 0.1754748
	speed: 0.0188s/iter; left time: 5899.6603s
	iters: 400, epoch: 9 | loss: 0.3400643 | orig_loss: 0.1767159 | penalty: 0.1769778
	speed: 0.0188s/iter; left time: 5893.9282s
	iters: 500, epoch: 9 | loss: 0.1749723 | orig_loss: 0.1798101 | penalty: 0.1803016
	speed: 0.0188s/iter; left time: 5906.4660s
	iters: 600, epoch: 9 | loss: 0.1354188 | orig_loss: 0.1676931 | penalty: 0.1678205
	speed: 0.0189s/iter; left time: 5943.2436s
	iters: 700, epoch: 9 | loss: 0.1545858 | orig_loss: 0.1766363 | penalty: 0.1770943
	speed: 0.0192s/iter; left time: 6012.7861s
	iters: 800, epoch: 9 | loss: 0.1940470 | orig_loss: 0.1737803 | penalty: 0.1739967
	speed: 0.0193s/iter; left time: 6040.2116s
	iters: 900, epoch: 9 | loss: 0.1135973 | orig_loss: 0.1711050 | penalty: 0.1711050
	speed: 0.0190s/iter; left time: 5951.4029s
	iters: 1000, epoch: 9 | loss: 0.3346405 | orig_loss: 0.1709774 | penalty: 0.1712466
	speed: 0.0191s/iter; left time: 5986.8089s
Epoch: 9 cost time: 20.565922498703003
Epoch Losses - Original: 0.1733631, With Penalty: 0.1736263, Difference: 0.0002632 (0.15%)
Epoch: 9, Steps: 1077 | Train Loss: 0.1980212 Vali Loss: 0.1287581 Test Loss: 0.1774849
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm2, seq_len: 48, pred_len: 72
Avg Original Loss: 0.1946186
Avg Penalty Loss: 0.1958191
Avg Difference: 0.0012005 (0.62%)
================================

>>>>>>>testing : long_term_forecast_ETTm2_48_72_SOFTS_ETTm2_ftM_sl48_ll48_pl72_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11449
mse:0.17378455112048952, mae:0.26112548536177294
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm2_48_96', model='SOFTS', data='ETTm2', root_path='./dataset/ETT/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=96, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm2_48_96_SOFTS_ETTm2_ftM_sl48_ll48_pl96_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34417
val 11425
test 11425

------ Mask Loss Comparison ------
Original Loss: 0.4118783
Best Mask Loss: 0.3927622 (mask length: 3)
Improvement: 0.0191161 (4.64%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4214364
---------------------------------

	iters: 100, epoch: 1 | loss: 0.1968388 | orig_loss: 0.2625830 | penalty: 0.2651396
	speed: 0.0339s/iter; left time: 10936.6444s
	iters: 200, epoch: 1 | loss: 0.2019551 | orig_loss: 0.2561116 | penalty: 0.2590113
	speed: 0.0195s/iter; left time: 6276.6459s
	iters: 300, epoch: 1 | loss: 0.3146723 | orig_loss: 0.2536174 | penalty: 0.2571063
	speed: 0.0189s/iter; left time: 6090.0438s
	iters: 400, epoch: 1 | loss: 0.2579209 | orig_loss: 0.2587885 | penalty: 0.2617040
	speed: 0.0188s/iter; left time: 6056.7117s

------ Mask Loss Comparison ------
Original Loss: 0.2929173
Best Mask Loss: 0.2784898 (mask length: 3)
Improvement: 0.0144276 (4.93%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3001311
---------------------------------

	iters: 500, epoch: 1 | loss: 0.1218520 | orig_loss: 0.2342166 | penalty: 0.2354887
	speed: 0.0188s/iter; left time: 6044.8497s
	iters: 600, epoch: 1 | loss: 0.2816119 | orig_loss: 0.2556666 | penalty: 0.2595749
	speed: 0.0188s/iter; left time: 6054.2872s
	iters: 700, epoch: 1 | loss: 0.2315404 | orig_loss: 0.2512460 | penalty: 0.2554600
	speed: 0.0187s/iter; left time: 6029.9918s
	iters: 800, epoch: 1 | loss: 0.1870636 | orig_loss: 0.2422939 | penalty: 0.2443088
	speed: 0.0188s/iter; left time: 6056.4896s

------ Mask Loss Comparison ------
Original Loss: 0.3758751
Best Mask Loss: 0.3314909 (mask length: 30)
Improvement: 0.0443842 (11.81%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3980672
---------------------------------

	iters: 900, epoch: 1 | loss: 0.3567694 | orig_loss: 0.2295270 | penalty: 0.2316613
	speed: 0.0188s/iter; left time: 6048.5096s
	iters: 1000, epoch: 1 | loss: 0.1713379 | orig_loss: 0.2511777 | penalty: 0.2535042
	speed: 0.0188s/iter; left time: 6047.2605s
Epoch: 1 cost time: 20.919642686843872
Epoch Losses - Original: 0.2483590, With Penalty: 0.2510579, Difference: 0.0026989 (1.09%)
Epoch: 1, Steps: 1076 | Train Loss: 0.2321562 Vali Loss: 0.1380690 Test Loss: 0.1943183
Validation loss decreased (inf --> 0.138069).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.2419231 | orig_loss: 0.2406573 | penalty: 0.2429749
	speed: 0.0572s/iter; left time: 18403.1950s
	iters: 200, epoch: 2 | loss: 0.1286002 | orig_loss: 0.2472661 | penalty: 0.2508725
	speed: 0.0186s/iter; left time: 5967.5682s

------ Mask Loss Comparison ------
Original Loss: 0.4436666
Best Mask Loss: 0.4392513 (mask length: 18)
Improvement: 0.0044153 (1.00%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.4458743
---------------------------------

	iters: 300, epoch: 2 | loss: 0.6300556 | orig_loss: 0.2363307 | penalty: 0.2374429
	speed: 0.0187s/iter; left time: 6002.2741s
	iters: 400, epoch: 2 | loss: 0.4567548 | orig_loss: 0.2524916 | penalty: 0.2534444
	speed: 0.0187s/iter; left time: 5997.1278s
	iters: 500, epoch: 2 | loss: 0.1451943 | orig_loss: 0.2275253 | penalty: 0.2286107
	speed: 0.0186s/iter; left time: 5984.2867s
	iters: 600, epoch: 2 | loss: 0.1333183 | orig_loss: 0.2386376 | penalty: 0.2418944
	speed: 0.0187s/iter; left time: 6001.9042s
	iters: 700, epoch: 2 | loss: 0.1739836 | orig_loss: 0.2396267 | penalty: 0.2411705
	speed: 0.0187s/iter; left time: 5991.5433s

------ Mask Loss Comparison ------
Original Loss: 0.2334165
Best Mask Loss: 0.2331336 (mask length: 3)
Improvement: 0.0002829 (0.12%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2335579
---------------------------------

	iters: 800, epoch: 2 | loss: 0.5099305 | orig_loss: 0.2324841 | penalty: 0.2353817
	speed: 0.0187s/iter; left time: 5989.1290s
	iters: 900, epoch: 2 | loss: 0.1446766 | orig_loss: 0.2331014 | penalty: 0.2352212
	speed: 0.0187s/iter; left time: 5986.0543s
	iters: 1000, epoch: 2 | loss: 0.2023771 | orig_loss: 0.2506492 | penalty: 0.2528043
	speed: 0.0187s/iter; left time: 5986.9994s
Epoch: 2 cost time: 20.245630979537964
Epoch Losses - Original: 0.2392457, With Penalty: 0.2413413, Difference: 0.0020956 (0.88%)
Epoch: 2, Steps: 1076 | Train Loss: 0.2766814 Vali Loss: 0.1358008 Test Loss: 0.1922328
Validation loss decreased (0.138069 --> 0.135801).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.1338765 | orig_loss: 0.2394717 | penalty: 0.2412132
	speed: 0.0565s/iter; left time: 18107.8166s
	iters: 200, epoch: 3 | loss: 0.2714409 | orig_loss: 0.2194004 | penalty: 0.2219867
	speed: 0.0186s/iter; left time: 5967.7108s

------ Mask Loss Comparison ------
Original Loss: 0.3534970
Best Mask Loss: 0.3452080 (mask length: 30)
Improvement: 0.0082889 (2.34%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3576414
---------------------------------

	iters: 300, epoch: 3 | loss: 0.1510963 | orig_loss: 0.2335542 | penalty: 0.2354218
	speed: 0.0185s/iter; left time: 5921.0288s
	iters: 400, epoch: 3 | loss: 0.2230940 | orig_loss: 0.2339753 | penalty: 0.2352682
	speed: 0.0184s/iter; left time: 5890.3182s
	iters: 500, epoch: 3 | loss: 0.1787420 | orig_loss: 0.2368225 | penalty: 0.2394850
	speed: 0.0186s/iter; left time: 5958.1952s
	iters: 600, epoch: 3 | loss: 0.1724263 | orig_loss: 0.2314756 | penalty: 0.2330426
	speed: 0.0186s/iter; left time: 5942.2565s
	iters: 700, epoch: 3 | loss: 0.2199392 | orig_loss: 0.2387850 | penalty: 0.2403579
	speed: 0.0186s/iter; left time: 5947.0386s
	iters: 800, epoch: 3 | loss: 0.1457240 | orig_loss: 0.2234377 | penalty: 0.2248913
	speed: 0.0186s/iter; left time: 5951.9127s
	iters: 900, epoch: 3 | loss: 0.2476374 | orig_loss: 0.2204948 | penalty: 0.2214931
	speed: 0.0186s/iter; left time: 5932.6278s

------ Mask Loss Comparison ------
Original Loss: 0.2229408
Best Mask Loss: 0.2216454 (mask length: 3)
Improvement: 0.0012953 (0.58%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2235885
---------------------------------

	iters: 1000, epoch: 3 | loss: 0.1430942 | orig_loss: 0.2321713 | penalty: 0.2350904
	speed: 0.0186s/iter; left time: 5941.1949s
Epoch: 3 cost time: 20.112653017044067
Epoch Losses - Original: 0.2322679, With Penalty: 0.2341285, Difference: 0.0018606 (0.80%)
Epoch: 3, Steps: 1076 | Train Loss: 0.1887071 Vali Loss: 0.1371056 Test Loss: 0.1938315
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 0.1257526 | orig_loss: 0.2310921 | penalty: 0.2330317
	speed: 0.0565s/iter; left time: 18038.9557s
	iters: 200, epoch: 4 | loss: 0.3070085 | orig_loss: 0.2373289 | penalty: 0.2394131
	speed: 0.0186s/iter; left time: 5944.8818s
	iters: 300, epoch: 4 | loss: 0.2313477 | orig_loss: 0.2147055 | penalty: 0.2149502
	speed: 0.0186s/iter; left time: 5935.5506s
	iters: 400, epoch: 4 | loss: 0.1960429 | orig_loss: 0.2304134 | penalty: 0.2315339
	speed: 0.0186s/iter; left time: 5935.3938s
	iters: 500, epoch: 4 | loss: 0.1317090 | orig_loss: 0.2369416 | penalty: 0.2399069
	speed: 0.0185s/iter; left time: 5914.4232s
	iters: 600, epoch: 4 | loss: 0.2446486 | orig_loss: 0.2289956 | penalty: 0.2294376
	speed: 0.0186s/iter; left time: 5933.2290s
	iters: 700, epoch: 4 | loss: 0.1854796 | orig_loss: 0.2267506 | penalty: 0.2285082
	speed: 0.0186s/iter; left time: 5925.0839s
	iters: 800, epoch: 4 | loss: 0.1919000 | orig_loss: 0.2196466 | penalty: 0.2212951
	speed: 0.0185s/iter; left time: 5911.9001s

------ Mask Loss Comparison ------
Original Loss: 0.3410285
Best Mask Loss: 0.3094137 (mask length: 3)
Improvement: 0.0316149 (9.27%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3568360
---------------------------------

	iters: 900, epoch: 4 | loss: 0.2216638 | orig_loss: 0.2184557 | penalty: 0.2192349
	speed: 0.0185s/iter; left time: 5907.6340s
	iters: 1000, epoch: 4 | loss: 0.1433955 | orig_loss: 0.2129238 | penalty: 0.2140976
	speed: 0.0184s/iter; left time: 5864.3770s
Epoch: 4 cost time: 20.11383080482483
Epoch Losses - Original: 0.2254529, With Penalty: 0.2268207, Difference: 0.0013678 (0.61%)
Epoch: 4, Steps: 1076 | Train Loss: 0.1978948 Vali Loss: 0.1333499 Test Loss: 0.1890029
Validation loss decreased (0.135801 --> 0.133350).  Saving model ...
Updating learning rate to 0.00029986842451482874
	iters: 100, epoch: 5 | loss: 0.1368284 | orig_loss: 0.2090214 | penalty: 0.2096225
	speed: 0.0555s/iter; left time: 17663.8468s
	iters: 200, epoch: 5 | loss: 0.1826813 | orig_loss: 0.2209953 | penalty: 0.2215876
	speed: 0.0184s/iter; left time: 5847.1459s
	iters: 300, epoch: 5 | loss: 0.2202566 | orig_loss: 0.2382043 | penalty: 0.2396786
	speed: 0.0184s/iter; left time: 5845.0813s
	iters: 400, epoch: 5 | loss: 0.2040022 | orig_loss: 0.2152495 | penalty: 0.2157701
	speed: 0.0184s/iter; left time: 5842.2000s
	iters: 500, epoch: 5 | loss: 0.2027739 | orig_loss: 0.2213153 | penalty: 0.2214464
	speed: 0.0184s/iter; left time: 5843.7865s
	iters: 600, epoch: 5 | loss: 0.2019904 | orig_loss: 0.2080953 | penalty: 0.2083810
	speed: 0.0184s/iter; left time: 5840.1458s
	iters: 700, epoch: 5 | loss: 0.1621133 | orig_loss: 0.2190129 | penalty: 0.2205203
	speed: 0.0184s/iter; left time: 5840.0225s
	iters: 800, epoch: 5 | loss: 0.1543305 | orig_loss: 0.2020796 | penalty: 0.2031873
	speed: 0.0184s/iter; left time: 5844.3247s
	iters: 900, epoch: 5 | loss: 0.2047982 | orig_loss: 0.2280455 | penalty: 0.2285013
	speed: 0.0184s/iter; left time: 5837.4315s
	iters: 1000, epoch: 5 | loss: 0.1603090 | orig_loss: 0.2165627 | penalty: 0.2170234
	speed: 0.0184s/iter; left time: 5836.6450s
Epoch: 5 cost time: 19.91300368309021
Epoch Losses - Original: 0.2182902, With Penalty: 0.2191219, Difference: 0.0008317 (0.38%)
Epoch: 5, Steps: 1076 | Train Loss: 0.1830084 Vali Loss: 0.1341319 Test Loss: 0.1891054
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.1627796 | orig_loss: 0.2016159 | penalty: 0.2022434
	speed: 0.0557s/iter; left time: 17678.8989s
	iters: 200, epoch: 6 | loss: 0.1747756 | orig_loss: 0.2271601 | penalty: 0.2283637
	speed: 0.0184s/iter; left time: 5824.7447s
	iters: 300, epoch: 6 | loss: 0.2702038 | orig_loss: 0.2175338 | penalty: 0.2183649
	speed: 0.0183s/iter; left time: 5817.4186s
	iters: 400, epoch: 6 | loss: 0.2841284 | orig_loss: 0.2150768 | penalty: 0.2166334
	speed: 0.0183s/iter; left time: 5808.8678s
	iters: 500, epoch: 6 | loss: 0.2263372 | orig_loss: 0.2096717 | penalty: 0.2099525
	speed: 0.0183s/iter; left time: 5811.7202s
	iters: 600, epoch: 6 | loss: 0.1822148 | orig_loss: 0.2041343 | penalty: 0.2052211
	speed: 0.0183s/iter; left time: 5808.2684s

------ Mask Loss Comparison ------
Original Loss: 0.3760560
Best Mask Loss: 0.3701051 (mask length: 27)
Improvement: 0.0059509 (1.58%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3790314
---------------------------------

	iters: 700, epoch: 6 | loss: 0.0985014 | orig_loss: 0.2163923 | penalty: 0.2172697
	speed: 0.0183s/iter; left time: 5802.4126s
	iters: 800, epoch: 6 | loss: 0.2204761 | orig_loss: 0.2154111 | penalty: 0.2170045
	speed: 0.0183s/iter; left time: 5802.0194s
	iters: 900, epoch: 6 | loss: 0.1556693 | orig_loss: 0.1944409 | penalty: 0.1952403
	speed: 0.0183s/iter; left time: 5804.4537s
	iters: 1000, epoch: 6 | loss: 0.1561585 | orig_loss: 0.2036944 | penalty: 0.2042512
	speed: 0.0183s/iter; left time: 5801.5101s
Epoch: 6 cost time: 19.900386571884155
Epoch Losses - Original: 0.2120163, With Penalty: 0.2128979, Difference: 0.0008815 (0.42%)
Epoch: 6, Steps: 1076 | Train Loss: 0.1931245 Vali Loss: 0.1369596 Test Loss: 0.1918423
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.1323435 | orig_loss: 0.2245126 | penalty: 0.2248913
	speed: 0.0563s/iter; left time: 17800.6769s
	iters: 200, epoch: 7 | loss: 0.1298502 | orig_loss: 0.2099740 | penalty: 0.2102581
	speed: 0.0185s/iter; left time: 5834.4032s
	iters: 300, epoch: 7 | loss: 0.1370261 | orig_loss: 0.1962901 | penalty: 0.1967348
	speed: 0.0184s/iter; left time: 5825.3941s
	iters: 400, epoch: 7 | loss: 0.1352246 | orig_loss: 0.2018597 | penalty: 0.2031266
	speed: 0.0185s/iter; left time: 5836.6111s
	iters: 500, epoch: 7 | loss: 0.4286220 | orig_loss: 0.2245675 | penalty: 0.2267086
	speed: 0.0184s/iter; left time: 5810.3158s
	iters: 600, epoch: 7 | loss: 0.5232335 | orig_loss: 0.2044170 | penalty: 0.2045668
	speed: 0.0184s/iter; left time: 5807.4350s
	iters: 700, epoch: 7 | loss: 0.1377071 | orig_loss: 0.1876908 | penalty: 0.1878965
	speed: 0.0184s/iter; left time: 5802.6518s
	iters: 800, epoch: 7 | loss: 0.1232129 | orig_loss: 0.1869931 | penalty: 0.1875748
	speed: 0.0184s/iter; left time: 5810.4077s
	iters: 900, epoch: 7 | loss: 0.1920045 | orig_loss: 0.2059334 | penalty: 0.2062485
	speed: 0.0184s/iter; left time: 5812.6697s
	iters: 1000, epoch: 7 | loss: 0.3240518 | orig_loss: 0.2064927 | penalty: 0.2069072
	speed: 0.0184s/iter; left time: 5806.4197s
Epoch: 7 cost time: 19.96890902519226
Epoch Losses - Original: 0.2059985, With Penalty: 0.2066134, Difference: 0.0006149 (0.30%)
Epoch: 7, Steps: 1076 | Train Loss: 0.2263276 Vali Loss: 0.1404629 Test Loss: 0.1984341
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm2, seq_len: 48, pred_len: 96
Avg Original Loss: 0.2259472
Avg Penalty Loss: 0.2274259
Avg Difference: 0.0014787 (0.65%)
================================

>>>>>>>testing : long_term_forecast_ETTm2_48_96_SOFTS_ETTm2_ftM_sl48_ll48_pl96_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.1890028622798638, mae:0.2704408834851805
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm2_48_144', model='SOFTS', data='ETTm2', root_path='./dataset/ETT/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=144, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm2_48_144_SOFTS_ETTm2_ftM_sl48_ll48_pl144_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34369
val 11377
test 11377

------ Mask Loss Comparison ------
Original Loss: 0.3268211
Best Mask Loss: 0.3201748 (mask length: 3)
Improvement: 0.0066464 (2.03%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3301443
---------------------------------

	iters: 100, epoch: 1 | loss: 0.2490185 | orig_loss: 0.2948633 | penalty: 0.3003514
	speed: 0.0331s/iter; left time: 10657.8422s
	iters: 200, epoch: 1 | loss: 0.2061682 | orig_loss: 0.3111492 | penalty: 0.3151865
	speed: 0.0187s/iter; left time: 6021.6009s

------ Mask Loss Comparison ------
Original Loss: 0.2322392
Best Mask Loss: 0.2218874 (mask length: 3)
Improvement: 0.0103518 (4.46%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2374151
---------------------------------

	iters: 300, epoch: 1 | loss: 0.1685003 | orig_loss: 0.2968286 | penalty: 0.3020956
	speed: 0.0187s/iter; left time: 6011.1873s
	iters: 400, epoch: 1 | loss: 0.1792025 | orig_loss: 0.3033148 | penalty: 0.3099352
	speed: 0.0185s/iter; left time: 5968.5198s
	iters: 500, epoch: 1 | loss: 0.2017428 | orig_loss: 0.3093254 | penalty: 0.3156944
	speed: 0.0186s/iter; left time: 5999.8280s

------ Mask Loss Comparison ------
Original Loss: 0.2383839
Best Mask Loss: 0.2207057 (mask length: 3)
Improvement: 0.0176782 (7.42%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2472230
---------------------------------

	iters: 600, epoch: 1 | loss: 0.3387498 | orig_loss: 0.2935314 | penalty: 0.2968251
	speed: 0.0186s/iter; left time: 6001.1781s
	iters: 700, epoch: 1 | loss: 0.3807806 | orig_loss: 0.3230621 | penalty: 0.3281921
	speed: 0.0186s/iter; left time: 5983.6216s
	iters: 800, epoch: 1 | loss: 0.2027251 | orig_loss: 0.3131897 | penalty: 0.3194571
	speed: 0.0187s/iter; left time: 6023.0534s
	iters: 900, epoch: 1 | loss: 0.2407615 | orig_loss: 0.3048338 | penalty: 0.3096173
	speed: 0.0186s/iter; left time: 5968.4658s

------ Mask Loss Comparison ------
Original Loss: 0.3249972
Best Mask Loss: 0.3207795 (mask length: 3)
Improvement: 0.0042176 (1.30%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3271060
---------------------------------

	iters: 1000, epoch: 1 | loss: 0.7017561 | orig_loss: 0.2836657 | penalty: 0.2885005
	speed: 0.0188s/iter; left time: 6041.3309s
Epoch: 1 cost time: 20.63292407989502
Epoch Losses - Original: 0.3034295, With Penalty: 0.3085643, Difference: 0.0051348 (1.69%)
Epoch: 1, Steps: 1075 | Train Loss: 0.2869405 Vali Loss: 0.1648414 Test Loss: 0.2356311
Validation loss decreased (inf --> 0.164841).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.1874684 | orig_loss: 0.2780333 | penalty: 0.2813966
	speed: 0.0571s/iter; left time: 18331.8051s

------ Mask Loss Comparison ------
Original Loss: 0.2828509
Best Mask Loss: 0.2627027 (mask length: 3)
Improvement: 0.0201482 (7.12%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2929250
---------------------------------

	iters: 200, epoch: 2 | loss: 0.3294734 | orig_loss: 0.3097311 | penalty: 0.3161898
	speed: 0.0185s/iter; left time: 5948.5947s
	iters: 300, epoch: 2 | loss: 0.2767315 | orig_loss: 0.2863218 | penalty: 0.2905217
	speed: 0.0186s/iter; left time: 5988.5643s
	iters: 400, epoch: 2 | loss: 0.2755424 | orig_loss: 0.3023811 | penalty: 0.3068481
	speed: 0.0186s/iter; left time: 5981.8155s

------ Mask Loss Comparison ------
Original Loss: 0.2579245
Best Mask Loss: 0.2531867 (mask length: 3)
Improvement: 0.0047378 (1.84%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2602934
---------------------------------

	iters: 500, epoch: 2 | loss: 0.1748456 | orig_loss: 0.2993547 | penalty: 0.3041280
	speed: 0.0188s/iter; left time: 6039.5043s
	iters: 600, epoch: 2 | loss: 0.5083759 | orig_loss: 0.2934815 | penalty: 0.2977171
	speed: 0.0189s/iter; left time: 6077.6149s
	iters: 700, epoch: 2 | loss: 0.2166557 | orig_loss: 0.3113271 | penalty: 0.3158543
	speed: 0.0189s/iter; left time: 6058.5904s
	iters: 800, epoch: 2 | loss: 0.2307391 | orig_loss: 0.2819636 | penalty: 0.2856815
	speed: 0.0189s/iter; left time: 6066.3669s

------ Mask Loss Comparison ------
Original Loss: 0.2550125
Best Mask Loss: 0.2453912 (mask length: 3)
Improvement: 0.0096213 (3.77%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2598231
---------------------------------

	iters: 900, epoch: 2 | loss: 0.1477135 | orig_loss: 0.2901735 | penalty: 0.2949876
	speed: 0.0189s/iter; left time: 6061.1972s
	iters: 1000, epoch: 2 | loss: 0.2691684 | orig_loss: 0.2881771 | penalty: 0.2904948
	speed: 0.0189s/iter; left time: 6044.5359s
Epoch: 2 cost time: 20.35632872581482
Epoch Losses - Original: 0.2942332, With Penalty: 0.2985380, Difference: 0.0043048 (1.46%)
Epoch: 2, Steps: 1075 | Train Loss: 0.2616714 Vali Loss: 0.1633888 Test Loss: 0.2328504
Validation loss decreased (0.164841 --> 0.163389).  Saving model ...
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.1833093 | orig_loss: 0.2763569 | penalty: 0.2790828
	speed: 0.0581s/iter; left time: 18618.7919s
	iters: 200, epoch: 3 | loss: 0.3732968 | orig_loss: 0.2700403 | penalty: 0.2731098
	speed: 0.0187s/iter; left time: 5976.8653s

------ Mask Loss Comparison ------
Original Loss: 0.6549848
Best Mask Loss: 0.5975798 (mask length: 3)
Improvement: 0.0574050 (8.76%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.6836873
---------------------------------

	iters: 300, epoch: 3 | loss: 0.3404935 | orig_loss: 0.2931294 | penalty: 0.2963855
	speed: 0.0184s/iter; left time: 5904.1665s
	iters: 400, epoch: 3 | loss: 0.2166271 | orig_loss: 0.2915626 | penalty: 0.2939902
	speed: 0.0185s/iter; left time: 5934.6716s
	iters: 500, epoch: 3 | loss: 0.2852782 | orig_loss: 0.2913018 | penalty: 0.2944067
	speed: 0.0188s/iter; left time: 6025.7666s
	iters: 600, epoch: 3 | loss: 0.1616796 | orig_loss: 0.2757339 | penalty: 0.2782625
	speed: 0.0191s/iter; left time: 6094.6222s
	iters: 700, epoch: 3 | loss: 0.5281196 | orig_loss: 0.3132562 | penalty: 0.3168672
	speed: 0.0188s/iter; left time: 6003.4993s

------ Mask Loss Comparison ------
Original Loss: 0.2831079
Best Mask Loss: 0.2616359 (mask length: 3)
Improvement: 0.0214721 (7.58%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.2938440
---------------------------------

	iters: 800, epoch: 3 | loss: 0.1761173 | orig_loss: 0.2682573 | penalty: 0.2719220
	speed: 0.0188s/iter; left time: 6000.4707s
	iters: 900, epoch: 3 | loss: 0.1620836 | orig_loss: 0.3079508 | penalty: 0.3127586
	speed: 0.0188s/iter; left time: 6014.6877s
	iters: 1000, epoch: 3 | loss: 0.3328227 | orig_loss: 0.2797290 | penalty: 0.2827147
	speed: 0.0188s/iter; left time: 6009.4000s
Epoch: 3 cost time: 20.351897478103638
Epoch Losses - Original: 0.2879458, With Penalty: 0.2911267, Difference: 0.0031809 (1.10%)
Epoch: 3, Steps: 1075 | Train Loss: 0.2759828 Vali Loss: 0.1638069 Test Loss: 0.2354133
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999259840548597
	iters: 100, epoch: 4 | loss: 1.0611058 | orig_loss: 0.2846534 | penalty: 0.2875167
	speed: 0.0580s/iter; left time: 18526.0934s

------ Mask Loss Comparison ------
Original Loss: 0.4894756
Best Mask Loss: 0.4653569 (mask length: 30)
Improvement: 0.0241187 (4.93%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5015349
---------------------------------

	iters: 200, epoch: 4 | loss: 0.1587791 | orig_loss: 0.3029076 | penalty: 0.3049989
	speed: 0.0184s/iter; left time: 5856.8804s
	iters: 300, epoch: 4 | loss: 0.1603142 | orig_loss: 0.2851573 | penalty: 0.2875713
	speed: 0.0184s/iter; left time: 5883.6585s
	iters: 400, epoch: 4 | loss: 0.2208306 | orig_loss: 0.2650205 | penalty: 0.2665095
	speed: 0.0184s/iter; left time: 5866.7540s
	iters: 500, epoch: 4 | loss: 0.2214850 | orig_loss: 0.2910610 | penalty: 0.2924799
	speed: 0.0183s/iter; left time: 5834.0041s
	iters: 600, epoch: 4 | loss: 0.4308819 | orig_loss: 0.2738493 | penalty: 0.2759915
	speed: 0.0184s/iter; left time: 5861.4424s
	iters: 700, epoch: 4 | loss: 0.1953540 | orig_loss: 0.2994430 | penalty: 0.3017211
	speed: 0.0188s/iter; left time: 5975.7761s
	iters: 800, epoch: 4 | loss: 0.2259620 | orig_loss: 0.2656060 | penalty: 0.2665912
	speed: 0.0183s/iter; left time: 5840.5898s
	iters: 900, epoch: 4 | loss: 0.3558917 | orig_loss: 0.2680571 | penalty: 0.2686112
	speed: 0.0185s/iter; left time: 5904.5230s
	iters: 1000, epoch: 4 | loss: 0.2964437 | orig_loss: 0.2731568 | penalty: 0.2755992
	speed: 0.0184s/iter; left time: 5865.7453s
Epoch: 4 cost time: 19.999509811401367
Epoch Losses - Original: 0.2802427, With Penalty: 0.2820385, Difference: 0.0017958 (0.64%)
Epoch: 4, Steps: 1075 | Train Loss: 0.3327048 Vali Loss: 0.1637612 Test Loss: 0.2334906
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029986842451482874

------ Mask Loss Comparison ------
Original Loss: 0.5133976
Best Mask Loss: 0.4128056 (mask length: 12)
Improvement: 0.1005920 (19.59%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5636936
---------------------------------

	iters: 100, epoch: 5 | loss: 0.2614955 | orig_loss: 0.2604020 | penalty: 0.2623763
	speed: 0.0557s/iter; left time: 17713.6985s
	iters: 200, epoch: 5 | loss: 0.7725066 | orig_loss: 0.2746058 | penalty: 0.2756053
	speed: 0.0184s/iter; left time: 5863.9979s
	iters: 300, epoch: 5 | loss: 0.2776645 | orig_loss: 0.3006214 | penalty: 0.3037772
	speed: 0.0187s/iter; left time: 5945.9891s
	iters: 400, epoch: 5 | loss: 0.3557811 | orig_loss: 0.2577842 | penalty: 0.2584737
	speed: 0.0185s/iter; left time: 5885.6366s
	iters: 500, epoch: 5 | loss: 0.4003622 | orig_loss: 0.2780039 | penalty: 0.2797412
	speed: 0.0182s/iter; left time: 5797.9184s
	iters: 600, epoch: 5 | loss: 0.2887776 | orig_loss: 0.2864138 | penalty: 0.2885247
	speed: 0.0183s/iter; left time: 5796.4849s
	iters: 700, epoch: 5 | loss: 0.3325917 | orig_loss: 0.2625703 | penalty: 0.2642930
	speed: 0.0182s/iter; left time: 5793.8052s
	iters: 800, epoch: 5 | loss: 0.4033799 | orig_loss: 0.2906823 | penalty: 0.2914728
	speed: 0.0182s/iter; left time: 5781.7114s
	iters: 900, epoch: 5 | loss: 0.1229830 | orig_loss: 0.2682731 | penalty: 0.2708630
	speed: 0.0182s/iter; left time: 5778.6250s
	iters: 1000, epoch: 5 | loss: 0.3010847 | orig_loss: 0.2504624 | penalty: 0.2511595
	speed: 0.0187s/iter; left time: 5927.5813s
Epoch: 5 cost time: 19.91832995414734
Epoch Losses - Original: 0.2725715, With Penalty: 0.2741341, Difference: 0.0015626 (0.57%)
Epoch: 5, Steps: 1075 | Train Loss: 0.3516627 Vali Loss: 0.1624571 Test Loss: 0.2344466
Validation loss decreased (0.163389 --> 0.162457).  Saving model ...
Updating learning rate to 0.000299794430213186
	iters: 100, epoch: 6 | loss: 0.1968145 | orig_loss: 0.2829679 | penalty: 0.2845873
	speed: 0.0561s/iter; left time: 17783.9005s
	iters: 200, epoch: 6 | loss: 0.2141945 | orig_loss: 0.2684381 | penalty: 0.2695009
	speed: 0.0185s/iter; left time: 5867.7377s
	iters: 300, epoch: 6 | loss: 0.1743224 | orig_loss: 0.2853114 | penalty: 0.2872659
	speed: 0.0182s/iter; left time: 5765.9052s

------ Mask Loss Comparison ------
Original Loss: 0.3388728
Best Mask Loss: 0.3184112 (mask length: 3)
Improvement: 0.0204616 (6.04%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.3491037
---------------------------------

	iters: 400, epoch: 6 | loss: 0.1371400 | orig_loss: 0.2595089 | penalty: 0.2612043
	speed: 0.0182s/iter; left time: 5767.5574s
	iters: 500, epoch: 6 | loss: 0.3196380 | orig_loss: 0.2578148 | penalty: 0.2588770
	speed: 0.0182s/iter; left time: 5771.0931s
	iters: 600, epoch: 6 | loss: 0.2834795 | orig_loss: 0.2613270 | penalty: 0.2626378
	speed: 0.0184s/iter; left time: 5822.2956s
	iters: 700, epoch: 6 | loss: 0.2383145 | orig_loss: 0.2568288 | penalty: 0.2597074
	speed: 0.0187s/iter; left time: 5916.0599s
	iters: 800, epoch: 6 | loss: 0.6156492 | orig_loss: 0.2631230 | penalty: 0.2634371
	speed: 0.0187s/iter; left time: 5913.1004s
	iters: 900, epoch: 6 | loss: 0.3547724 | orig_loss: 0.2667198 | penalty: 0.2675103
	speed: 0.0186s/iter; left time: 5867.3103s
	iters: 1000, epoch: 6 | loss: 0.2293380 | orig_loss: 0.2468681 | penalty: 0.2483991
	speed: 0.0183s/iter; left time: 5778.9614s
Epoch: 6 cost time: 19.937381982803345
Epoch Losses - Original: 0.2658010, With Penalty: 0.2672298, Difference: 0.0014288 (0.54%)
Epoch: 6, Steps: 1075 | Train Loss: 0.2763663 Vali Loss: 0.1635902 Test Loss: 0.2323712
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002997040092642407
	iters: 100, epoch: 7 | loss: 0.2498856 | orig_loss: 0.2639933 | penalty: 0.2639933
	speed: 0.0557s/iter; left time: 17597.9824s
	iters: 200, epoch: 7 | loss: 0.1821067 | orig_loss: 0.2722114 | penalty: 0.2750396
	speed: 0.0184s/iter; left time: 5797.1891s
	iters: 300, epoch: 7 | loss: 0.2782165 | orig_loss: 0.2443649 | penalty: 0.2451823
	speed: 0.0184s/iter; left time: 5803.1460s
	iters: 400, epoch: 7 | loss: 0.5434223 | orig_loss: 0.2508490 | penalty: 0.2520785
	speed: 0.0182s/iter; left time: 5758.8646s
	iters: 500, epoch: 7 | loss: 0.2267953 | orig_loss: 0.2623445 | penalty: 0.2633996
	speed: 0.0183s/iter; left time: 5760.7973s
	iters: 600, epoch: 7 | loss: 0.2252012 | orig_loss: 0.2727797 | penalty: 0.2731622
	speed: 0.0183s/iter; left time: 5783.5080s
	iters: 700, epoch: 7 | loss: 0.2887554 | orig_loss: 0.2532274 | penalty: 0.2538931
	speed: 0.0184s/iter; left time: 5797.6004s
	iters: 800, epoch: 7 | loss: 0.2028082 | orig_loss: 0.2409949 | penalty: 0.2412084
	speed: 0.0184s/iter; left time: 5793.8228s
	iters: 900, epoch: 7 | loss: 0.3857681 | orig_loss: 0.2513605 | penalty: 0.2525367
	speed: 0.0184s/iter; left time: 5792.3456s
	iters: 1000, epoch: 7 | loss: 0.2145015 | orig_loss: 0.2788372 | penalty: 0.2798883
	speed: 0.0183s/iter; left time: 5774.7479s
Epoch: 7 cost time: 19.86334729194641
Epoch Losses - Original: 0.2593972, With Penalty: 0.2604804, Difference: 0.0010832 (0.42%)
Epoch: 7, Steps: 1075 | Train Loss: 0.2797461 Vali Loss: 0.1659649 Test Loss: 0.2366913
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002995971715836687
	iters: 100, epoch: 8 | loss: 0.1664890 | orig_loss: 0.2466790 | penalty: 0.2476723
	speed: 0.0558s/iter; left time: 17581.2250s
	iters: 200, epoch: 8 | loss: 0.2823845 | orig_loss: 0.2694787 | penalty: 0.2703072
	speed: 0.0183s/iter; left time: 5772.6402s

------ Mask Loss Comparison ------
Original Loss: 0.4537749
Best Mask Loss: 0.3253923 (mask length: 15)
Improvement: 0.1283826 (28.29%)
Applied Penalty Factor: 0.5
Final Loss with Penalty: 0.5179663
---------------------------------

	iters: 300, epoch: 8 | loss: 0.8928916 | orig_loss: 0.2515411 | penalty: 0.2524671
	speed: 0.0185s/iter; left time: 5812.5883s
	iters: 400, epoch: 8 | loss: 0.2103404 | orig_loss: 0.2554300 | penalty: 0.2560142
	speed: 0.0185s/iter; left time: 5814.2654s
	iters: 500, epoch: 8 | loss: 0.3163200 | orig_loss: 0.2566145 | penalty: 0.2574069
	speed: 0.0185s/iter; left time: 5807.6996s
	iters: 600, epoch: 8 | loss: 0.2016637 | orig_loss: 0.2431598 | penalty: 0.2434555
	speed: 0.0186s/iter; left time: 5851.9752s
	iters: 700, epoch: 8 | loss: 0.2139196 | orig_loss: 0.2591671 | penalty: 0.2593978
	speed: 0.0184s/iter; left time: 5789.5244s
	iters: 800, epoch: 8 | loss: 0.2798961 | orig_loss: 0.2557247 | penalty: 0.2559617
	speed: 0.0182s/iter; left time: 5733.6807s
	iters: 900, epoch: 8 | loss: 0.2308388 | orig_loss: 0.2560003 | penalty: 0.2565700
	speed: 0.0183s/iter; left time: 5738.8504s
	iters: 1000, epoch: 8 | loss: 0.1898590 | orig_loss: 0.2415311 | penalty: 0.2422524
	speed: 0.0183s/iter; left time: 5761.0832s
Epoch: 8 cost time: 19.913188219070435
Epoch Losses - Original: 0.2533223, With Penalty: 0.2539292, Difference: 0.0006069 (0.24%)
Epoch: 8, Steps: 1075 | Train Loss: 0.2984603 Vali Loss: 0.1674883 Test Loss: 0.2368908
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm2, seq_len: 48, pred_len: 144
Avg Original Loss: 0.2771179
Avg Penalty Loss: 0.2795051
Avg Difference: 0.0023872 (0.86%)
================================

>>>>>>>testing : long_term_forecast_ETTm2_48_144_SOFTS_ETTm2_ftM_sl48_ll48_pl144_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11377
mse:0.2344466343894044, mae:0.3008222564134131
Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTm2_48_192', model='SOFTS', data='ETTm2', root_path='./dataset/ETT/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=192, seasonal_patterns='Monthly', enc_in=7, dec_in=7, c_out=7, d_model=256, d_core=256, e_layers=1, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.0, embed='timeF', activation='gelu', output_attention=False, attention_type='full', use_norm=True, num_workers=4, itr=1, train_epochs=300, batch_size=32, patience=3, learning_rate=0.0003, des='Exp', loss='MSE', lradj='cosine', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', save_model=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ETTm2_48_192_SOFTS_ETTm2_ftM_sl48_ll48_pl192_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34321
val 11329
test 11329

------ Mask Loss Comparison ------
Original Loss: 1.1457530
Best Mask Loss: 1.0076221 (mask length: 12)
Improvement: 0.1381309 (12.06%)
Applied Penalty Factor: 5
Final Loss with Penalty: 1.8364075
---------------------------------

	iters: 100, epoch: 1 | loss: 0.3464128 | orig_loss: 0.3901386 | penalty: 0.4710789
	speed: 0.0332s/iter; left time: 10674.0852s
	iters: 200, epoch: 1 | loss: 0.3401124 | orig_loss: 0.3430659 | penalty: 0.4069467
	speed: 0.0190s/iter; left time: 6120.4886s

------ Mask Loss Comparison ------
Original Loss: 0.4605000
Best Mask Loss: 0.4491400 (mask length: 6)
Improvement: 0.0113600 (2.47%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.5173000
---------------------------------

	iters: 300, epoch: 1 | loss: 0.5783012 | orig_loss: 0.3219997 | penalty: 0.3731565
	speed: 0.0187s/iter; left time: 6027.1641s
	iters: 400, epoch: 1 | loss: 0.1944283 | orig_loss: 0.3368648 | penalty: 0.4110639
	speed: 0.0188s/iter; left time: 6029.1119s

------ Mask Loss Comparison ------
Original Loss: 0.5156600
Best Mask Loss: 0.4830977 (mask length: 18)
Improvement: 0.0325623 (6.31%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.6784716
---------------------------------

	iters: 500, epoch: 1 | loss: 0.7457071 | orig_loss: 0.3516001 | penalty: 0.4177238
	speed: 0.0189s/iter; left time: 6064.6050s
	iters: 600, epoch: 1 | loss: 0.3495462 | orig_loss: 0.3347381 | penalty: 0.3956134
	speed: 0.0187s/iter; left time: 6021.2407s
	iters: 700, epoch: 1 | loss: 0.2544364 | orig_loss: 0.3366942 | penalty: 0.3821764
	speed: 0.0189s/iter; left time: 6077.4092s

------ Mask Loss Comparison ------
Original Loss: 0.5267717
Best Mask Loss: 0.4556589 (mask length: 3)
Improvement: 0.0711128 (13.50%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.8823355
---------------------------------

	iters: 800, epoch: 1 | loss: 0.3064809 | orig_loss: 0.3643109 | penalty: 0.4272090
	speed: 0.0189s/iter; left time: 6077.5015s
	iters: 900, epoch: 1 | loss: 0.5768757 | orig_loss: 0.3555804 | penalty: 0.4224874
	speed: 0.0189s/iter; left time: 6073.6641s

------ Mask Loss Comparison ------
Original Loss: 0.2628234
Best Mask Loss: 0.2612650 (mask length: 3)
Improvement: 0.0015583 (0.59%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.2706150
---------------------------------

	iters: 1000, epoch: 1 | loss: 0.3875067 | orig_loss: 0.3508984 | penalty: 0.4181888
	speed: 0.0189s/iter; left time: 6058.7838s
Epoch: 1 cost time: 20.810346364974976
Epoch Losses - Original: 0.3469122, With Penalty: 0.4106636, Difference: 0.0637514 (18.38%)
Epoch: 1, Steps: 1073 | Train Loss: 0.4079808 Vali Loss: 0.1804273 Test Loss: 0.2590247
Validation loss decreased (inf --> 0.180427).  Saving model ...
Updating learning rate to 0.0002999917754048268
	iters: 100, epoch: 2 | loss: 0.2075960 | orig_loss: 0.3458304 | penalty: 0.4044979
	speed: 0.0581s/iter; left time: 18643.8497s

------ Mask Loss Comparison ------
Original Loss: 0.5009689
Best Mask Loss: 0.4555512 (mask length: 3)
Improvement: 0.0454177 (9.07%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.7280574
---------------------------------

	iters: 200, epoch: 2 | loss: 0.4219034 | orig_loss: 0.3197961 | penalty: 0.3803251
	speed: 0.0189s/iter; left time: 6048.4366s
	iters: 300, epoch: 2 | loss: 0.3765955 | orig_loss: 0.3692725 | penalty: 0.4305073
	speed: 0.0189s/iter; left time: 6058.5840s
	iters: 400, epoch: 2 | loss: 0.1942417 | orig_loss: 0.3512647 | penalty: 0.3975271
	speed: 0.0190s/iter; left time: 6096.1363s

------ Mask Loss Comparison ------
Original Loss: 0.5754316
Best Mask Loss: 0.4873791 (mask length: 15)
Improvement: 0.0880525 (15.30%)
Applied Penalty Factor: 5
Final Loss with Penalty: 1.0156941
---------------------------------

	iters: 500, epoch: 2 | loss: 0.2948561 | orig_loss: 0.3486765 | penalty: 0.4156088
	speed: 0.0188s/iter; left time: 6015.3735s
	iters: 600, epoch: 2 | loss: 0.5523294 | orig_loss: 0.3372655 | penalty: 0.3876131
	speed: 0.0190s/iter; left time: 6070.8759s
	iters: 700, epoch: 2 | loss: 0.4848773 | orig_loss: 0.3094475 | penalty: 0.3371223
	speed: 0.0189s/iter; left time: 6040.0165s

------ Mask Loss Comparison ------
Original Loss: 0.2240129
Best Mask Loss: 0.2234884 (mask length: 3)
Improvement: 0.0005246 (0.23%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.2266358
---------------------------------

	iters: 800, epoch: 2 | loss: 0.4488537 | orig_loss: 0.3338816 | penalty: 0.3738753
	speed: 0.0191s/iter; left time: 6101.2800s
	iters: 900, epoch: 2 | loss: 0.5093877 | orig_loss: 0.3227056 | penalty: 0.3754961
	speed: 0.0187s/iter; left time: 5992.5906s
	iters: 1000, epoch: 2 | loss: 0.2236895 | orig_loss: 0.3452073 | penalty: 0.3792396
	speed: 0.0187s/iter; left time: 5987.6902s

------ Mask Loss Comparison ------
Original Loss: 0.2987330
Best Mask Loss: 0.2820774 (mask length: 6)
Improvement: 0.0166556 (5.58%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.3820110
---------------------------------

Epoch: 2 cost time: 20.41041612625122
Epoch Losses - Original: 0.3372792, With Penalty: 0.3881617, Difference: 0.0508825 (15.09%)
Epoch: 2, Steps: 1073 | Train Loss: 0.3714330 Vali Loss: 0.1814770 Test Loss: 0.2616627
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002999671025212268
	iters: 100, epoch: 3 | loss: 0.4073271 | orig_loss: 0.3277142 | penalty: 0.3700693
	speed: 0.0577s/iter; left time: 18444.5423s
	iters: 200, epoch: 3 | loss: 0.1530914 | orig_loss: 0.3118939 | penalty: 0.3402633
	speed: 0.0188s/iter; left time: 6006.5068s

------ Mask Loss Comparison ------
Original Loss: 0.3028228
Best Mask Loss: 0.2967228 (mask length: 3)
Improvement: 0.0061000 (2.01%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.3333228
---------------------------------

	iters: 300, epoch: 3 | loss: 1.0897486 | orig_loss: 0.3416282 | penalty: 0.3934088
	speed: 0.0188s/iter; left time: 6006.4665s
	iters: 400, epoch: 3 | loss: 0.1640569 | orig_loss: 0.3329498 | penalty: 0.3562801
	speed: 0.0192s/iter; left time: 6126.1684s
	iters: 500, epoch: 3 | loss: 0.3325749 | orig_loss: 0.3134659 | penalty: 0.3330441
	speed: 0.0192s/iter; left time: 6123.3421s
	iters: 600, epoch: 3 | loss: 0.1845829 | orig_loss: 0.3400138 | penalty: 0.3763962
	speed: 0.0192s/iter; left time: 6122.5670s

------ Mask Loss Comparison ------
Original Loss: 0.6437693
Best Mask Loss: 0.6170503 (mask length: 3)
Improvement: 0.0267189 (4.15%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.7773638
---------------------------------

	iters: 700, epoch: 3 | loss: 0.2770060 | orig_loss: 0.3401408 | penalty: 0.3887240
	speed: 0.0191s/iter; left time: 6079.2072s
	iters: 800, epoch: 3 | loss: 0.1370073 | orig_loss: 0.3400411 | penalty: 0.3875247
	speed: 0.0187s/iter; left time: 5963.9540s
	iters: 900, epoch: 3 | loss: 0.5299691 | orig_loss: 0.3261753 | penalty: 0.3645917
	speed: 0.0187s/iter; left time: 5958.6229s
	iters: 1000, epoch: 3 | loss: 0.1878846 | orig_loss: 0.3337626 | penalty: 0.3696956
	speed: 0.0187s/iter; left time: 5960.5813s
Epoch: 3 cost time: 20.42166781425476
Epoch Losses - Original: 0.3296749, With Penalty: 0.3652017, Difference: 0.0355268 (10.78%)
Epoch: 3, Steps: 1073 | Train Loss: 0.3463249 Vali Loss: 0.1835479 Test Loss: 0.2638198
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002999259840548597

------ Mask Loss Comparison ------
Original Loss: 0.3341596
Best Mask Loss: 0.3257642 (mask length: 30)
Improvement: 0.0083954 (2.51%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.3761367
---------------------------------

	iters: 100, epoch: 4 | loss: 0.2058232 | orig_loss: 0.3243869 | penalty: 0.3478383
	speed: 0.0578s/iter; left time: 18399.1051s
	iters: 200, epoch: 4 | loss: 0.1637423 | orig_loss: 0.3011533 | penalty: 0.3247264
	speed: 0.0189s/iter; left time: 6019.8356s
	iters: 300, epoch: 4 | loss: 0.1800618 | orig_loss: 0.3042718 | penalty: 0.3234254
	speed: 0.0187s/iter; left time: 5944.5589s
	iters: 400, epoch: 4 | loss: 0.1938815 | orig_loss: 0.3271219 | penalty: 0.3468167
	speed: 0.0187s/iter; left time: 5945.9966s
	iters: 500, epoch: 4 | loss: 0.4437967 | orig_loss: 0.3046878 | penalty: 0.3181457
	speed: 0.0187s/iter; left time: 5939.7019s
	iters: 600, epoch: 4 | loss: 0.3856969 | orig_loss: 0.3363126 | penalty: 0.3516957
	speed: 0.0187s/iter; left time: 5935.4964s
	iters: 700, epoch: 4 | loss: 0.1987133 | orig_loss: 0.3132267 | penalty: 0.3325078
	speed: 0.0187s/iter; left time: 5954.4743s

------ Mask Loss Comparison ------
Original Loss: 0.5726355
Best Mask Loss: 0.5699191 (mask length: 3)
Improvement: 0.0027164 (0.47%)
Applied Penalty Factor: 5
Final Loss with Penalty: 0.5862173
---------------------------------

	iters: 800, epoch: 4 | loss: 0.2005643 | orig_loss: 0.3587110 | penalty: 0.4028449
	speed: 0.0188s/iter; left time: 5966.8152s
	iters: 900, epoch: 4 | loss: 0.4319426 | orig_loss: 0.3090640 | penalty: 0.3262992
	speed: 0.0191s/iter; left time: 6081.8438s
	iters: 1000, epoch: 4 | loss: 0.3031948 | orig_loss: 0.3238211 | penalty: 0.3355460
	speed: 0.0190s/iter; left time: 6049.9174s
Epoch: 4 cost time: 20.3099946975708
Epoch Losses - Original: 0.3210058, With Penalty: 0.3416922, Difference: 0.0206864 (6.44%)
Epoch: 4, Steps: 1073 | Train Loss: 0.2707417 Vali Loss: 0.1825280 Test Loss: 0.2623504
EarlyStopping counter: 3 out of 3
Early stopping

===== Training Loss Summary =====
Dataset: ETTm2, seq_len: 48, pred_len: 192
Avg Original Loss: 0.3337180
Avg Penalty Loss: 0.3764298
Avg Difference: 0.0427118 (12.80%)
================================

>>>>>>>testing : long_term_forecast_ETTm2_48_192_SOFTS_ETTm2_ftM_sl48_ll48_pl192_dm256_el1_dl1_df256_fc1_ebtimeF_dtTrue_Exp<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.25902469830796476, mae:0.3152498449015169
================ JOB COMPLETED ================
End time: Fri Apr 11 02:27:04 AM EDT 2025
===========================================
Extracting results to summary file: /scratch/sx2490/SOFTS_exp/SOFTS_mask/59192923_summary.txt
Summary extraction completed. Results saved to /scratch/sx2490/SOFTS_exp/SOFTS_mask/59192923_summary.txt
JobID             State ExitCode                 Reason 
------------ ---------- -------- ---------------------- 
59192923        RUNNING      0:0                   None 
59192923.ba+    RUNNING      0:0                        
59192923.ex+    RUNNING      0:0                        
